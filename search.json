[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nDates\n\n\nTopic\n\n\n\n\n\n\nWeek 01\n\n\nJan 08 - Jan 12\n\n\nWelcome to Digital Twins for Physical Systems\n\n\n\n\nWeek 02\n\n\nJanuary 15 - 19\n\n\nAdjoint methods for inverse problems\n\n\n\n\nWeek 03\n\n\nJanuary 22- 26\n\n\nAutomatic Differentiation & Variational Data Assimilation\n\n\n\n\nWeek 04\n\n\nJan/Feb 29 - 02\n\n\nData Assimilation\n\n\n\n\nWeek 05\n\n\nFeb 05 - 09\n\n\nBayesian Data Assimilation\n\n\n\n\n\nNo matching items\n\n\n\n\nLabs will be due on Fridays at 11:59pm.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#weekly-schedule",
    "href": "schedule.html#weekly-schedule",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nDates\n\n\nTopic\n\n\n\n\n\n\nWeek 01\n\n\nJan 08 - Jan 12\n\n\nWelcome to Digital Twins for Physical Systems\n\n\n\n\nWeek 02\n\n\nJanuary 15 - 19\n\n\nAdjoint methods for inverse problems\n\n\n\n\nWeek 03\n\n\nJanuary 22- 26\n\n\nAutomatic Differentiation & Variational Data Assimilation\n\n\n\n\nWeek 04\n\n\nJan/Feb 29 - 02\n\n\nData Assimilation\n\n\n\n\nWeek 05\n\n\nFeb 05 - 09\n\n\nBayesian Data Assimilation\n\n\n\n\n\nNo matching items\n\n\n\n\nLabs will be due on Fridays at 11:59pm.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\n\n\n\nReuseCC-BY",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#motivation",
    "href": "slides/week-04/06-Bayes-Inference.html#motivation",
    "title": "Statistical Inverse Problems",
    "section": "Motivation",
    "text": "Motivation\nSo, far we dealt with deterministic problems\n\nproduce single answer\nassume there was no noise\nno uncertainties in forward model\n\nReality is different\n\nneed to handle uncertainty\nBayesian inference provides a framework for principled handling of uncertainty\nproduces estimate for the posterior distribution—i.e, offers complete statistical description of all parameter values that are consistent with the data."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-approach",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-approach",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nBayesian analysis provides approach for models to learn from data\n\nCapture our prior knowledge about a model.\nRigorously assess the plausibility of candidate model classes based on system data.\nFinally, we can make robust probabilistic predictions that incorporate all uncertainties, allowing for better decision making and design.\n\nRelevant for system identification, or parameter estimation, where the inverse problems are often ill-posed and many candidate models exist to describe the behavior of a system.\n\nEstimation of uncertain parameters.\nPrediction of future events.\nTests of hypotheses.\nDecision making."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-inference",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-inference",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nIn the most general case, where we want to perform Bayesian inference for the estimation of parameters (an inverse problem!), we simply replace the probabilities by the corresponding density functions.\nThen Bayesian inference is performed in three steps:\n\nChoose a probability density \\(f(\\theta),\\) called the prior distribution, that expresses our beliefs, or prior experimental or historical knowledge, about a parameter \\(\\theta\\) before we see any data.\nChoose a statistical model \\(f(x\\mid\\theta)\\) that reflects our beliefs about \\(x\\) given \\(\\theta.\\) Notice that this is expressed as a conditional probability, called the likelihood function, and not as a joint probability function.\nAfter observing data \\(x_{1},\\ldots,x_{n},\\) update our beliefs and calculate the posterior distribution \\(f(\\theta\\mid x_{1},\\ldots,x_{n}).\\)\n\nLet us look more closely at the three components of Bayes’ Law."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section",
    "href": "slides/week-04/06-Bayes-Inference.html#section",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Definition 1 Prior Distribution. For a given statistical model that depends on a parameter \\(\\theta,\\) considered as random, the distribution assigned to \\(\\theta\\) before observing the other random variables of interest is called the prior distribution. This is just the marginal distribution of the parameter.\n\n\nDefinition 2 Posterior Distribution. For a statistical inference problem, with parameter \\(\\theta\\) and random sample \\(X_{1},\\ldots,X_{n},\\) the conditional distribution of \\(\\theta\\) given \\(X_{1}=x_{1},\\) \\(\\ldots,\\) \\(X_{n}=x_{n}\\) is called the posterior distribution of \\(\\theta.\\)\n\n\nDefinition 3 Likelihood Function. Suppose that \\(X_{1},X_{2},\\ldots,X_{n}\\) have a joint density \\[f(X_{1},X_{2},\\ldots,X_{n}\\mid\\theta).\\] Given observations \\(X_{1}=x_{1},\\) \\(X_{2}=x_{2},\\) \\(\\ldots,\\) \\(X_{n}=x_{n},\\) the likelihood function of \\(\\theta\\) is \\[L(\\theta)=L(\\theta\\mid x_{1},x_{2},\\ldots,x_{n})=f(x_{1},x_{2},\\ldots,x_{n}\\mid\\theta).\\]"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-1",
    "href": "slides/week-04/06-Bayes-Inference.html#section-1",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "If the \\(X_{i}\\) are i.i.d. with density \\(f(X_{i}\\mid\\theta),\\) then the joint density is a product and \\[L(\\theta\\mid x_{1},x_{2},\\ldots,x_{n})=\\prod_{i=1}^{n}f(x_{i}\\mid\\theta).\\]\nWe point out the following properties of the likelihood:\n\nThe likelihood is not a probability density function and can take values outside the interval \\([0,1].\\)\nLikelihood is an important concept in both frequentist and Bayesian statistics.\nLikelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric modelthis will be very important when we will deal with parameter estimation, and inverse problems in general.\nThe likelihood measures the support (evidence) provided by the data for each possible value of the parameter. This means that if we compute the likelihood function at two points, \\(\\theta=\\theta_{1},\\) \\(\\theta=\\theta_{2},\\) and find that \\(L(\\theta_{1}\\mid x)&gt;L(\\theta_{2}\\mid x)\\), then the sample observed is more likely to have occurred if \\(\\theta=\\theta_{1}.\\) We say that \\(\\theta_{1}\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_{2}.\\)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-2",
    "href": "slides/week-04/06-Bayes-Inference.html#section-2",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "For i.i.d. random variables, thelog-likelihood is usually used, since it reduces the product to a sum.\nWe now formulate the general version of Bayes’ Theorem.\n\nSuppose that \\(n\\) random variables, \\(X_{1},\\ldots,X_{n},\\) form a random sample from a distribution with density, or probability function in the case of a discrete distribution, \\(f(x\\mid\\theta).\\) Suppose also that the unknown parameter, \\(\\theta,\\) has a prior pdf \\(f(\\theta).\\) Then the posterior pdf of \\(\\theta\\) is \\[f(\\theta\\mid x)=\\frac{f(x_{1}\\mid\\theta)\\cdots f(x_{n}\\mid\\theta)f(\\theta)}{f_{n}(x)},\\label{eq:bayes}\\] where \\(f_{n}(x)\\) is the marginal joint pdf of \\(X_{1},\\ldots,X_{n}.\\)\n\nIn this theorem,\n\nthe prior, \\(f(\\theta),\\) represents the credibility of, or belief in the values of the parameters we seek, without any consideration of the data/observations;\nthe posterior, \\(f(\\theta\\mid x),\\) is the credibility of the parameters with the data taken into account;"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-3",
    "href": "slides/week-04/06-Bayes-Inference.html#section-3",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "\\(f(x\\mid\\theta),\\) considered as a function of \\(\\theta,\\) is the likelihoodfunction, which is the probability that the data/observation could be generated by the model with a given value of the parameter;\nthe denominator, called the evidence, \\(f_{n}(x),\\) is thetotal probability of the data taken over all the possible parameter values, also called the marginal likelihood, or the marginal, and can be considered as a normalization factor;\nthe posterior distribution is thus proportional to the product of the likelihood and the prior distribution, or, in applied terms, \\[f(\\mathrm{parameter}\\mid\\mathrm{data})\\propto f(\\mathrm{data}\\mid\\mathrm{parameter})\\,f(\\mathrm{parameter}).\\]\n\nWhat can one do with the posterior distribution thus obtained? The answer is a lot of things, in fact a complete quantification of the incertitude of the parameter’s estimation is possible. We can compute:\n\nPoint estimates by summarizing the center of the posterior. Typically, these are the posterior mean or the posterior mode."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-4",
    "href": "slides/week-04/06-Bayes-Inference.html#section-4",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Interval estimates for a given level \\(\\alpha\\) see below.\nEstimates of the probability of an event, such as \\(\\mathrm{P}(a&lt;\\theta&lt;b)\\) or \\(P(\\theta&gt;b).\\)\nPosterior quantiles.\n\n\n\n\n\n\n\nNote\n\n\nBayesian Inference is a general framework that plays a crucial rule in solving inverse and data-assimilation problems. It also plays a major role in uncertainty quantification. These will be discussed\n\nwhen we introduce Bayesian filtering\nextensions of Kalman filtering\nsimulation-based inference that leverages the ability of deep neural networks to capture the posterior from simulations"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-5",
    "href": "slides/week-04/06-Bayes-Inference.html#section-5",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Given data, \\(\\mathcal{D}\\), we want to do three things:\n\nEstimate the parameters \\(\\boldsymbol{\\theta}\\) of a model \\(\\mathcal{M}\\), not as point or optimal values, but inferring their probability distribution. This is known as parameter estimation.\nSelect the best model by comparing different parametrizations using the model posterior distribution. This is known as model selection.\nPredict from the chosen model the data for some new input values. This is known as posterior prediction.\n\nBayesian parameter estimation (BPE) is a universal approach to fitting models to data\n\ndefine a generative model for the data,\na likelihood function, and\na prior distribution over the parameters.\n\nThe posterior rarely has closed form, so we characterize it by sampling, from which we can find the best model parameters together with their uncertainties."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayes-theorem",
    "href": "slides/week-04/06-Bayes-Inference.html#bayes-theorem",
    "title": "Statistical Inverse Problems",
    "section": "Bayes’ theorem",
    "text": "Bayes’ theorem\n\n\nAccording to Theorem 2.61, Bayes’ rule take the form\n\\[p(\\boldsymbol{\\theta}\\mid \\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\int_{\\boldsymbol{\\theta}}p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}\\]\nwhere\n\n\\(p(\\mathcal{D}\\mid\\boldsymbol{\\theta})\\) is the conditional probability known as likelihood—.i.e, physical model generating data\n\\(p(\\boldsymbol{\\theta})\\) is the prior distribution\n\\(p(\\boldsymbol{\\theta}\\mid \\mathcal{D})\\) is the posterior probability distribution.\n\n\n\n\n\nfrom Asch (2022)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-6",
    "href": "slides/week-04/06-Bayes-Inference.html#section-6",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Likelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric model—this will be very important in all the chapters where we will deal with parameter estimation.\nBayes’ theorem can be extended to include a model, \\(\\mathcal{M}\\), for the parameters yielding\n\\[p(\\boldsymbol{\\theta}\\mid \\mathcal{D},\\mathcal{M})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta},\\mathcal{M})p(\\boldsymbol{\\theta}\\mid\\mathcal{M})}{p(\\mathcal{D}\\mid \\mathcal{M})}\\]\nor in practical terms\n\\[p(\\text{parameter}\\mid\\text{data})\\propto p(\\text{data}\\mid \\text{parameter})p(\\text{parameter})\\]\nMost probable estimator is called the maximum a posteriori (MAP) estimator — the \\(\\boldsymbol{\\theta}\\) that maximizes the posterior—i.e.,\n\\[\\boldsymbol{\\theta}_\\ast=\\mathop{\\mathrm{arg\\,max}}_{\\boldsymbol{\\theta}}p(\\boldsymbol{\\theta}\\mid \\mathcal{D})\\]"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-priors",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-priors",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian priors",
    "text": "Bayesian priors\nPriors can be classified in three broad categories:\n\nUninformative priors, which are flat/diffuse and have smallest impact on posterior.\nWeakly informative priors, use some knowledge,e.g. positivity.\nInformative priors, convey maximal information\n\n\n\nPriors can be sequential. Suppose we observe \\(y_1,\\dots, y_n\\) sequentially, then we can write\n\\[p(\\theta\\mid y_1,\\dots, y_m)\\propto p(\\theta\\mid y_1,\\dots, y_{m-1})p(y_m\\mid \\theta),\\]\n\\(m=2,3.\\dots,n\\) where the posterior distribution of the parameter \\(\\theta\\) after \\(m − 1\\) measurements acts as a prior with \\(p(y_m\\mid \\theta)\\) the likelihood.\n\ncan be applied recursively\nholds when \\(\\theta\\) does not vary with time\n\n\n\n\n\nfrom Asch (2022)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-regression",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-regression",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nModel relation between a dependent variable, \\(y\\), and observed independent variables, \\(x_1,x_2,\\dots x_p\\) that represent properties of a process.\n\\[y=f(x;\\theta)\\]\nForward/direct problem: compute \\(y\\) given \\(\\theta\\) (easy)\nInverse/indirect problem: compute \\(\\theta\\) given \\(y\\) (difficult)\nwhere\n\n\\(f\\) is an operator/equation/system\n\\(x\\) is the independent variable\n\\(\\theta\\) is the parameter/feature\n\\(y\\) is the measurement/dependent variable"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-7",
    "href": "slides/week-04/06-Bayes-Inference.html#section-7",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "We have \\[\\mathcal{D}=\\left\\{(\\mathbf{x}_i,y_1),\\,i=1,\\dots n\\right\\}\\]\nthat can be solved by\n\nclassical least squares—i.e., find parameters \\(\\theta\\) that minimize the squared error.\nmaximum likelihood where we choose the parameters that maximize the likelihood of the parameters.\nBayesian regression, where we use Bayes’ formula to compute a posterior distribution of the parameters.\n\nMethods 1 and 2 yield point estimates while 3 provides full posterior."
  },
  {
    "objectID": "slides/week-03/04-AD.html#outline",
    "href": "slides/week-03/04-AD.html#outline",
    "title": "Differential Programming",
    "section": "Outline",
    "text": "Outline\n\nAutomatic differentiation for scientific machine learning:\n\nDifferentiable programming with autograd and PyTorch and Zygote.jl in Flux.jl.\nGradients, adjoints, backpropagation and inverse problems.\nNeural networks for scientific machine learning.\nPhysics-informed neural networks.\nThe use of automatic differentiation in scientific machine learning.\nThe challenges of applying automatic differentiation to scientific applications.\n\n\nDifferential programming is a technique for automatically computing the derivatives of functions.\nThis can be done using a variety of techniques, including:"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section",
    "href": "slides/week-03/04-AD.html#section",
    "title": "Differential Programming",
    "section": "",
    "text": "Symbolic differentiation: This involves using symbolic mathematics to represent the function and its derivatives. This can be a powerful technique, but it can be difficult to use for complex functions.\nNumerical differentiation: This involves using numerical methods to approximate the derivatives of the function. This is a simpler technique than symbolic differentiation, but it is less accurate.\nAutomatic differentiation: This is a technique that combines symbolic and numerical differentiation to automatically compute the derivatives of functions. This is the most powerful technique for differential programming, and it is the most commonly used technique in scientific machine learning.\n\nThe mathematical theory of differential programming is based on the concept of gradients.\n\nThe gradient of a function is a vector that tells you how the function changes as its input changes. In other words, the gradient of a function tells you the direction of steepest ascent or descent."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-1",
    "href": "slides/week-03/04-AD.html#section-1",
    "title": "Differential Programming",
    "section": "",
    "text": "The gradient of a function can be calculated using the gradient descent algorithm. The gradient descent algorithm works by starting at a point and then moving in the direction of the gradient until it reaches a minimum or maximum.\nIn ML, we use stochastic gradientoptimization methods\n\nDifferential programming can be used to solve a variety of problems in scientific machine learning, including:\n\nCalculating the gradients of loss functions for machine learning models—this is important for training machine learning models.\nSolving differential equations—this can be used to model the behavior of physical systems.\nPerforming optimization—this can be used to find the optimal solution to a problem.\nSolving inverse and data assimilation problems—this is none other than a special case of optimization."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-2",
    "href": "slides/week-03/04-AD.html#section-2",
    "title": "Differential Programming",
    "section": "",
    "text": "OPTIMIZATION"
  },
  {
    "objectID": "slides/week-03/04-AD.html#optimization",
    "href": "slides/week-03/04-AD.html#optimization",
    "title": "Differential Programming",
    "section": "Optimization",
    "text": "Optimization\n\n \n\nOptimization routines typically use local information about a function to iteratively approach a local minimum.\nIn this (rare) case, where we have a convex function, we easily find a global minimum.\nBut in general, global optimization can be very difficult\nWe usually get stuck in local minima!\nThings get MUCH harder in higher spatial dimensions…"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-3",
    "href": "slides/week-03/04-AD.html#section-3",
    "title": "Differential Programming",
    "section": "",
    "text": "DIFFERENTIAL PROGRAMMING"
  },
  {
    "objectID": "slides/week-03/04-AD.html#differential-programming",
    "href": "slides/week-03/04-AD.html#differential-programming",
    "title": "Differential Programming",
    "section": "Differential Programming",
    "text": "Differential Programming\nThere are 3 ways to compute derivatives of functions:\n\nSymbolic differentiation.\nNumerical differentiation.\nAutomatic differentiation.\n\nSee Notebooks for Intro Pytorch and Differential Programming."
  },
  {
    "objectID": "slides/week-03/04-AD.html#symbolic-differentiation",
    "href": "slides/week-03/04-AD.html#symbolic-differentiation",
    "title": "Differential Programming",
    "section": "Symbolic Differentiation",
    "text": "Symbolic Differentiation\nComputes exact, analytical derivatives, in the form of a mathematical expression.\n\nThere is no approximation error.\nOperates recursively by applying simple rules to symbols.\nThere may be no analytical expression for gradients of some functions.\nCan lead to redundant and overly complex expressions.\n\nBased on the sympy package of Python.\n\nOther software: Mathematica, Maple, Sage, etc."
  },
  {
    "objectID": "slides/week-03/04-AD.html#numerical-differentiation",
    "href": "slides/week-03/04-AD.html#numerical-differentiation",
    "title": "Differential Programming",
    "section": "Numerical Differentiation",
    "text": "Numerical Differentiation\n\nDefinition 1 If \\(f\\) is a differentiable function, then \\[f'(x)=\\lim_{h\\rightarrow0}\\frac{f(x+h)-f(x)}{h}\\]\n\nUsing Taylor expansions, and the definition of the derivative, we can obtain finite-difference, numerical approximationsto the derivatives of \\(f,\\) such as \\[f'(x)=\\frac{f(x+h)-f(x)}{h}+\\mathcal{O}(h),\\] \\[f'(x)=\\frac{f(x+h)-f(x-h)}{2h}+\\mathcal{O}(h^{2})\\]\n\nconceptually simple and very easy to code\ncompute gradients of \\(f\\colon\\mathbb{R}^{m}\\rightarrow\\mathbb{R},\\) requires at least \\(\\mathcal{O}(m)\\) function evaluations\nbig numerical errors due to truncation and roundoff."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-4",
    "href": "slides/week-03/04-AD.html#section-4",
    "title": "Differential Programming",
    "section": "",
    "text": "AUTOMATIC DIFFERENTIATION"
  },
  {
    "objectID": "slides/week-03/04-AD.html#automatic-differentiation",
    "href": "slides/week-03/04-AD.html#automatic-differentiation",
    "title": "Differential Programming",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nAutomatic differentiation is an umbrella term for a variety of techniques for efficiently computing accurate derivatives of more or less general programs.\n\nIt is employed by all major neural network frameworks, where a single reverse-mode AD backpass (also known as “backpropagation”) can compute a full gradient.\nNumerical differentiation would either require many forward passes or symbolic differentiation that is simply untenable due to expression explosion.\nThe survey paper (Baydin et al. 2018) provides an excellent review of all the methods and tools available.\n\nMany algorithms in machine learning, computer vision, physical simulation, and other fields require the calculation of gradients and other derivatives.\n\nManual derivation of gradients can be both time-consuming and error-prone.\nAutomatic differentiation comprises a set of techniques to calculate the derivative of a numerical computation expressed as a computer code."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-5",
    "href": "slides/week-03/04-AD.html#section-5",
    "title": "Differential Programming",
    "section": "",
    "text": "These techniques of AD, commonly used for data assimilation in atmospheric sciences and optimal design in computational fluid dynamics, have more recently also been adopted by machine learning researchers.\nThe backpropagation algorithm, used for optimally computing the weights of a neural network, is just a special case of general AD.\nAD can be found in all the major software libraries for ML/DL, such as TensorFlow, PyTorch, JaX, and Julia’s Flux.jl/Zygote.jl.\n\n\nPractitioners across many fields have built a wide set of automatic differentiation tools, using different programming languages, computational primitives, and intermediate compiler representations.\n\nEach of these choices comes with positive and negative trade-offs, in terms of their usability, flexibility, and performance in specific domains."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-6",
    "href": "slides/week-03/04-AD.html#section-6",
    "title": "Differential Programming",
    "section": "",
    "text": "Nevertheless, the availability of such tools should not be neglected, since the potential gain from their use is very large.\nMoreover, the fact that they are already built-in to a large number of ML methods, makes their use quite straightforward.\n\nAD can be readily and extensively used and is thus applicable to many industrial and practical Digital Twin contexts (Asch 2022).\nHowever Digital Twins that require large-scale ML remain challenging.\nWhile substantial efforts are made within the ML communities of PyTorch/Tensorflow, these approaches struggle for large-scale problems that need to\n\nbe frugal with memory use\nexploit parallelism across multiple nodes/GPUs\nintegrate with existing (parallel) CSE applications\n\nWorthwhile to explore Julia’s more integrated approach to HPC Differential Programming (Innes et al. 2019) and SciML (Rackauckas and Nie 2017)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#ad-for-sciml",
    "href": "slides/week-03/04-AD.html#ad-for-sciml",
    "title": "Differential Programming",
    "section": "AD for SciML",
    "text": "AD for SciML\nRecent progress in machine learning (ML) technology has been spectacular.\nAt the heart of these advances is the ability to obtain high-quality solutions to non-convex optimization problems for functions with billions—or even hundreds of billions—of parameters.\nIncredible opportunity for progress in classical applied mathematics problems.\n\nIn particular, the increased proficiency for systematically handling large, non-convex optimization scenarios may help solve some classical problems that have long been a challenge.\nWe now have the chance to make substantial headway on questions that have not yet been formulated or studied because we lacked the tools to solve them."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-7",
    "href": "slides/week-03/04-AD.html#section-7",
    "title": "Differential Programming",
    "section": "",
    "text": "To be clear, we do not wish to oversell the state of the art, however:\n\nAlgorithms that identify the global optimum for non-convex optimization problems do not yet exist.\nThe ML community has instead developed efficient, open source software tools that find candidate solutions.\nThey have created benchmarks to measure solution quality.\nThey have cultivated a culture of competition against these benchmarks."
  },
  {
    "objectID": "slides/week-03/04-AD.html#automatic-differentiationbackprop-autograd-zygote.jl-etc.",
    "href": "slides/week-03/04-AD.html#automatic-differentiationbackprop-autograd-zygote.jl-etc.",
    "title": "Differential Programming",
    "section": "Automatic Differentiation—backprop, autograd, Zygote.jl, etc.",
    "text": "Automatic Differentiation—backprop, autograd, Zygote.jl, etc.\n\nBackprop is a special case of Algorithmic Differentiation (AD).\nAutograd is a particular AD package that us supported w/i Python (as part of Pytorch).\nMost exercises of this course use PyTorch’s AD.\nHaving said that we strongly encourage students to do the exercises in Julia using its extensive AD capabilities (see JuliaDiff), integration in the Julia language, and use of abstractions that allow for\nmixing of hand-derived (adjoint-state) gradients and AD via ChainRules.jl\na single AD interface irrespective of the AD backend through the use of AbstractDifferentiation.jl."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-8",
    "href": "slides/week-03/04-AD.html#section-8",
    "title": "Differential Programming",
    "section": "",
    "text": "Important\n\n\nAD is NOT finite differences, nor symbolic differentiation. Finite differences are too expensive (one forward pass for each discrete point). They induce huge numerical errors (truncation/approximation and roundoff) and are very unstable in the presence of noise.\n\n\n\n\n\n\n\n\n\nNote\n\n\nAD is both efficient—linear in the cost of computing the value—and numerically stable.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe goal of AD is not a formula, but a procedure for computing derivatives."
  },
  {
    "objectID": "slides/week-03/04-AD.html#tools-for-ad",
    "href": "slides/week-03/04-AD.html#tools-for-ad",
    "title": "Differential Programming",
    "section": "Tools for AD",
    "text": "Tools for AD\nNew opportunities that exist because of the widespread, open-source deployment of effective software tools for automatic differentiation.\nWhile the mathematical framework for automatic differentiation was established long ago—dating back at least to the evolution of adjoint-based optimization in optimal control (Asch, Bocquet, and Nodet 2016; Asch 2022)—ML researchers have recently designed efficient software frameworks that natively run on hardware accelerators (GPUs).\n\nThese frameworks have served as a core technology for the ML revolution over the last decade and inspired high-quality software libraries such as\n\nJAX,\nPyTorch and TensorFlow\nJulia’s ML with Flux.jl and AD with Zygote.jl and abstractions with ChainRules.jl and AbstractDifferentiation.jl"
  },
  {
    "objectID": "slides/week-03/04-AD.html#statements",
    "href": "slides/week-03/04-AD.html#statements",
    "title": "Differential Programming",
    "section": "Statements",
    "text": "Statements\nThe technology’s key feature is: the computational cost of computing derivatives of a target loss function is independent of the number of parameters;\n\nthis trait makes it possible for users to implement gradient-based optimization algorithms for functions with staggering numbers of parameters.\n\n\n“Gradient descent can write code better than you, I’m sorry.”\n“Yes, you should understand backprop.”\n“I’ve been using PyTorch a few months now and I’ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.”\n\n\nAndrej Karpathy [~2017] (Tesla AI, OpenAI)\n\n\n\n\n\n\n\nNote\n\n\nTools such as PyTorch and TensorFlow may not scale to 3D problems and are challenging to integrate with physics-based simulations and gradients (via adjoint state)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-9",
    "href": "slides/week-03/04-AD.html#section-9",
    "title": "Differential Programming",
    "section": "",
    "text": "BACKPROPAGATION"
  },
  {
    "objectID": "slides/week-03/04-AD.html#backpropagationoptimization-problem",
    "href": "slides/week-03/04-AD.html#backpropagationoptimization-problem",
    "title": "Differential Programming",
    "section": "Backpropagation—optimization problem",
    "text": "Backpropagation—optimization problem\nWe want to solve a (nonlinear, non-convex) optimization problem, either\n\nfor a dynamic system, \\[\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t}=f(\\mathbf{x};\\mathbf{\\theta}),\\] where \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{\\theta}\\in\\mathbb{R}^{p}\\) with \\(n,p\\gg1.\\)\nor for a machine learning model \\[\\mathbf{y}=f(\\mathbf{x};\\mathbf{w}),\\] where \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{w}\\in\\mathbb{R}^{p}\\) with \\(n,p\\gg1.\\)\n\nTo find the minimum/optimum, we want to minimize an appropriate cost/loss function \\[J(\\mathbf{x},\\mathbf{\\theta}),\\quad\\mathcal{L}(\\mathbf{w},\\mathbf{\\theta})\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-10",
    "href": "slides/week-03/04-AD.html#section-10",
    "title": "Differential Programming",
    "section": "",
    "text": "usually someerror norm, and then (usually) compute its average\nThe best/fastest way to solve this optimization problem, is to use gradients and gradient-based methods.\n\nDefinition 2 Backpropagation is an algorithm for computing gradients.\nBackpropagation is an instance of reverse-mode automatic differentiation\n\nvery broadly applicable to machine learning, data assimilation and inverse problems in general\nit is “just” a clever and efficient use of the Chain Rule for derivatives"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-11",
    "href": "slides/week-03/04-AD.html#section-11",
    "title": "Differential Programming",
    "section": "",
    "text": "We can prove mathematically the following equivalences:\n\nflowchart TD\n  A[Backpropagation]&lt;--&gt;B[Reverse-mode automatic differentiation]&lt;--&gt;C[Discrete adjoint-state method]\n\n\n\nflowchart TD\n  A[Backpropagation]&lt;--&gt;B[Reverse-mode automatic differentiation]&lt;--&gt;C[Discrete adjoint-state method]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nRecall: the adjoint-state method is the theoretical basis for Data Assimilation, as well as many other inverse problems—see Basic Course, Lecture on Adjoint Methods)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#chain-rule",
    "href": "slides/week-03/04-AD.html#chain-rule",
    "title": "Differential Programming",
    "section": "Chain Rule",
    "text": "Chain Rule\nWe want to compute the cost/loss function gradient, which is usually the average over the training samples of the loss gradient, \\[\\nabla_{w}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial w},\\quad\\nabla_{\\theta}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial\\theta},\\] or, in general \\[\\nabla_{z}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial z},\\] where \\(z=w\\) or \\(z=\\theta,\\) etc.\nRecall: if \\(f(x)\\) and \\(x(t)\\) are univariate (differentiable) functions, then \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x(t))=\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-12",
    "href": "slides/week-03/04-AD.html#section-12",
    "title": "Differential Programming",
    "section": "",
    "text": "and this can be easily generalized to the multivariate case, such as \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x(t),y(t))=\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\frac{\\mathrm{d}x}{\\mathrm{d}t}+\\frac{\\mathrm{d}f}{\\mathrm{d}y}\\frac{\\mathrm{d}y}{\\mathrm{d}t}\\]\n\nExample 1 Consider \\[f(x,y,z)=(x+y)z\\]\nDecompose \\(f\\) into simple differentiable elements \\[q(x,y)=x+y,\\] then \\[f=qz\\]\n\n\n\n\n\n\n\nNote\n\n\nEach element has an analytical (exact/known) derivative—eg. sums, products, sines, cosines, min, max, exp, log, etc."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-13",
    "href": "slides/week-03/04-AD.html#section-13",
    "title": "Differential Programming",
    "section": "",
    "text": "Compute the gradient of \\(f\\) with respect to its three variables, using the chain rule\n\nwe begin with \\[\\frac{\\partial f}{\\partial q}=z,\\quad\\frac{\\partial f}{\\partial z}=q\\] and \\[\\frac{\\partial q}{\\partial x}=1,\\quad\\frac{\\partial q}{\\partial y}=1\\]\nthen the chain rule gives the terms of the gradient, \\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial x} & =\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=z\\cdot1\\\\\n    \\frac{\\partial f}{\\partial y} & =\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=z\\cdot1\\\\\n    \\frac{\\partial f}{\\partial z} & =q\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-14",
    "href": "slides/week-03/04-AD.html#section-14",
    "title": "Differential Programming",
    "section": "",
    "text": "# set some inputs\nx = -2; y = 5; z = -4\n# perform the forward pass\nq = x + y # q becomes 3\nf = q * z # f becomes -12\n# perform the backward pass (backpropagation)\n# in reverse order:\n# first backprop through f = q * z \ndfdz = q  # df/dz = q, so gradient on z becomes 3 \ndfdq = z  # df/dq = z, so gradient on q becomes -4 \ndqdx = 1.0\ndqdy = 1.0\n\n# now backprop through q = x + y\n# dfdx = dfdq * dqdx  # The * here is the chain rule!\ndfdy = dfdq * dqdy\n\nWe obtain the gradient in the variables [``dfdx, dfdy, dfdz``] that give us the sensitivity of the function f to the variables x, y and z.\nIt’s all done with graphs... DAGs, in fact"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-15",
    "href": "slides/week-03/04-AD.html#section-15",
    "title": "Differential Programming",
    "section": "",
    "text": "The above computation can be visualized with a circuit diagram:\n\n\n\nthe forward pass, computes values from inputs to outputs\nthe backward pass then performs backpropagation, starting at the end and recursively applying the chain rule to compute the gradients all the way to the inputs of the circuit.\n\nThe gradients can be thought of as flowing backwards through the circuit."
  },
  {
    "objectID": "slides/week-03/04-AD.html#forward-vs-reverse-mode",
    "href": "slides/week-03/04-AD.html#forward-vs-reverse-mode",
    "title": "Differential Programming",
    "section": "Forward vs Reverse Mode",
    "text": "Forward vs Reverse Mode\nForward mode is used for\n\nsolving nonlinear equations\nsensitivity analysis\nuncertainty propagation/quantification \\[f(x+\\Delta x)\\approx f(x)+f'(x)\\Delta x\\]\n\nReverse mode is used for\n\nmachine/deep learning\noptimization"
  },
  {
    "objectID": "slides/week-03/04-AD.html#backprop---ml-example",
    "href": "slides/week-03/04-AD.html#backprop---ml-example",
    "title": "Differential Programming",
    "section": "Backprop - ML example",
    "text": "Backprop - ML example\nFor a univariate, logistic least-squares problem, we have:\n\nlinear model/function of \\(x\\): \\(z=wx+b\\)\nnonlinear activation: \\(y=\\sigma(x)\\)\nquadratic loss: \\(\\mathcal{L}=(1/2)(y-t)^{2},\\) where \\(t\\) is the target/observed value\n\nObjective: find the values of the parameters/weights, \\(w\\) and \\(b,\\) that minimize the loss \\(\\mathcal{L}\\)\n\nto do this, we will use the gradient of \\(\\mathcal{L}\\) with respect to the parameters/weights, \\(w\\) and \\(b,\\) \\[\\nabla_{w}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial w},\\quad\\nabla_{b}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial b}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#brute-force",
    "href": "slides/week-03/04-AD.html#brute-force",
    "title": "Differential Programming",
    "section": "Brute force",
    "text": "Brute force\nCalculus approach:\n\n\n\nIt’s a mess... too many computations, too complex to program!"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-16",
    "href": "slides/week-03/04-AD.html#section-16",
    "title": "Differential Programming",
    "section": "",
    "text": "Structured approach: \\[\\begin{aligned}\n     & \\mathrm{compute\\ loss} &  & \\mathrm{compute\\ derivatives}\\\\\n     & \\mathrm{{\\color{blue}forwards}} &  & \\mathrm{{\\color{red}backwards}}\\\\\n    z & =wx+b & \\frac{\\partial\\mathcal{L}}{\\partial y} & =y-t\\\\\n    y & =\\sigma(z) & \\frac{\\partial\\mathcal{L}}{\\partial z} & =\\frac{\\partial\\mathcal{L}}{\\partial y}\\frac{\\partial y}{\\partial z}=\\frac{\\partial\\mathcal{L}}{\\partial y}\\sigma'(z)\\\\\n    \\mathcal{L} & =\\frac{1}{2}(y-t)^{2} & \\frac{\\partial\\mathcal{L}}{\\partial w} & =\\frac{\\partial\\mathcal{L}}{\\partial z}\\frac{\\partial z}{\\partial w}=\\frac{\\partial\\mathcal{L}}{\\partial z}x\\\\\n     &  & \\frac{\\partial\\mathcal{L}}{\\partial b} & =\\frac{\\partial\\mathcal{L}}{\\partial z}\\frac{\\partial z}{\\partial b}=\\frac{\\partial\\mathcal{L}}{\\partial z}\\cdot1\n    \\end{aligned}\\]\n\ncan easily be written as a computational graphwith\nnodes = inputs and computed quantities\nedges = nodes computed directly as functions of other nodes"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-17",
    "href": "slides/week-03/04-AD.html#section-17",
    "title": "Differential Programming",
    "section": "",
    "text": "Lossis computed in the forward pass\nGradient is computed in the backward pass\n\nthe derivatives of \\(y\\) and \\(z\\) are exact/known\nthe derivatives of \\(\\mathcal{L}\\) are computed, starting from the end\nthe gradients wrt to the parameters are readily obtained by backpropagation using the chain rule!"
  },
  {
    "objectID": "slides/week-03/04-AD.html#full-backprop-algorithm",
    "href": "slides/week-03/04-AD.html#full-backprop-algorithm",
    "title": "Differential Programming",
    "section": "Full Backprop Algorithm",
    "text": "Full Backprop Algorithm\n\n\n\nwhere \\(\\bar{v}_{i}\\) denotes the derivatives of the loss function with respect to \\(v_{i},\\) \\[\\frac{\\partial\\mathcal{L}}{\\partial v_{i}}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-18",
    "href": "slides/week-03/04-AD.html#section-18",
    "title": "Differential Programming",
    "section": "",
    "text": "Computational cost of backprop: approximately two forward passes, and hence linear in the number of unknowns\n\nBackprop is used to train the overwhelming majority of neural nets today.\nOptimization algorithms, in addition to gradient descent (e.g. second-order methods) use backprop to compute the gradients.\nBackprop can thus be used in SciML, and in particular for Digital Twins (direct and inverse problems), wherever derivatives and/or gradients need to be computed."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-19",
    "href": "slides/week-03/04-AD.html#section-19",
    "title": "Differential Programming",
    "section": "",
    "text": "AUTOGRAD"
  },
  {
    "objectID": "slides/week-03/04-AD.html#autograd",
    "href": "slides/week-03/04-AD.html#autograd",
    "title": "Differential Programming",
    "section": "Autograd",
    "text": "Autograd\nAutograd can automatically differentiate native Python and Numpy code.\n\nIt can handle a large subset of Python’s features, including loops, ifs, recursion and closures.\nIt can even take derivatives of derivatives of derivatives, etc.\nIt supports reverse-mode differentiation (a.k.a. backpropagation), which means it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments, as well as forward-mode differentiation (to compute sensitivities), and the two can be composed arbitrarily.\nThe main intended application of Autograd is gradient-based optimization.\n\nAfter a function is evaluated, Autograd has a graph specifying all operations that were performed on the inputs with respect to which we want to differentiate.\n\nThis is the computational graph of the function evaluation."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-20",
    "href": "slides/week-03/04-AD.html#section-20",
    "title": "Differential Programming",
    "section": "",
    "text": "To compute the derivative, we simply apply the basic rules of (analytical) differentiation to each node in the graph.\n\nReverse mode differentiation\n\nGiven a function made up of several nested function calls, there are several ways to compute its derivative.\nFor example, given \\[L(x)=F(G(H(x))),\\] the chain rule says that its gradient is \\[\\mathrm{d}L/\\mathrm{d}x=\\mathrm{d}F/\\mathrm{d}G*\\mathrm{d}G/\\mathrm{d}H*\\mathrm{d}H/\\mathrm{d}x.\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-21",
    "href": "slides/week-03/04-AD.html#section-21",
    "title": "Differential Programming",
    "section": "",
    "text": "If we evaluate this product from right-to-left: \\[(\\mathrm{d}F/\\mathrm{d}G*(\\mathrm{d}G/\\mathrm{d}H*\\mathrm{d}H/\\mathrm{d}x)),\\] the same order as the computations themselves were performed, this is called forward-mode differentiation.\nIf we evaluate this product from left-to-right: \\[((\\mathrm{d}F/\\mathrm{d}G*\\mathrm{d}G/\\mathrm{d}H)*\\mathrm{d}H/\\mathrm{d}x),\\] the reverse order as the computations themselves were performed, this is called reverse-mode differentiation.\n\nCompared to finite differences or forward-mode, reverse-mode differentiation is by far the more practical method for differentiating functions that take in a (very)large vectorand output a single number.\nIn the machine learning community, reverse-mode differentiation is known as ‘backpropagation’, since the gradients propagate backwards through the function (as seen above)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-22",
    "href": "slides/week-03/04-AD.html#section-22",
    "title": "Differential Programming",
    "section": "",
    "text": "It’s particularly nice since you don’t need to instantiate the intermediate Jacobian matrices explicitly, and instead only rely on applying a sequence of matrix-free vector-Jacobian productfunctions (VJPs).\nBecause Autograd supports higher derivatives as well, Hessian-vector products (a form of second-derivative) are also available and efficient to compute.\n\n\n\n\n\n\nImportant\n\n\nAutograd is now being superseded by JAX."
  },
  {
    "objectID": "slides/week-03/04-AD.html#pytorch-versus-julia",
    "href": "slides/week-03/04-AD.html#pytorch-versus-julia",
    "title": "Differential Programming",
    "section": "PyTorch versus Julia",
    "text": "PyTorch versus Julia\nWhile extremely easy to use and featured, PyTorch & Jax are walled gardens\n\nmaking it difficult integrate w/ CSE software\ngo off the beaten path\n\nIn response to the prompt “Can you list in Markdown table form pros and cons of PyTorch and Julia AD systems” ChatGTP4.0 generated the following adapted table\n\n\n\n\n\n\n\n\n\nFeature\nPyTorch\nJulia AD\n\n\n\n\nLanguage\nPython-based, widely used in ML community\nJulia, known for high performance and mathematical syntax\n\n\nPerformance\nFast, but can be limited by Python’s speed\nGenerally faster, benefits from Julia’s performance\n\n\nEase of Use\nUser-friendly, extensive documentation and community support\nSteeper learning curve, but elegant for mathematical operations"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-23",
    "href": "slides/week-03/04-AD.html#section-23",
    "title": "Differential Programming",
    "section": "",
    "text": "Feature\nPyTorch\nJulia AD\n\n\n\n\nDynamic Computation Graph\nYes, allows flexibility\nYes, with support for advanced features\n\n\nEcosystem\nExtensive, with many libraries and tools\nGrowing, with packages for scientific computing\n\n\nCommunity Support\nLarge community, well-established in industry and academia\nSmaller but growing community, strong in scientific computing\n\n\nIntegration\nEasy integration with Python libraries and tools\nGood integration w/i Julia ecosystem\n\n\nDebugging\nGood debugging tools, but can be tricky due to dynamic nature\nGood, with benefits from Julia’s compiler & type system\n\n\nParallel & GPU \nExcellent support\nExcellent, potentially faster due to Julia’s design\n\n\nMaturity\nMature, widely adopted\nLess but rapidly evolving"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-24",
    "href": "slides/week-03/04-AD.html#section-24",
    "title": "Differential Programming",
    "section": "",
    "text": "Important\n\n\nThis table highlights key aspects but may not cover all nuances. Both systems are continuously evolving, so it’s always good to check the latest developments and community feedback when making a choice.\n\n\n\n\nFor those of you interested in Julia checkout the lecture Forward- & Reverse-Mode AD by Adrian Hill"
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#inverse-problems",
    "href": "slides/week-02/03-adjoint.html#inverse-problems",
    "title": "Adjoint state",
    "section": "Inverse problems",
    "text": "Inverse problems\n\n\n\n\nfrom Asch (2022)\n\n\nIngredients of an inverse problem: the physical reality (top) and the direct mathematical model (bottom). The inverse problem uses the difference between the model- predicted observations, u (calculated at the receiver array points xr), and the real observations measured on the array, in order to find the unknown model parameters, m, or the source s (or both)."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#adjoint-state-methods",
    "href": "slides/week-02/03-adjoint.html#adjoint-state-methods",
    "title": "Adjoint state",
    "section": "Adjoint-state Methods",
    "text": "Adjoint-state Methods\nA very general approach for solving inverse problems... including Machine Learning!\nVariational DA is based on an adjoint approach.\n\n\n\n\n\n\nDefinition 1 An adjoint is a general mathematical technique, based on variational calculus, that enables the computation of the gradient of an objective, or cost functional with respect to the model parameters in a very efficient manner."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#adjoint-statecontinuous-formulation",
    "href": "slides/week-02/03-adjoint.html#adjoint-statecontinuous-formulation",
    "title": "Adjoint state",
    "section": "Adjoint-state—continuous formulation",
    "text": "Adjoint-state—continuous formulation\nLet \\(\\mathbf{u}(\\mathbf{x},t)\\) be the state of a dynamical system whose behavior depends on model parameters \\(\\mathbf{m}(\\mathbf{x},t)\\) and is described by a differential operator equation \\[\\mathbf{L}(\\mathbf{u},\\mathbf{m})=\\mathbf{f},\\] where \\(\\mathbf{f}(\\mathbf{x},t)\\) represents external forces.\nDefine a cost function \\(J(\\mathbf{m})\\) as an energy functional1 or, more commonly, as a misfit functionalthat quantifies the error (\\(L^{2}\\)-distance2) between the observation and the model prediction \\(\\mathbf{u}(\\mathbf{x},t;\\mathbf{m}).\\) For example, \\[J(\\mathbf{m})=\\int_{0}^{T}\\int_{\\Omega}\\left(\\mathbf{u}(\\mathbf{x},t;\\mathbf{m})-\\mathbf{u}^{\\mathrm{obs}}(\\mathbf{x},t)\\right)^{2}\\,\\mathrm{d}\\mathbf{x}\\:\\mathrm{d}t,\\] where \\(\\mathbf{x}\\in\\Omega\\subset\\mathbb{R}^{n},\\,n=1,2,3,\\) and \\(0\\le t\\le T.\\)\nA functional is a generalization of a function. The functional depends on functions, whereas a function depends on variables. We then say that a functional is mapping from a space of functions into the real numbers.The \\(L^{2}\\)-space is a Hilbert space of (measurable) functions that are square-integrable (in Lebesgue sense)."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#adjoint-statecontinuous-formulation-1",
    "href": "slides/week-02/03-adjoint.html#adjoint-statecontinuous-formulation-1",
    "title": "Adjoint state",
    "section": "Adjoint-state—continuous formulation",
    "text": "Adjoint-state—continuous formulation\nOur objective is to choose the model parameters \\(\\mathbf{m}\\) as a function of the observed output \\(\\mathbf{u}^{\\mathrm{obs}},\\) such that the cost function \\(J(\\mathbf{m})\\) is minimized.\nThe minimization is most frequently performed by a gradient-based method, the simplest of which is steepest gradient, though usually some variant of a quasi-Newton approach is used, see Asch (2022), Wright (2006).\nIf we can obtain an expression for the gradient, then the minimization will be considerably facilitated.\nThis is the objective of the adjoint method that provides an explicit formula for the gradient of \\(J(\\mathbf{m}).\\)"
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#adjoint-methodsoptimization-formulation",
    "href": "slides/week-02/03-adjoint.html#adjoint-methodsoptimization-formulation",
    "title": "Adjoint state",
    "section": "Adjoint methods—optimization formulation",
    "text": "Adjoint methods—optimization formulation\nSuppose we are given a (P)DE,\n\\[F(\\mathbf{u};\\mathbf{m})=0, \\tag{1}\\]\nwhere\n\n\\(\\mathbf{u}\\) is the state vector,\n\\(\\mathbf{m}\\) is the parameter vector, and\n\\(F\\) includes the partial differential operator \\(\\mathbf{L},\\) the right-hand side (source) \\(\\mathbf{f},\\) boundary and initial conditions.\n\nNote that the components of \\(\\mathbf{m}\\) can appear as any combination of\n\ncoefficients in the equation,\nthe source,\nor as components of the boundary/initial conditions."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-1",
    "href": "slides/week-02/03-adjoint.html#section-1",
    "title": "Adjoint state",
    "section": "",
    "text": "To solve this very general parameter estimation problem, we are given a cost function \\(J(\\mathbf{m};\\mathbf{u}).\\)\nThe constrained optimization problem is then \\[\\begin{cases}\n        \\mathrm{minimize}_{\\mathbf{m}} & J\\left(\\mathbf{u}(\\mathbf{m}),\\mathbf{m}\\right)\\\\\n        \\mathrm{subject\\,to} & F(\\mathbf{u};\\mathbf{m})=0,\n        \\end{cases}\\] where \\(J\\) can depend on both \\(\\mathbf{u}\\) and on \\(\\mathbf{m}\\) explicitly in the presence of eventual regularization terms.\n\n\n\n\n\n\nNote\n\n\n\nthe constraint is a (partial) differential equation and\nthe minimization is with respect to a (vector) function.\n\n\n\n\nThis type of optimization is the subject of variational calculus, a generalization of classical calculus where differentiation is performed with respect to a variable, not a function."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-2",
    "href": "slides/week-02/03-adjoint.html#section-2",
    "title": "Adjoint state",
    "section": "",
    "text": "The gradient of \\(J\\) with respect to \\(\\mathbf{m}\\) (also known as the sensitivity) is then obtained by the chain-rule, \\[\\nabla_{\\mathbf{m}}J=\\frac{\\partial J}{\\partial\\mathbf{u}}\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{m}}+\\frac{\\partial J}{\\partial\\mathbf{m}}.\\]\n\nThe partial derivatives of \\(J\\) with respect to \\(\\mathbf{u}\\) and \\(\\mathbf{m}\\) are readily computed from the expression for \\(J,\\)\nbut the derivative of \\(\\mathbf{u}\\) with respect to \\(\\mathbf{m}\\) requires a potentially very large number of evaluations, corresponding to the product of the dimensions of \\(\\mathbf{u}\\) and \\(\\mathbf{m}\\) that can both be very large.\n\nThe adjoint method is a way to avoid calculating all of these derivatives.\nWe use the fact that if \\(F(\\mathbf{u};\\mathbf{m})=0\\) everywhere, then this implies that the total derivative of \\(F\\) with respect to \\(\\mathbf{m}\\) is equal to zero everywhere too."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-3",
    "href": "slides/week-02/03-adjoint.html#section-3",
    "title": "Adjoint state",
    "section": "",
    "text": "Differentiating the PDE 1, we can thus write \\[\\frac{\\partial F}{\\partial\\mathbf{u}}\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{m}}+\\nabla_{\\mathbf{m}}F=0.\\]\nThis can be solved for the untractable derivative of \\(\\mathbf{u}\\) with respect to \\(\\mathbf{m},\\) to give \\[\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{m}}=-\\left(\\frac{\\partial F}{\\partial\\mathbf{u}}\\right)^{-1}\\nabla_{\\mathbf{m}}F\\] assuming that the inverse of \\(\\partial F/\\partial{\\mathbf{u}}\\) exists.\nSubstituting in the expression for the gradient of \\(J,\\) we obtain \\[\\begin{aligned}\n    \\nabla_{\\mathbf{m}}J & =-\\frac{\\partial J}{\\partial\\mathbf{u}}\\left(\\frac{\\partial F}{\\partial\\mathbf{u}}\\right)^{-1}\\nabla_{\\mathbf{m}}F+\\frac{\\partial J}{\\partial\\mathbf{m}},\\\\\n     & =\\mathbf{p}\\nabla_{\\mathbf{m}}F+\\frac{\\partial J}{\\partial\\mathbf{m}}\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-4",
    "href": "slides/week-02/03-adjoint.html#section-4",
    "title": "Adjoint state",
    "section": "",
    "text": "where we have conveniently defined \\(\\mathbf{p}\\) as the solution of the adjoint equation \\[\\left(\\frac{\\partial F}{\\partial\\mathbf{u}}\\right)^{\\mathrm{T}}\\mathbf{p}=-\\frac{\\partial J}{\\partial\\mathbf{u}}.\\]"
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#adjoint-methodssumming-up",
    "href": "slides/week-02/03-adjoint.html#adjoint-methodssumming-up",
    "title": "Adjoint state",
    "section": "Adjoint Methods—summing up",
    "text": "Adjoint Methods—summing up\nIn summary, we have a three-step procedure that combines a model-based approach (through the PDE) with a data-driven approach (through the cost function):\n\nSolve the adjoint equation for the adjoint state, \\(\\mathbf{p}.\\)\nUsing the adjoint state, compute the gradient of the cost function \\(J.\\)\nUsing the gradient, solve the optimization problem to estimate the parameters \\(\\mathbf{m}\\) that minimize the mismatch between model and observations.\n\n\n\n\n\n\n\nImportant\n\n\nThis key result enables us to compute the desired gradient \\(\\nabla_\\mathbf{m}J\\), without the explicit knowledge of the variation of \\(\\mathbf{u}\\)."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-5",
    "href": "slides/week-02/03-adjoint.html#section-5",
    "title": "Adjoint state",
    "section": "",
    "text": "A number of important remarks can be made.\n\nWe obtain explicit formulasin terms of the adjoint statefor the gradient with respect to each/any model parameter. Note that this has been done in a completely general setting, without any restrictions on the operator \\({F}\\) or on the model parameters \\(\\mathbf{m}.\\)\nThe computational cost is one solution of the adjoint equation which is usually of the same order as (if not identical to) the direct equation, but with a reversal of time. Note that for nonlinear equations this may not be the case and one may require four or five times the computational effort.\nThe variation (Gâteaux derivative) of \\({F}\\) with respect to the model parameters \\(\\mathbf{m}\\) is, in general, straightforward to compute.\nWe have not explicitly considered boundary (or initial) conditions in the above, general approach. In real cases, these are potential sources of difficulties for the use of the adjoint approach the discrete adjoint can provide a way to overcome this hurdle."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-6",
    "href": "slides/week-02/03-adjoint.html#section-6",
    "title": "Adjoint state",
    "section": "",
    "text": "For complete mathematical rigor, the above development should be performed in an appropriate Hilbert space setting that guarantees the existence of all the inner products and adjoint operators. The interested reader could consult Tröltzsch (2010).\nIn many real problems, the optimization of the misfit functional leads to multiple local minima and often to very “flat” cost functions. These are hard problems for gradient-based optimization methods. These difficulties can be (partially) overcome by a panoply of tools:\n\nRegularization terms can alleviate the non-uniqueness problem.\nRescaling the parameters and/or variables in the equations can help with the “flatness.” This technique is often employed in numerical optimization.\nHybrid algorithms, that combine stochastic and deterministic optimization (e.g., Simulated Annealing), can be used to avoid local minima.\nJudicious use of machine learning techniques and methods."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#section-7",
    "href": "slides/week-02/03-adjoint.html#section-7",
    "title": "Adjoint state",
    "section": "",
    "text": "When measurement and modeling errors can be modeled by Gaussian distributions and a background (prior) solution exists, the objective function may be generalized by including suitable covariance matrices. This is the approach employed systematically in data assimilationsee below."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#adjoint-state-methoddiscrete-systems",
    "href": "slides/week-02/03-adjoint.html#adjoint-state-methoddiscrete-systems",
    "title": "Adjoint state",
    "section": "Adjoint-state method—Discrete systems",
    "text": "Adjoint-state method—Discrete systems\nIn practice, we often work with discretized systems of equations, e.g. discretized PDEs.\nAssume that \\(\\mathbf{x}\\) depends, as usual on a parameter vector, \\(\\mathbf{m}\\), made up of \\(p\\)\n\ncontrol variables, design parameters, or decision parameters\n\nGoal: To optimize these values for a given cost function, \\(J(\\mathbf{x}, \\mathbf{m})\\), we need to\n\nevaluate \\(J(\\mathbf{x}, \\mathbf{m})\\) and compute, as for the continuous case, the gradient \\(\\mathrm{d}J/\\mathrm{d}\\mathbf{m}\\).\n\nPossible with an adjoint-state method at a cost that is independent of \\(p\\).\nInvolves inversion of linear system at \\(\\mathcal{O}(n^3)\\) operations — making it computational feasible to deal with problems of this type\n\\[A[\\mathbf{m}]\\mathbf{x}=\\mathbf{b} \\tag{2}\\]\nwhere \\(A[\\mathbf{m}]\\) represents the discretized PDE, \\(\\mathbf{m}\\) spatially varying coefficients of the PDE, and \\(\\mathbf{b}\\) a source term."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#automatic-differentiation",
    "href": "slides/week-02/03-adjoint.html#automatic-differentiation",
    "title": "Adjoint state",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nGradients w.r.t \\(\\mathbf{m}\\) of objectives that include \\(A[\\mathbf{m}]\\), e.g.\n\\[J(\\mathbf{m})=\\frac{1}{2}\\|\\mathbf{b}-PA^{-1}[\\mathbf{m}]\\mathbf{q}\\|^2_2\\]\ncan be calculated with the adjoint-state method leading to\n\nfast and scalable codes (e.g. Full-waveform inversion)\nbut coding can be involved especially when combined with machine learning (e.g. when \\(\\mathbf{m}\\) is parameterized by a neural network)\n\nUse automatic differentiation (AD) tools instead of analytical tools.\nTwo approaches to compute the adjoint state and the resulting gradients\n\nContinuous case — discretization of the (analytical) adjoint, denoted by AtD = adjoint then discretize;\nDiscrete case — adjoint of the discretization (the code), denoted as DtA = discretize then adjoint."
  },
  {
    "objectID": "slides/week-02/03-adjoint.html#problems-ad",
    "href": "slides/week-02/03-adjoint.html#problems-ad",
    "title": "Adjoint state",
    "section": "Problems AD",
    "text": "Problems AD\nRealistic (complex and large scale) solved with DtA using AD — provided by ML\n\nPyTorch, TensorFlow (Python), or Flux.jl/Zygote.jl (Julia)\n\n\n\n\n\n\n\nCaution\n\n\nWhile convenient, AD as part of ML tools (known as backpropagation)\n\nmay not scale\nintegrates poorly with existing CSE codes\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nSolution: Combine hand-derived gradients (via the adjoint-state method) w/ Automatic Differentiation\n\nuse ChainRules.jl to include hand-derived gradients in AD systems (e.g. Zygote.jl)\nknown as intrusive automatic differentiation (Li et al. 2020), see e.g. Louboutin et al. (2023) for Julia implementation\n\n\n\n\nWe will devote a separate lecture on AD."
  },
  {
    "objectID": "slides/week-05/09-UQ.html#uncertainty-quantification",
    "href": "slides/week-05/09-UQ.html#uncertainty-quantification",
    "title": "Uncertainty Quantification",
    "section": "Uncertainty Quantification",
    "text": "Uncertainty Quantification\nDefinition 11.1 (UQ). UQ is the composite task of assessing uncertainty in the computational estimate of a given quantity of interest (QoI).\nDefinition 11.2 (QoI). In an uncertainty quantification problem, we seek statistical information about a chosen output of interest. This statistic is know as the quantity of interest (QoI) and is usually defined by a high-dimensional integral. The QoI accounts for uncertainty in the model or process.\nWe can decompose UQ into a number of subtasks:\n\nQuantify uncertainties in model inputs by specifying probability distributions.\nPropagate input uncertainties through the model and quantify effects on the QoI.\nQuantify variability in the true physical QoI\nAggregate uncertainties arising from different sources to appraise overall uncertainty (input, model, numerical errors, inherent variability)\nCompute sensitivities of output variables with respect to input variables (forward UQ)"
  },
  {
    "objectID": "slides/week-05/09-UQ.html#bayesian-parameter-estimation-in-state-space-models",
    "href": "slides/week-05/09-UQ.html#bayesian-parameter-estimation-in-state-space-models",
    "title": "Uncertainty Quantification",
    "section": "Bayesian Parameter Estimation in State Space Models",
    "text": "Bayesian Parameter Estimation in State Space Models\n\\[\n\\begin{aligned}\n\\boldsymbol{\\theta} & \\sim p(\\boldsymbol{\\theta}) \\\\\n\\mathbf{x}_0 & \\sim p\\left(\\mathbf{x}_0 \\mid \\boldsymbol{\\theta}\\right) \\\\\n\\mathbf{x}_k & \\sim p\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}, \\boldsymbol{\\theta}\\right) \\\\\n\\mathbf{y}_k & \\sim p\\left(\\mathbf{y}_k \\mid \\mathbf{x}_k, \\boldsymbol{\\theta}\\right), \\quad k=0,1,2, \\ldots\n\\end{aligned}\n\\]\nApplying Bayes’ Law of Theorem 2.61, we can compute the complete posterior distribution of the state plus parameters as \\[\np\\left(\\mathbf{x}_{0: T}, \\boldsymbol{\\theta} \\mid \\mathbf{y}_{1: T}\\right)=\\frac{p\\left(\\mathbf{y}_{1: T} \\mid \\mathbf{x}_{0: T}, \\boldsymbol{\\theta}\\right) p\\left(\\mathbf{x}_{0: T} \\mid \\boldsymbol{\\theta}\\right) p(\\boldsymbol{\\theta})}{p\\left(\\mathbf{y}_{1: T}\\right)},\n\\] where the joint prior of the states, \\(\\mathbf{x}_{0: T}=\\left\\{\\mathbf{x}_0, \\ldots, \\mathbf{x}_T\\right\\}\\), and the joint likelihood of the measurements, \\(\\mathbf{y}_{1: T}=\\left\\{\\mathbf{y}_1, \\ldots, \\mathbf{y}_T\\right\\}\\), conditioned on the parameters, are"
  },
  {
    "objectID": "slides/week-05/09-UQ.html#section",
    "href": "slides/week-05/09-UQ.html#section",
    "title": "Uncertainty Quantification",
    "section": "",
    "text": "\\[\np\\left(\\mathbf{x}_{0: T} \\mid \\boldsymbol{\\theta}\\right)=p\\left(\\mathbf{x}_0 \\mid \\boldsymbol{\\theta}\\right) \\prod_{k=1}^T p\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}, \\boldsymbol{\\theta}\\right)\n\\] and \\[\np\\left(\\mathbf{y}_{1: T} \\mid \\mathbf{x}_{0: T}, \\boldsymbol{\\theta}\\right)=\\prod_{k=1}^T p\\left(\\mathbf{y}_k \\mid \\mathbf{x}_k, \\boldsymbol{\\theta}\\right)\n\\] respectively. Finally, we need to marginalize over the state to obtain the predictive posterior for the parameter \\(\\boldsymbol{\\theta}\\), \\[\np\\left(\\boldsymbol{\\theta} \\mid \\mathbf{y}_{1: T}\\right)=\\int p\\left(\\mathbf{x}_{0: T}, \\boldsymbol{\\theta} \\mid \\mathbf{y}_{1: T}\\right) \\mathrm{d} \\mathbf{x}_{0: T}\n\\]\nIntegrals are difficult to compute — will seek iterative (learned) methods to capture statistics\n\n\n\n🔗 https://flexie.github.io/CSE-8803-Twin/"
  },
  {
    "objectID": "slides/week-01/01-intro.html#course-outline",
    "href": "slides/week-01/01-intro.html#course-outline",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Course outline",
    "text": "Course outline\nThis is a new advanced course that is being developed during this term.\n\nSyllabus\nSchedule\netc.\n\nare all made available and constantly updated on\nhttps://flexie.github.io/CSE-8803-Twin//"
  },
  {
    "objectID": "slides/week-01/01-intro.html#objectives",
    "href": "slides/week-01/01-intro.html#objectives",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Objectives",
    "text": "Objectives\nBy the end of the semester, you will be made familiar with …\n\nthe concept of Digital Twins and how they interact with their environment\nStatistical Inverse Problems and Bayesian Inference\ntechniques from Data Assimilation (DAT), Simulation-Based Inference (SBI), and Recursive Bayesian Inference (RBI), and Uncertainty Quantification [UQ]\n\nFor more on the Course outline, Topics, and Learning goals, see Goals."
  },
  {
    "objectID": "slides/week-01/01-intro.html#textbook",
    "href": "slides/week-01/01-intro.html#textbook",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Textbook",
    "text": "Textbook\n\n\n\nData assimilation: methods, algorithms, and applications\nAsch, Bocquet, and Nodet (2016), Mark\nSIAM, 2016\n\n\nA toolbox for digital twins: from model-based to data-driven1\nAsch (2022), Mark\nSIAM, 2022\n\n\n\nThis 1000 page book is rather comprehensive. While the complete material is beyond this course, you are encouraged to use the extensive cross-referencing in this book to your advantage."
  },
  {
    "objectID": "slides/week-01/01-intro.html#journal-papers",
    "href": "slides/week-01/01-intro.html#journal-papers",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Journal Papers",
    "text": "Journal Papers\n\n\n\nA comprehensive review of digital twin—part 1: modeling and twinning enabling technologies\nThelen et al. (2022)\nSpringer, 2022\n\n\nA comprehensive review of digital twin—part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives\nThelen et al. (2023)\nSpringer, 2022\n\n\nSequential Bayesian inference for uncertain nonlinear dynamic systems: A tutorial\nTatsis, Dertimanis, and Chatzi (2022)\nArxiv, 2022"
  },
  {
    "objectID": "slides/week-01/01-intro.html#introduction",
    "href": "slides/week-01/01-intro.html#introduction",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Introduction",
    "text": "Introduction\n\nDifferent definitions of Digital Twins\nData Flows\nDimensions Digital Twins\nthe Inference Cycle"
  },
  {
    "objectID": "slides/week-01/01-intro.html#definition-of-digital-twin",
    "href": "slides/week-01/01-intro.html#definition-of-digital-twin",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Definition of Digital Twin",
    "text": "Definition of Digital Twin\nDefinition from (Asch 2022): “A set of virtual information constructs that mimics the structure, context, and behavior of an individual/unique physical asset, or a group of physical assets, is dynamically updated with data from its physical twin throughout its life cycle and informs decisions that realize value.”\n\nDefinition by the Aerospace Industries Association\nMirroring physical assets in a dynamic manner\n\nDefinition by IBM: “A digital twin is a virtual representation of an object or system that spans its lifecycle, is updated from real-time data, and uses simulation, machine learning and reasoning to help decision making.”"
  },
  {
    "objectID": "slides/week-01/01-intro.html#definition-of-digital-twin-conted",
    "href": "slides/week-01/01-intro.html#definition-of-digital-twin-conted",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Definition of Digital Twin (cont’ed)",
    "text": "Definition of Digital Twin (cont’ed)\nDefinition from (National Academies of Sciences, Medicine, et al. 2023): “A digital twin is a set of virtual information constructs that mimics the structure, context, and behavior of a natural, engineered, or social system (or system-of-systems), is dynamically updated with data from its physical twin, has a predictive capability, and informs decisions that realize value. The bidirectional interaction between the virtual and the physical is central to the digital twin.”\nAlso see discussion Section 2 of “A comprehensive review of digital twin — part 1”."
  },
  {
    "objectID": "slides/week-01/01-intro.html#cyber-physical-systems-cps",
    "href": "slides/week-01/01-intro.html#cyber-physical-systems-cps",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Cyber-Physical Systems (CPS)",
    "text": "Cyber-Physical Systems (CPS)\n\n\n\nModel Systems of Systems (SoS) by equations\nHow the two worlds—digital and physical—intertwine?\nHow does the digital inform the physical, and how does the physical shape our understanding of digital processes?\n\n\n\n\n\nSource"
  },
  {
    "objectID": "slides/week-01/01-intro.html#data-flows",
    "href": "slides/week-01/01-intro.html#data-flows",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Data flows",
    "text": "Data flows\n\n\n\nFor a digital model, data flow between the physical space and virtual space is optional\nFor a digital shadow, data flow is unidirectional from physical to digital.\nBut for digital twin, the data flow has to be bidirectional. See Figure.\n\n\n\n\n\nfrom (Thelen et al. 2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#five-dimensional-digital-twin",
    "href": "slides/week-01/01-intro.html#five-dimensional-digital-twin",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Five dimensional Digital Twin",
    "text": "Five dimensional Digital Twin\n\n\n\\[\\mathrm{DT} = \\mathbb{F} (\\mathrm{PS, DS, P2V, V2P, OPT})\\]\nfive- dimensional digital twin model consists of\n\na physical system (PS),\na digital system (DS),\nan updating engine (P2V),\na prediction engine (V2P),\nand an optimization dimension (OPT).\n\n\\(\\mathbb{F}(⋅)\\) integrates all five dimensions together to define a Digital Twin.\n\n\n\n\nfrom (Thelen et al. 2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#five-dimensions",
    "href": "slides/week-01/01-intro.html#five-dimensions",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Five dimensions",
    "text": "Five dimensions\n\\[\\mathrm{DT} = \\mathbb{F} (\\mathrm{PS, DS, P2V, V2P, OPT})\\]\n\nfrom (Thelen et al. 2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#the-inference-cycle",
    "href": "slides/week-01/01-intro.html#the-inference-cycle",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "The Inference Cycle",
    "text": "The Inference Cycle\n\n\n\nScientific method — inferential process\nAbduction — going from (unexplained) effect to (possible) cause\nDeduction — going from cause to effect\nInduction — going from specific to general\n\n\n\n\n\nSource Asch (2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#the-concept-of-a-digital-twin",
    "href": "slides/week-01/01-intro.html#the-concept-of-a-digital-twin",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "The Concept of a Digital Twin",
    "text": "The Concept of a Digital Twin\n\nthe availability of (large) volumes of (often real-time) data,\nthe accessibility to this data,\nthe tools and implementations of AI-based algorithms,\nthe body of knowledge of mathematical models,\nthe readiness and low cost of computational devices,\n\nNecessary ingredients for a Digital Twin that learns during its life cycle\n\nconsists of static part — initial model, design, and\ndynamic part — includes the simulation process, coupled with data acquisition, and finally autoupdating."
  },
  {
    "objectID": "slides/week-01/01-intro.html#the-spectrum-of-digital-twins",
    "href": "slides/week-01/01-intro.html#the-spectrum-of-digital-twins",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "The Spectrum of Digital Twins",
    "text": "The Spectrum of Digital Twins\n\nFrom model-driven to data-driven\nImportance of models, data, and competencies"
  },
  {
    "objectID": "slides/week-01/01-intro.html#digital-twins-in-the-digital-continuum",
    "href": "slides/week-01/01-intro.html#digital-twins-in-the-digital-continuum",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Digital Twins in the Digital Continuum",
    "text": "Digital Twins in the Digital Continuum\n\n\n\nInteraction with digital infrastructure\nCloud computing, IoT, and cybersecurity\nEmphasis in this course will be on monitoring & control of physical systems ruled by PDEs\n\ngeological CO2 storage\nbattery life\n\n\n\n\n\n\nSource Asch (2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#geological-carbon-storage",
    "href": "slides/week-01/01-intro.html#geological-carbon-storage",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Geological Carbon Storage",
    "text": "Geological Carbon Storage\n\n\ncoupling of fluid-flow physics and\nwave physics"
  },
  {
    "objectID": "slides/week-01/01-intro.html#models-data-and-coupling-in-digital-twins",
    "href": "slides/week-01/01-intro.html#models-data-and-coupling-in-digital-twins",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Models, Data, and Coupling in Digital Twins",
    "text": "Models, Data, and Coupling in Digital Twins\nQuestions:\n\nWhat is meant by a “model”?\nWhat are data to be used for?\nHow, if possible, can we couple the above two to construct the most informative DT?\n\nWill be discussing two types of models throughout the book:\n\nEquation-based models that are derived, most often, from some conservation laws.\nStatistical, data-driven models that are based on measured data and its analysis.\n\nA good statistical model: should attain a good balance between underfitting.\nPhysical models: need to capture the higher order terms and neglect small terms\nBloated models will often be difficult to solve numerically."
  },
  {
    "objectID": "slides/week-01/01-intro.html#examples-and-use-cases",
    "href": "slides/week-01/01-intro.html#examples-and-use-cases",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Examples and Use Cases",
    "text": "Examples and Use Cases\n\nPredictive maintenance, personalized medicine, sports, agriculture, geophysics"
  },
  {
    "objectID": "slides/week-01/01-intro.html#recommended-approachthe-triple-loop-method",
    "href": "slides/week-01/01-intro.html#recommended-approachthe-triple-loop-method",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Recommended Approach—The Triple-Loop Method",
    "text": "Recommended Approach—The Triple-Loop Method\nThree loops: Space and time, optimization, decision-making\n\nLoops over space and time and solution of the physical problem in the inner loop.\nOptimization in the outer loop, including control, solution of an inverse problem, parameter estimation, uncertainty quantification, and multifidelity modeling using surrogates.\nDecision making in the outer-outer loop— preventative maintenance, shutting down operations …\n\nImportant to ensure trustworthiness of the DT - checking operational and validity regimes of the model, and - implement an expert system based on engineering experience."
  },
  {
    "objectID": "slides/week-01/01-intro.html#future-directions",
    "href": "slides/week-01/01-intro.html#future-directions",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Future Directions",
    "text": "Future Directions\n\n\n\nEvolution from simple to AI-integrated systems\nTheoretical and practical advancements\nFacilitates a more general interpretation where we can map machine learning techniques, or AI, to any of the stages\n\n\n\n\n\nfrom Asch (2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#references",
    "href": "slides/week-01/01-intro.html#references",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "References",
    "text": "References\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. SIAM.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. SIAM.\n\n\nNational Academies of Sciences, Engineering, Medicine, et al. 2023. “Foundational Research Gaps and Future Directions for Digital Twins.”\n\n\nTatsis, Konstantinos E, Vasilis K Dertimanis, and Eleni N Chatzi. 2022. “Sequential Bayesian Inference for Uncertain Nonlinear Dynamic Systems: A Tutorial.” arXiv Preprint arXiv:2201.08180.\n\n\nThelen, Adam, Xiaoge Zhang, Olga Fink, Yan Lu, Sayan Ghosh, Byeng D Youn, Michael D Todd, Sankaran Mahadevan, Chao Hu, and Zhen Hu. 2022. “A Comprehensive Review of Digital Twin—Part 1: Modeling and Twinning Enabling Technologies.” Structural and Multidisciplinary Optimization 65 (12): 354.\n\n\n———. 2023. “A Comprehensive Review of Digital Twin—Part 2: Roles of Uncertainty Quantification and Optimization, a Battery Digital Twin, and Perspectives.” Structural and Multidisciplinary Optimization 66 (1): 1.\n\n\n\n\n\n🔗 https://flexie.github.io/CSE-8803-Twin/"
  },
  {
    "objectID": "goals.html",
    "href": "goals.html",
    "title": "Goal",
    "section": "",
    "text": "Goal\nThe overall goal of this course is to bring you to where the current literature is regarding the use of Digital Twins to\n\nmonitor physical systems from indirect measurements\nassess uncertainty\ncontrol the system\n\nThe course will start with introducing topics from traditional Data Assimilation (DA) and Bayesian inference and will make it through to the latest developments in Differential Programming (DP), Simulation-Based Inference (SBI), recursive Bayesian Inference (RBI), and learned RBI through the use of Generative AI.\n\n\nCourse outline\n\nIntroduction\n\nwelcome\noverview Digital Twins\n\nInverse Problems\n\nill-posedness\nTikhonov regularization\nGeneral Formulation\nDiscrepancy principle\nCross-validation\n\nBasic Data Assimilation\n\nintroduction\nadjoint state method\nvariational data assimilation\n\nStatistical Inverse Problems\ndifferential programming\n\nreverse-mode = adjoint state\n\nAdvanced Data Assimilation\nNeural Density Estimation\n\ngenerative Networks\nNormalizing Flows\nconditional Normalizing Flows\n\nSimulation-based inference\n\nintroduction scientific ML\nBayesian inference\n\nSurrogate Modeling\n\nFourier Neural Operators FNOs\n\nLearned Data Assimilation\n\n\n\nTopics\n\n\nLearning goals\n\n\n\n\nReuseCC-BY",
    "crumbs": [
      "Goals"
    ]
  },
  {
    "objectID": "hw/w3-hw02.html",
    "href": "hw/w3-hw02.html",
    "title": "HW 02: Data visualization",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 03",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nDifferential Programming\n\n\nAutomatic Differentiation\n\n\nMon, Jan 22\n\n\n\n\nVariational Data Assimilation\n\n\nData Assimilation\n\n\nWed, Jan 31\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nDifferentiable Programming\n\n\nFri, Sep 16\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 9 — A toolbox for digital twins sections until section 9.4\n\n\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 05",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nStatistical Inverse Problems\n\n\nBayesian Inference\n\n\nMon, Jan 29\n\n\n\n\nData Assimilation\n\n\nstatistical data assimilation\n\n\nWed, Jan 31\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nDifferentiable Programming\n\n\nFri, Sep 16\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 8 — A toolbox for digital twins section 8.8\n\n\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/w2-lab01-PyTorch.html",
    "href": "labs/w2-lab01-PyTorch.html",
    "title": "PyTorch intro",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/w1-lab01-intro.html",
    "href": "labs/w1-lab01-intro.html",
    "title": "First Lab",
    "section": "",
    "text": "Rerun the examples of this Notebook and redo the experiment for \\(y=-2\\tan(\\pi x)\\).\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html",
    "href": "labs/02Examples/pytorch/pytorch_101.html",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Based on Bourkes’s https://www.learnpytorch.io/"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#tensors-are-everywhere",
    "href": "labs/02Examples/pytorch/pytorch_101.html#tensors-are-everywhere",
    "title": "Introduction to PyTorch",
    "section": "1. Tensors are everywhere",
    "text": "1. Tensors are everywhere\nTensors are the fundamental building blocks of machine learning.\n\nimport torch\ntorch.__version__\n\n'1.13.1'\n\n\nStart by creating basic tensors\n\nscalar\nvector\nmatrix\ntensor\n\nPlease see official doc at https://pytorch.org/docs/stable/tensors.html\n\n# scalar\nscalar = torch.tensor(7)\nscalar\n\ntensor(7)\n\n\n\nscalar.ndim\n\n0\n\n\nTo retrieve the value, use the item method\n\nscalar.item()\n\n7\n\n\n\n# vector\nvector = torch.tensor([7, 7])\nvector\n\ntensor([7, 7])\n\n\n\nvector.ndim\n\n1\n\n\n\nvector.shape\n\ntorch.Size([2])\n\n\n\nndim gives the number of external square brackets\nshape gives the actual dimension = length\n\n\n# matrix\nMAT = torch.tensor([[7, 8], [9, 10]])\nMAT\n\ntensor([[ 7,  8],\n        [ 9, 10]])\n\n\n\nMAT.ndim\n\n2\n\n\n\nMAT.shape\n\ntorch.Size([2, 2])\n\n\n\n# tensor\nTEN = torch.tensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\nTEN\n\ntensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\n\n\n\nTEN.shape\n\ntorch.Size([1, 3, 3])\n\n\n\nTEN.ndim\n\n3\n\n\nThe dimensions of a tensor go from outer to inner. That means there’s 1 dimension of 3 by 3."
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#random-valued-tensors",
    "href": "labs/02Examples/pytorch/pytorch_101.html#random-valued-tensors",
    "title": "Introduction to PyTorch",
    "section": "Random-valued Tensors",
    "text": "Random-valued Tensors\nTensors of random numbers are very common in ML. They are used evrywhere.\n\nrand_tensor = torch.rand(size=(3, 4))\nrand_tensor, rand_tensor.dtype\n\n(tensor([[0.2350, 0.7880, 0.7052, 0.5025],\n         [0.5523, 0.6008, 0.9949, 0.4443],\n         [0.5769, 0.7505, 0.1360, 0.8363]]),\n torch.float32)"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#other-special-tensors",
    "href": "labs/02Examples/pytorch/pytorch_101.html#other-special-tensors",
    "title": "Introduction to PyTorch",
    "section": "Other special tensors",
    "text": "Other special tensors\n\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n\n(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n torch.float32)\n\n\n\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n\n(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n torch.float32)\n\n\nCreate a range of numbers in a tensor.\ntorch.arange(start, end, step)\n\nzero_to_ten = torch.arange(0,10)\nzero_to_ten\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nTo get a tensor of the same shape as another.\ntorch.zeros_like(input)\n\nten_zeros = torch.zeros_like(zero_to_ten)\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nTensor datatypes\nMany datatypes are available, some specific for CPUs, others better for GPUs.\nDefault datatype is a float32, defined by torch.float32() or just torch.float().\nLower precision values are faster to compute with, but less acccurate…\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='cpu'))\n\n\nLet us place a tensor on the GPU (usually “cuda”, bit here it is “mps” for a Mac)\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=\"mps\", # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='mps', index=0))\n\n\nMost common issues for mismatch are:\n\nshape differences\ndatatype\ndevice issues\n\n\nfloat_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n\ntorch.float16\n\n\n\n\nGet info from tensors\nThis is often necessary to ensure compatibility and to avoid pesky bugs.\n\n# Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n\ntensor([[0.7783, 0.1803, 0.1316, 0.2174],\n        [0.5707, 0.7213, 0.5195, 0.5730],\n        [0.6286, 0.9001, 0.8025, 0.5707]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n\nTensor operations\n\nBasic operations\nMatrix multiplication\nAggregation (min, max, mean, etc.)\nReshaping, squeezing\n\n\n# Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# Multiply it by 10\ntensor * 10\n\ntensor([10, 20, 30])\n\n\n\n# Can also use torch functions\ntm = torch.mul(tensor, 10)\nta = torch.add(tensor, 10)\n\nprint(\"tm = \", tm)\nprint(\"ta = \", ta)\n\ntm =  tensor([10, 20, 30])\nta =  tensor([11, 12, 13])\n\n\n\n# Element-wise multiplication \n# (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n\ntensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n\n\n\ntensor = torch.tensor([1, 2, 3])\n# Element-wise matrix multiplication\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\n\n# Matrix multiplication\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\nBuilt-in torch.matmul() is much faster and should always be used.\n\n%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n\nCPU times: user 1.16 ms, sys: 738 µs, total: 1.89 ms\nWall time: 1.31 ms\n\n\ntensor(14)\n\n\n\n%%time\ntorch.matmul(tensor, tensor)\n\nCPU times: user 310 µs, sys: 85 µs, total: 395 µs\nWall time: 368 µs\n\n\ntensor(14)\n\n\nOf course, shapes must be compatible for matrix multiplication…\n\n# Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)\n\n\n\n# View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n\n\n\n# The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n\nOriginal shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n\n\n\n# torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#tensor-multiplication-example-of-linear-regression",
    "href": "labs/02Examples/pytorch/pytorch_101.html#tensor-multiplication-example-of-linear-regression",
    "title": "Introduction to PyTorch",
    "section": "Tensor multiplication: example of linear regression",
    "text": "Tensor multiplication: example of linear regression\nNeural networks are full of matrix multiplications and dot products.\nThe torch.nn.Linear() module (that we will see in the next pytorch tutorial), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input \\(x\\) and a weights matrix \\(A.\\)\n\\[ y = x A^T + b, \\]\nwhere\n\n\\(x\\) is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other).\n\\(A\\) is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the “T”, that’s because the weights matrix gets transposed). Note: You might also often see \\(W\\) or another letter like \\(X\\) used to denote the weights matrix.\n\\(b\\) is the bias term used to slightly offset the weights and inputs.\n\\(y\\) is the output (a manipulation of the input in the hope to discover patterns in it).\n\nThis is just a linear function, of type \\(y = ax+b,\\) that can be used to draw a straight line.\n\n# Since the linear layer starts with a random weights matrix, we make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2,  # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n\nInput shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#aggregation-functions",
    "href": "labs/02Examples/pytorch/pytorch_101.html#aggregation-functions",
    "title": "Introduction to PyTorch",
    "section": "Aggregation Functions",
    "text": "Aggregation Functions\nNow for some aggregation functions.\n\n# Create a tensor\nx = torch.arange(0, 100, 10)\nx\n\ntensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n\n\n\nprint(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n\nMinimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n\n\n\n# alternative: use torch methods\ntorch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n\n(tensor(90), tensor(0), tensor(45.), tensor(450))\n\n\nPositional min/max functions are\n\ntorch.argmin()\ntorch.argmax()\n\n\n# Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n\nTensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n\n\nChanging datatype is possible (recasting)\n\n# Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n\ntorch.float32\n\n\n\n# Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n\ntensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)\n\n\n\n# Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n\ntensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)\n\n\n\nReshaping, stacking, squeezing of tensors\n\ntorch.reshape(input, shape)\ntorch.Tensor.view(shape) to obtain a view\ntorch.stack(tensors, dim=0) to concatenate along a given direction\ntorch.squeeze(input) to remove all dimensions of value 1\ntorch.unsqueeze(input, dim) to add a dimension of value 1 at dim\ntorch.permute(input, dims) to permute to dims\n\n\n# Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))\n\n\n\n# Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\n\n# Change view (keeps same data as original but changes view)\nz = x.view(1, 7)\nz, z.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\n\n# Changing z changes x\nz[:, 0] = 5\nz, x\n\n(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))\n\n\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])\n\n\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=1) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 5., 5., 5.],\n        [2., 2., 2., 2.],\n        [3., 3., 3., 3.],\n        [4., 4., 4., 4.],\n        [5., 5., 5., 5.],\n        [6., 6., 6., 6.],\n        [7., 7., 7., 7.]])\n\n\n\nprint(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n\nPrevious tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n\n\n\n# Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n\nPrevious shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#slicing",
    "href": "labs/02Examples/pytorch/pytorch_101.html#slicing",
    "title": "Introduction to PyTorch",
    "section": "Slicing",
    "text": "Slicing\nOften we need to extract subsets of data from tensors, usually, some rows or columns.\nLet’s look at indexing.\n\n# Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n\n(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n torch.Size([1, 3, 3]))\n\n\n\n# Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n\nFirst square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n\n\n\n# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n\ntensor([[2, 5, 8]])\n\n\n\nPyTorch tensors and NumPy\nWe often need to interact with numpy, especially for numerical computations.\nThe two main methods are:\n\ntorch.from_numpy(ndarray)\ntorch.Tensor.numpy()\n\n\n# NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n\n(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\n\n# many pytorch calculations require 'float32'\ntensor32 = torch.from_numpy(array).type(torch.float32)\ntensor32, tensor32.dtype\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.float32)\n\n\n\n# Tensor to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n\n(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#reproducibility",
    "href": "labs/02Examples/pytorch/pytorch_101.html#reproducibility",
    "title": "Introduction to PyTorch",
    "section": "2. Reproducibility",
    "text": "2. Reproducibility\nTo ensure reproducibility of computations, especially ML training, we need to set the random seed.\n\nimport torch\n#import random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n\nTensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n\n\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#running-computations-on-the-gpu",
    "href": "labs/02Examples/pytorch/pytorch_101.html#running-computations-on-the-gpu",
    "title": "Introduction to PyTorch",
    "section": "3. Running computations on the GPU",
    "text": "3. Running computations on the GPU\nSee also the code in pytorch_M2.ipynb.\nUsually the command sequence is:\n# Check for GPU\nimport torch\ntorch.cuda.is_available()\n# Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n# check for gpu on mac\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu'\ndevice\n\n'mps'\n\n\nTo put tensors on the GPU, just use the method tensor.to(device), or put the device option directly into the tensor initialization as seen above:\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None,  \n                               device=\"mps\",  \n                               requires_grad=False)  \n\n# Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n\ntensor([1, 2, 3]) cpu\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\nIf you need to interact with your tensors (numpy, matplotlib, etc.), then need to get them back to the CPU. Here we use the method Tensor.cpu()\n\n# If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n\nTypeError: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n\n\n\n# Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n\narray([1, 2, 3])\n\n\n\n# original is still on the GPU\ntensor_on_gpu\n\ntensor([1, 2, 3], device='mps:0')"
  },
  {
    "objectID": "labs/02Examples/pytorch/diff_prog.html",
    "href": "labs/02Examples/pytorch/diff_prog.html",
    "title": "Differentiable Programming 101",
    "section": "",
    "text": "We study some initial examples of\n\nnumerical differentiation\nsymbolic differentiation\nautomatic differentiation\n\n\nNumerical Differentiation\nConsider the sine function and its derivative,\n\\[ f(x) = \\sin(x), \\quad f'(x)=\\cos (x) \\]\nevaluated at the point \\(x = 0.1.\\)\n\nimport numpy as np\nf = lambda x: np.sin(x)\nx0 = 0.1\nexact = np.cos(x0)\nprint(\"True derivative:\", exact)\nprint(\"Forward Difference\\tError\\t\\t\\tCentral Difference\\tError\\n\")\nfor i in range(10):\n    h = 1/(10**i)\n    f1 = (f(x0+h)-f(x0))/h\n    f2 = (f(x0+h)-f(x0-h))/(2*h)\n    e1 = np.abs(f1 - exact)\n    e2 = np.abs(f2 - exact)\n    print('%.5e\\t\\t%.5e\\t\\t%.5e\\t\\t%.5e'%(f1,e1,f2,e2))\n\nTrue derivative: 0.9950041652780258\nForward Difference  Error           Central Difference  Error\n\n7.91374e-01     2.03630e-01     8.37267e-01     1.57737e-01\n9.88359e-01     6.64502e-03     9.93347e-01     1.65751e-03\n9.94488e-01     5.15746e-04     9.94988e-01     1.65833e-05\n9.94954e-01     5.00825e-05     9.95004e-01     1.65834e-07\n9.94999e-01     4.99333e-06     9.95004e-01     1.65828e-09\n9.95004e-01     4.99183e-07     9.95004e-01     1.66720e-11\n9.95004e-01     4.99136e-08     9.95004e-01     2.10021e-12\n9.95004e-01     4.96341e-09     9.95004e-01     3.25943e-11\n9.95004e-01     1.06184e-10     9.95004e-01     1.06184e-10\n9.95004e-01     2.88174e-09     9.95004e-01     2.88174e-09\n\n\n\n\nSymbolic Differentiation\nThough very useful in simple cases, symbolic differentiation often leads to complex and redundant expressions. In addition, balckbox routines cannot be differentiated.\n\nfrom sympy import *\nx = symbols('x')\n#\ndiff(cos(x), x)\n\n\\(\\displaystyle - \\sin{\\left(x \\right)}\\)\n\n\n\n# a more complicated esxpression\ndef sigmoid(x):\n  return 1 / (1 + exp(-x))\n\ndiff(sigmoid(x),x)\n\n\\(\\displaystyle \\frac{e^{- x}}{\\left(1 + e^{- x}\\right)^{2}}\\)\n\n\nNote that the derivative of \\[ \\sigma(x) = \\frac{1}{1+e^{-x}}\\] can be simply written as \\[ \\frac{d\\sigma }{dx}= (1-\\sigma(x)) \\sigma(x)\\]\n\n# much more complicated\nx,w1,w2,w3,b1,b2,b3 = symbols('x w1 w2 w3 b1 b2 b3')\ny = w3*sigmoid(w2*sigmoid(w1*x + b1) + b2) + b3\ndiff(y, w1)\n\n\\(\\displaystyle \\frac{w_{2} w_{3} x e^{- b_{1} - w_{1} x} e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}}}{\\left(e^{- b_{1} - w_{1} x} + 1\\right)^{2} \\left(e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}} + 1\\right)^{2}}\\)\n\n\n\ndydw1 = diff(y, w1)\nprint(dydw1)\n\nw2*w3*x*exp(-b1 - w1*x)*exp(-b2 - w2/(exp(-b1 - w1*x) + 1))/((exp(-b1 - w1*x) + 1)**2*(exp(-b2 - w2/(exp(-b1 - w1*x) + 1)) + 1)**2)\n\n\n\n\nAutomatic Differentiation\nHere we show the simplicity and efficiency of autograd from numpy.\n\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad as egrad  # for functions that vectorize over inputs\n\n# We could use np.tanh, but let's write our own as an example.\ndef tanh(x):\n    return (1.0 - np.exp(-x))  / (1.0 + np.exp(-x))\n\nx = np.linspace(-7, 7, 200)\nplt.plot(x, tanh(x),\n         x, egrad(tanh)(x),                                # first  derivative\n         x, egrad(egrad(tanh))(x),                          # second derivative\n         x, egrad(egrad(egrad(tanh)))(x),                    # third  derivative\n         x, egrad(egrad(egrad(egrad(tanh))))(x),              # fourth derivative\n         x, egrad(egrad(egrad(egrad(egrad(tanh)))))(x),        # fifth  derivative\n         x, egrad(egrad(egrad(egrad(egrad(egrad(tanh))))))(x))  # sixth  derivative\n\nplt.axis('off')\nplt.savefig(\"tanh.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom autograd import grad\ngrad_tanh = grad(tanh)            # Obtain its gradient function\ngA = grad_tanh(1.0)               # Evaluate the gradient at x = 1.0\ngN = (tanh(1.01) - tanh(0.99)) / 0.02  # Compare to finite differences\nprint(gA, gN)\n\n0.39322386648296376 0.3932226889551027\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html",
    "href": "labs/02Examples/ad/autograd_tut.html",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "References:"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#recall-approaches-for-computing-derivatives",
    "href": "labs/02Examples/ad/autograd_tut.html#recall-approaches-for-computing-derivatives",
    "title": "Autograd Tutorial",
    "section": "Recall: Approaches for Computing Derivatives",
    "text": "Recall: Approaches for Computing Derivatives\n\nSymbolic differentiation: automatic manipulation of mathematical expressions to get derivatives\n\nTakes a math expression and returns a math expression: \\(f(x) = x^2 \\rightarrow \\frac{df(x)}{dx} = 2x\\)\nUsed in Mathematica, Maple, Sympy, etc.\n\nNumeric differentiation: Approximating derivatives by finite differences: \\[\n\\frac{\\partial}{\\partial x_i} f(x_1, \\dots, x_N) = \\lim_{h \\to 0} \\frac{f(x_1, \\dots, x_i + h, \\dots, x_N) - f(x_1, \\dots, x_i - h, \\dots, x_N)}{2h}\n\\]\nAutomatic differentiation: Takes code that computes a function and returns code that computes the derivative of that function.\n\nReverse Mode AD: A method to get exact derivatives efficiently, by storing information as you go forward that you can reuse as you go backwards\n“The goal isn’t to obtain closed-form solutions, but to be able to wirte a program that efficiently computes the derivatives.”\nAutograd, Torch Autograd"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#reverse-mode-automatic-differentiation",
    "href": "labs/02Examples/ad/autograd_tut.html#reverse-mode-automatic-differentiation",
    "title": "Autograd Tutorial",
    "section": "Reverse Mode Automatic Differentiation",
    "text": "Reverse Mode Automatic Differentiation\nIn machine learning, we have functions that have large fan-in, e.g. a neural net can have millions of parameters, that all squeeze down to one scalar that tells you how well it predicts something. eg. cats…\n\nGeneral Idea for Implementation\n\nCreate a “tape” data structure that tracks the operations performed in computing a function\nOverload primitives to:\n\nAdd themselves to the tape when called\nCompute gradients with respect to their local inputs\n\nForward pass computes the function, and adds operations to the tape\nReverse pass accumulates the local gradients using the chain rule\nThis is efficient for graphs with large fan-in, like most loss functions in ML"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#autograd",
    "href": "labs/02Examples/ad/autograd_tut.html#autograd",
    "title": "Autograd Tutorial",
    "section": "Autograd",
    "text": "Autograd\n\nAutograd is a Python package for automatic differentiation.\nTo install Autograd: pip install autograd\nThere are a lot of great examples provided with the source code\n\n\nWhat can Autograd do?\nFrom the Autograd Github repository:\n\nAutograd can automatically differentiate native Python and Numpy code.\nIt can handle a large subset of Python’s features, including loops, conditional statements (if/else), recursion and closures\nIt can also compute higher-order derivatives\nIt uses reverse-mode differentiation (a.k.a. backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments."
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#autograd-basic-usage",
    "href": "labs/02Examples/ad/autograd_tut.html#autograd-basic-usage",
    "title": "Autograd Tutorial",
    "section": "Autograd Basic Usage",
    "text": "Autograd Basic Usage\n\nimport autograd.numpy as np # Import thinly-wrapped numpy\nfrom autograd import grad   # Basicallly the only function you need\n\n\n# Define a function as usual, using Python and Numpy\ndef tanh(x):\n    y = np.exp(-x)\n    return (1.0 - y) / (1.0 + y)\n\n# Create a *function* that computes the gradient of tanh\ngrad_tanh = grad(tanh)\n\n# Evaluate the gradient at x = 1.0\nprint(grad_tanh(1.0))\n\n# Compare to numeric gradient computed using finite differences\nprint((tanh(1.0001) - tanh(0.9999)) / 0.0002)\n\n0.39322386648296376\n0.39322386636453377"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#autograd-vs-manual-gradients-via-staged-computation",
    "href": "labs/02Examples/ad/autograd_tut.html#autograd-vs-manual-gradients-via-staged-computation",
    "title": "Autograd Tutorial",
    "section": "Autograd vs Manual Gradients via Staged Computation",
    "text": "Autograd vs Manual Gradients via Staged Computation\nIn this example, we will see how a complicated computation can be written as a composition of simpler functions, and how this provides a scalable strategy for computing gradients using the chain rule.\nWe want to write a function to compute the gradient of the sigmoid function: \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\] We can write \\(\\sigma(x)\\) as a composition of several elementary functions, as \\(\\sigma(x) = s(c(b(a(x))))\\), where\n\\[\n\\begin{align}\na(x) &= -x \\\\\nb(a) & = e^a \\\\\nc(b) & = 1 + b \\\\\ns(c) = \\frac{1}{c}.\n\\end{align}\n\\]\nHere, we have “staged” the computation such that it contains several intermediate variables, each of which are basic expressions for which we can easily compute the local gradients.\nThe computation graph for this expression is\n\\[ x \\longrightarrow a \\longrightarrow b \\longrightarrow c \\longrightarrow  s.\\]\nThe input to this function is \\(x\\), and the output is represented by node \\(s\\). We want to compute the gradient of \\(s\\) with respect to \\(x\\), \\[\\frac{\\partial s}{\\partial x}.\\] In order to make use of our intermediate computations, we just use the chain rule, \\[\n\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x},\n\\] where we clearly observe the backward propagation of the gradients, from \\(s\\) to \\(a.\\)\n\nGiven a vector-to-scalar function, \\(\\mathbb{R}^D \\to \\mathbb{R}\\), composed of a set of primitive functions \\(\\mathbb{R}^M \\to \\mathbb{R}^N\\) (for various \\(M\\), \\(N\\)), the gradient of the composition is given by the product of the gradients of the primitive functions, according to the chain rule. But the chain rule doesn’t prescribe the order in which to multiply the gradients. From the perspective of computational complexity, the order makes all the difference.\n\n\ndef grad_sigmoid_manual(x):\n    \"\"\"Implements the gradient of the logistic sigmoid function \n    $\\sigma(x) = 1 / (1 + e^{-x})$ using staged computation\n    \"\"\"\n    # Forward pass, keeping track of intermediate values for use in the \n    # backward pass\n    a = -x         # -x in denominator\n    b = np.exp(a)  # e^{-x} in denominator\n    c = 1 + b      # 1 + e^{-x} in denominator\n    s = 1.0 / c    # Final result: 1.0 / (1 + e^{-x})\n    \n    # Backward pass (differentiate basic functions)\n    dsdc = (-1.0 / (c**2))\n    dsdb = dsdc * 1\n    dsda = dsdb * np.exp(a)\n    dsdx = dsda * (-1)\n    \n    return dsdx\n\n\ndef sigmoid(x):\n    y = 1.0 / (1.0 + np.exp(-x))\n    return y\n\n# Instead of writing grad_sigmoid_manual manually, we can use \n# Autograd's grad function:\ngrad_sigmoid_automatic = grad(sigmoid)\n\n# Compare the results of manual and automatic gradient functions:\nprint(grad_sigmoid_automatic(2.0))\nprint(grad_sigmoid_manual(2.0))\n\n0.1049935854035065\n0.1049935854035065"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#gradient-functions",
    "href": "labs/02Examples/ad/autograd_tut.html#gradient-functions",
    "title": "Autograd Tutorial",
    "section": "Gradient Functions",
    "text": "Gradient Functions\nThere are several functions that compute gradients, which have different signatures\n\ngrad(fun, argnum=0)\n\nReturns a function which computes the gradient of fun with respect to positional argument number argnum. The returned function takes the same arguments as fun, but returns the gradient instead. The function fun should be scalar-valued. The gradient has the same type as the argument.\n\ngrad_named(fun, argname)\n\nTakes gradients with respect to a named argument.\n\nmultigrad(fun, argnums=[0])\n\nTakes gradients wrt multiple arguments simultaneously.\n\nmultigrad_dict(fun)\n\nTakes gradients with respect to all arguments simultaneously, and returns a dict mapping argname to gradval"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#modularity-implementing-custom-gradients",
    "href": "labs/02Examples/ad/autograd_tut.html#modularity-implementing-custom-gradients",
    "title": "Autograd Tutorial",
    "section": "Modularity: Implementing Custom Gradients",
    "text": "Modularity: Implementing Custom Gradients\nThe implementation of Autograd is simple, readable, and extensible!\nOne thing you can do is define custom gradients for your own functions. There are several reasons you might want to do this, including:\n\nSpeed: You may know a faster way to compute the gradient for a specific function.\nNumerical Stability\nWhen your code depends on external library calls\n\nThe @primitive decorator wraps a function so that its gradient can be specified manually and its invocation can be recorded.\n\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nfrom autograd import grad\nfrom autograd.extend import primitive, defvjp\n\n# From the Autograd examples:\n# @primitive tells autograd not to look inside this function, but instead\n# to treat it as a black box, whose gradient might be specified later.\n@primitive\ndef logsumexp(x):\n    \"\"\"Numerically stable log(sum(exp(x)))\"\"\"\n    max_x = np.max(x)\n    return max_x + np.log(np.sum(np.exp(x - max_x)))\n\n# Next, we write a function that specifies the gradient with a closure.\ndef make_grad_logsumexp(ans, x):\n    # If you want to be able to take higher-order derivatives, then all the\n    # code inside this function must be itself differentiable by autograd.\n    def gradient_product(g):\n        return np.full(x.shape, g) * np.exp(x - np.full(x.shape, ans))\n    return gradient_product\n\n# Now we tell autograd that logsumexmp has a gradient-making function.\ndefvjp(logsumexp, make_grad_logsumexp)\n\n\n# Now we can use logsumexp() inside a larger function that we want to differentiate.\ndef example_func(y):\n    z = y**2\n    lse = logsumexp(z)\n    return np.sum(lse)\n\ngrad_of_example = grad(example_func)\nprint(\"Gradient: \", grad_of_example(npr.randn(10)))\n\n# Check the gradients numerically, just to be safe.\n# Fails if a mismatch occurs\nfrom autograd.test_util import check_grads\ncheck_grads(example_func, modes=['rev'], order=2)(npr.randn(10))\n\nGradient:  [ 0.00445388 -0.06760237 -0.0030408  -0.00398812  0.13980291 -0.65753857\n -0.57149671 -0.0969613  -0.27697817  1.59207253]"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#linear-regression",
    "href": "labs/02Examples/ad/autograd_tut.html#linear-regression",
    "title": "Autograd Tutorial",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nReview\nWe are given a set of data points \\(\\{ (x_1, t_1), (x_2, t_2), \\dots, (x_N, t_N) \\}\\), where each point \\((x_i, t_i)\\) consists of an input value \\(x_i\\) and a target value \\(t_i\\).\nThe model we use is: \\[\ny_i = wx_i + b\n\\]\nWe want each predicted value \\(y_i\\) to be close to the ground truth value \\(t_i\\). In linear regression, we use squared error to quantify the disagreement between \\(y_i\\) and \\(t_i\\). The loss function for a single example is: \\[\n\\mathcal{L}(y_i,t_i) = \\frac{1}{2} (y_i - t_i)^2\n\\]\nThe cost function is the loss averaged over all the training examples: \\[\n\\mathcal{E}(w,b) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(y_i, t_i) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left(wx_i + b - t_i \\right)^2\n\\]\n\nimport autograd.numpy as np # Import wrapped NumPy from Autograd\nimport autograd.numpy.random as npr # For convenient access to numpy.random\nfrom autograd import grad # To compute gradients\n\nimport matplotlib.pyplot as plt # For plotting\n\n%matplotlib inline"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#generate-synthetic-data",
    "href": "labs/02Examples/ad/autograd_tut.html#generate-synthetic-data",
    "title": "Autograd Tutorial",
    "section": "Generate Synthetic Data",
    "text": "Generate Synthetic Data\nWe generate a synthetic dataset \\(\\{ (x_i, t_i) \\}\\) by first taking the \\(x_i\\) to be linearly spaced in the range \\([0, 10]\\) and generating the corresponding value of \\(t_i\\) using the following equation (where \\(w = 4\\) and \\(b=10\\)): \\[\nt_i = 4 x_i + 10 + \\epsilon\n\\]\nHere, \\(\\epsilon \\sim \\mathcal{N}(0, 2),\\) that is, \\(\\epsilon\\) is drawn from a Gaussian distribution with mean 0 and variance 2. This introduces some random fluctuation in the data, to mimic real data that has an underlying regularity, but for which individual observations are corrupted by random noise.\n\n# In our synthetic data, we have w = 4 and b = 10\nN = 100 # Number of training data points\nx = np.linspace(0, 10, N)\nt = 4 * x + 10 + npr.normal(0, 2, x.shape[0])\nplt.plot(x, t, 'r.')\n\n\n\n\n\n\n\n\n\n# Initialize random parameters\nw = npr.normal(0, 1)\nb = npr.normal(0, 1)\nparams = { 'w': w, 'b': b } # One option: aggregate parameters in a dictionary\n\ndef cost(params):\n    y = params['w'] * x + params['b']\n    return (1 / N) * np.sum(0.5 * np.square(y - t))\n\n# Find the gradient of the cost function using Autograd\ngrad_cost = grad(cost) \n\nnum_epochs = 1000  # Number of epochs of training\nalpha = 0.01       # Learning rate\n\nfor i in range(num_epochs):\n    # Evaluate the gradient of the current parameters stored in params\n    cost_params = grad_cost(params)\n    \n    # Update parameters w and b\n    params['w'] = params['w'] - alpha * cost_params['w']\n    params['b'] = params['b'] - alpha * cost_params['b']\n\nprint(params)\n\n{'w': 4.084961144270687, 'b': 9.264915086528749}\n\n\n\n# Plot the training data, together with the line defined by y = wx + b,\n# where w and b are our final learned parameters\nplt.plot(x, t, 'r.')\nplt.plot([0, 10], [params['b'], params['w'] * 10 + params['b']], 'b-')\nplt.show()"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#linear-regression-with-a-feature-mapping",
    "href": "labs/02Examples/ad/autograd_tut.html#linear-regression-with-a-feature-mapping",
    "title": "Autograd Tutorial",
    "section": "Linear Regression with a Feature Mapping",
    "text": "Linear Regression with a Feature Mapping\nIn this example we will fit a polynomial using linear regression with a polynomial feature mapping. The target function is\n\\[\nt = x^4 - 10 x^2 + 10 x + \\epsilon,\n\\]\nwhere \\(\\epsilon \\sim \\mathcal{N}(0, 4).\\)\nThis is an example of a generalized linear model, in which we perform a fixed nonlinear transformation of the inputs \\(\\mathbf{x} = (x_1, x_2, \\dots, x_D)\\), and the model is still linear in the parameters. We can define a set of feature mappings (also called feature functions or basis functions) \\(\\phi\\) to implement the fixed transformations.\nIn this case, we have \\(x \\in \\mathbb{R}\\), and we define the feature mapping: \\[\n\\mathbf{\\phi}(x) = \\begin{pmatrix}\\phi_1(x) \\\\ \\phi_2(x) \\\\ \\phi_3(x) \\\\ \\phi_4(x) \\end{pmatrix} = \\begin{pmatrix}1\\\\x\\\\x^2\\\\x^3\\end{pmatrix}\n\\]\n\n# Generate synthetic data\nN = 100 # Number of data points\nx = np.linspace(-3, 3, N) # Generate N values linearly-spaced between -3 and 3\nt = x ** 4 - 10 * x ** 2 + 10 * x + npr.normal(0, 4, x.shape[0]) # Generate corresponding targets\nplt.plot(x, t, 'r.') # Plot data points\n\n\n\n\n\n\n\n\n\nM = 4 # Degree of polynomial to fit to the data (this is a hyperparameter)\nfeature_matrix = np.array([[item ** i for i in range(M+1)] for item in x]) # Construct a feature matrix \nW = npr.randn(feature_matrix.shape[-1])\n\ndef cost(W):\n    y = np.dot(feature_matrix, W)\n    return (1.0 / N) * np.sum(0.5 * np.square(y - t))\n\n# Compute the gradient of the cost function using Autograd\ncost_grad = grad(cost)\n\nnum_epochs = 10000\nlearning_rate = 0.001\n\n# Manually implement gradient descent\nfor i in range(num_epochs):\n    W = W - learning_rate * cost_grad(W)\n\n# Print the final learned parameters.\nprint(W)\n\n[  0.0545782   10.42891793 -10.10707337  -0.03126282   1.02186303]\n\n\n\n# Plot the original training data again, together with the polynomial we fit\nplt.plot(x, t, 'r.')\nplt.plot(x, np.dot(feature_matrix, W), 'b-')\nplt.show()"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#neural-net-regression",
    "href": "labs/02Examples/ad/autograd_tut.html#neural-net-regression",
    "title": "Autograd Tutorial",
    "section": "Neural Net Regression",
    "text": "Neural Net Regression\nIn this example we will implement a (nonlinear) regression model using a neural network.\nTo implement and train a neural net using Autograd, you only have to define the forward pass of the network and the loss function you wish to use; you do not need to implement the backward pass of the network. When you take the gradient of the loss function using grad, Autograd automatically computes the backward pass. It essentially executes the backpropagation algorithm implicitly.\n\nimport matplotlib.pyplot as plt\n\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nfrom autograd import grad\nfrom autograd.misc import flatten #, flatten_func\n\nfrom autograd.misc.optimizers import sgd\n\n%matplotlib inline\n\n\nAutograd Implementation of Stochastic Gradient Descent (with momentum)\ndef sgd(grad, init_params, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n    \"\"\"Stochastic gradient descent with momentum.\n    grad() must have signature grad(x, i), where i is the iteration number.\"\"\"\n    flattened_grad, unflatten, x = flatten_func(grad, init_params)\n    \n    velocity = np.zeros(len(x))\n    for i in range(num_iters):\n        g = flattened_grad(x, i)\n        if callback:\n            callback(unflatten(x), i, unflatten(g))\n        velocity = mass * velocity - (1.0 - mass) * g\n        x = x + step_size * velocity\n    return unflatten(x)\nThe next example shows how to use the sgd function.\n\n# Generate synthetic data\nx = np.linspace(-5, 5, 1000)\nt = x ** 3 - 20 * x + 10 + npr.normal(0, 4, x.shape[0])\nplt.plot(x, t, 'r.')\n\n\n\n\n\n\n\n\n\ninputs = x.reshape(x.shape[-1],1)\nW1 = npr.randn(1,4)\nb1 = npr.randn(4)\nW2 = npr.randn(4,4)\nb2 = npr.randn(4)\nW3 = npr.randn(4,1)\nb3 = npr.randn(1)\n\nparams = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3 }\n\ndef relu(x):\n    return np.maximum(0, x)\n\n#nonlinearity = np.tanh\nnonlinearity = relu\n\ndef predict(params, inputs):\n    h1 = nonlinearity(np.dot(inputs, params['W1']) + params['b1'])\n    h2 = nonlinearity(np.dot(h1, params['W2']) + params['b2'])\n    output = np.dot(h2, params['W3']) + params['b3']\n    return output\n\ndef loss(params, i):\n    output = predict(params, inputs)\n    return (1.0 / inputs.shape[0]) * np.sum(0.5 * np.square(output.reshape(output.shape[0]) - t))\n\nprint(loss(params, 0))\n\noptimized_params = sgd(grad(loss), params, step_size=0.01, num_iters=5000)\nprint(optimized_params)\nprint(loss(optimized_params, 0))\n\nfinal_y = predict(optimized_params, inputs)\nplt.plot(x, t, 'r.')\nplt.plot(x, final_y, 'b-')\nplt.show()\n\n307.27526955036535\n{'W1': array([[-2.94559997,  0.16121652, -1.30047875, -1.22974889]]), 'b1': array([-5.28166397, -0.82528464,  2.41753918,  6.14589224]), 'W2': array([[-5.05168877e-01, -2.36611718e+00, -3.27880077e+00,\n         2.75007753e+00],\n       [-1.92904027e-01, -2.37367667e-01, -4.65899580e-01,\n        -1.92235478e+00],\n       [ 4.09549644e-01, -2.22665262e+00,  1.40462107e+00,\n         2.85735187e-03],\n       [-3.72648981e+00,  2.46773804e+00,  1.86232133e+00,\n        -1.55498882e+00]]), 'b2': array([ 5.30739445,  0.42395663, -4.68675653, -2.42712697]), 'W3': array([[ 6.08166514],\n       [-3.52731677],\n       [ 4.50279179],\n       [-3.36406041]]), 'b3': array([0.11658614])}\n8.641052001438881\n\n\n\n\n\n\n\n\n\n\n# A plot of the result of this model using tanh activations\nplt.plot(x, final_y, 'b-')\nplt.show()\n\n\n\n\n\n\n\n\n\n# A plot of the result of this model using ReLU activations\nplt.plot(x, final_y, 'b-')\nplt.show()"
  },
  {
    "objectID": "labs/w4-lab02-Diff-Prog.html",
    "href": "labs/w4-lab02-Diff-Prog.html",
    "title": "Differentiable Programming",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/w3-lab02-Diff-Prog.html",
    "href": "labs/w3-lab02-Diff-Prog.html",
    "title": "Differentiable Programming",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_lin_reg.html",
    "href": "labs/02Examples/ad/autograd_lin_reg.html",
    "title": "Linear Regression with autograd",
    "section": "",
    "text": "We use autograd to perform a linear regression on some randomly distributed data, with added random noise. We then compare the results with a linear regression performed using sklearn.\nIn the autograd implementation, we will use a basic gradient descent that minimizes the mean-squared loss function to find the two coefficients, slope and intercept.\nIn a later example, this will be done using pytorch.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport autograd.numpy as ag_np\nfrom autograd import grad\n\n# Generate some random data and form a linear function\nnp.random.seed(42)\nX = np.random.rand(50, 1) * 10\ny = 2 * X + 3 + np.random.randn(50, 1) # noisy line\n\n# Define the linear regression model\ndef linear_regression(params, x):\n    return ag_np.dot(x, params[0]) + params[1]\n\n# Define the loss function = mean squared error\ndef mean_squared_error(params, x, y):\n    predictions = linear_regression(params, x)\n    return ag_np.mean((predictions - y) ** 2)\n\n# Initialize parameters\ninitial_params = [ag_np.ones((1, 1)), ag_np.zeros((1,))]\nlr = 0.01\nnum_epochs = 1000\n\n# Gradient of the loss function using autograd\ngrad_loss = grad(mean_squared_error)\n\n# Optimization loop\nparams = initial_params\nfor epoch in range(num_epochs):\n    gradient = grad_loss(params, X, y)\n    params[0] -= lr * gradient[0]\n    params[1] -= lr * gradient[1]\n\n# Extract the learned slope and intercept\nslope = params[0][0, 0]\nintercept = params[1][0]\n\n# Plot the data points and the resulting line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, label='Data Points')\nplt.plot(X, slope * X + intercept, color='red', label='Regression Line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression using Autograd')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nLet us compare with scikit_learn\n\nfrom sklearn.linear_model import LinearRegression\n# setup model\nmodel = LinearRegression()\n# fit\nres = model.fit(X, y)\n# predict\npredictions = model.predict(X)\n# plot\nplt.figure(figsize=(8, 6))\nplt.plot(X, predictions)\nplt.grid(True)\nplt.show()\nprint(\"sklearn: intercept = \",res.intercept_,\"slope = \", res.coef_[0],)\nprint(\"autograd: intercept = \",intercept,\"slope = \", slope,)\n\n\n\n\n\n\n\n\nsklearn: intercept =  [3.09668927] slope =  [1.9776566]\nautograd: intercept =  3.087098312274722 slope =  1.9791961905803472\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/02Examples/ad/diff_prog.html",
    "href": "labs/02Examples/ad/diff_prog.html",
    "title": "Differentiable Programming 101",
    "section": "",
    "text": "We study some initial examples of\n\nnumerical differentiation\nsymbolic differentiation\nautomatic differentiation\n\n\nNumerical Differentiation\nConsider the sine function and its derivative,\n\\[ f(x) = \\sin(x), \\quad f'(x)=\\cos (x) \\]\nevaluated at the point \\(x = 0.1.\\)\n\nimport numpy as np\nf = lambda x: np.sin(x)\nx0 = 0.1\nexact = np.cos(x0)\nprint(\"True derivative:\", exact)\nprint(\"Forward Difference\\tError\\t\\t\\tCentral Difference\\tError\\n\")\nfor i in range(10):\n    h = 1/(10**i)\n    f1 = (f(x0+h)-f(x0))/h\n    f2 = (f(x0+h)-f(x0-h))/(2*h)\n    e1 = np.abs(f1 - exact)\n    e2 = np.abs(f2 - exact)\n    print('%.5e\\t\\t%.5e\\t\\t%.5e\\t\\t%.5e'%(f1,e1,f2,e2))\n\nTrue derivative: 0.9950041652780258\nForward Difference  Error           Central Difference  Error\n\n7.91374e-01     2.03630e-01     8.37267e-01     1.57737e-01\n9.88359e-01     6.64502e-03     9.93347e-01     1.65751e-03\n9.94488e-01     5.15746e-04     9.94988e-01     1.65833e-05\n9.94954e-01     5.00825e-05     9.95004e-01     1.65834e-07\n9.94999e-01     4.99333e-06     9.95004e-01     1.65828e-09\n9.95004e-01     4.99183e-07     9.95004e-01     1.66720e-11\n9.95004e-01     4.99136e-08     9.95004e-01     2.10021e-12\n9.95004e-01     4.96341e-09     9.95004e-01     3.25943e-11\n9.95004e-01     1.06184e-10     9.95004e-01     1.06184e-10\n9.95004e-01     2.88174e-09     9.95004e-01     2.88174e-09\n\n\n\n\nSymbolic Differentiation\nThough very useful in simple cases, symbolic differentiation often leads to complex and redundant expressions. In addition, balckbox routines cannot be differentiated.\n\nfrom sympy import *\nx = symbols('x')\n#\ndiff(cos(x), x)\n\n\\(\\displaystyle - \\sin{\\left(x \\right)}\\)\n\n\n\n# a more complicated esxpression\ndef sigmoid(x):\n  return 1 / (1 + exp(-x))\n\ndiff(sigmoid(x),x)\n\n\\(\\displaystyle \\frac{e^{- x}}{\\left(1 + e^{- x}\\right)^{2}}\\)\n\n\nNote that the derivative of \\[ \\sigma(x) = \\frac{1}{1+e^{-x}}\\] can be simply written as \\[ \\frac{d\\sigma }{dx}= (1-\\sigma(x)) \\sigma(x)\\]\n\n# much more complicated\nx,w1,w2,w3,b1,b2,b3 = symbols('x w1 w2 w3 b1 b2 b3')\ny = w3*sigmoid(w2*sigmoid(w1*x + b1) + b2) + b3\ndiff(y, w1)\n\n\\(\\displaystyle \\frac{w_{2} w_{3} x e^{- b_{1} - w_{1} x} e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}}}{\\left(e^{- b_{1} - w_{1} x} + 1\\right)^{2} \\left(e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}} + 1\\right)^{2}}\\)\n\n\n\ndydw1 = diff(y, w1)\nprint(dydw1)\n\nw2*w3*x*exp(-b1 - w1*x)*exp(-b2 - w2/(exp(-b1 - w1*x) + 1))/((exp(-b1 - w1*x) + 1)**2*(exp(-b2 - w2/(exp(-b1 - w1*x) + 1)) + 1)**2)\n\n\n\n\nAutomatic Differentiation\nHere we show the simplicity and efficiency of autograd from numpy.\n\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad as egrad  # for functions that vectorize over inputs\n\n# We could use np.tanh, but let's write our own as an example.\ndef tanh(x):\n    return (1.0 - np.exp(-x))  / (1.0 + np.exp(-x))\n\nx = np.linspace(-7, 7, 200)\nplt.plot(x, tanh(x),\n         x, egrad(tanh)(x),                                # first  derivative\n         x, egrad(egrad(tanh))(x),                          # second derivative\n         x, egrad(egrad(egrad(tanh)))(x),                    # third  derivative\n         x, egrad(egrad(egrad(egrad(tanh))))(x),              # fourth derivative\n         x, egrad(egrad(egrad(egrad(egrad(tanh)))))(x),        # fifth  derivative\n         x, egrad(egrad(egrad(egrad(egrad(egrad(tanh))))))(x))  # sixth  derivative\n\nplt.axis('off')\nplt.savefig(\"tanh.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom autograd import grad\ngrad_tanh = grad(tanh)            # Obtain its gradient function\ngA = grad_tanh(1.0)               # Evaluate the gradient at x = 1.0\ngN = (tanh(1.01) - tanh(0.99)) / 0.02  # Compare to finite differences\nprint(gA, gN)\n\n0.39322386648296376 0.3932226889551027\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/02Examples/pytorch/Torch_test_GPU_CPU.html",
    "href": "labs/02Examples/pytorch/Torch_test_GPU_CPU.html",
    "title": "Conclusion",
    "section": "",
    "text": "# check availability of GPU\nimport torch\nif torch.backends.mps.is_available():\n    mps_device = torch.device(\"mps\")\n    x = torch.ones(1, device=mps_device)\n    print (x)\nelse:\n    print (\"MPS device not found.\")\n\ntensor([1.], device='mps:0')\n\n\n\n# toy example on GPU\nimport timeit\nimport torch\nimport random\n\nx = torch.ones(5000, device=\"mps\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=100000)\n#Out[17]: 4.568202124999971\n\n# toy example cpu\n#x = torch.ones(5000, device=\"cpu\")\n# timeit.timeit(lambda: x * random.randint(0,100), number=100000)\n#Out[18]: 0.30446054200001527\n\n2.0122362919998977\n\n\n\n# toy example on cpu\nx = torch.ones(5000, device=\"cpu\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=100000)\n\n0.24692429099991386\n\n\nThe CPU is approximately 10 times faster than the GPU…\nHere is a slightly more complex examples, with a matrix-vector tensor multiplication.\n\na_cpu = torch.rand(250, device='cpu')\nb_cpu = torch.rand((250, 250), device='cpu')\na_mps = torch.rand(250, device='mps')\nb_mps = torch.rand((250, 250), device='mps')\n\nprint('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000))\nprint('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000))\n\ncpu 0.8405147910000323\nmps 2.3573820419999265\n\n\nNow, we drastically increase the problem size, using the tensor dimension.\n\nx = torch.ones(50000000, device=\"mps\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=1)\n\n0.00048149999997804116\n\n\n\nx = torch.ones(50000000, device=\"cpu\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=1)\n\n0.03234533299996656\n\n\n\n.0323/.00048\n\n67.29166666666667\n\n\nGPU works well, but only for LARGE memory problems. This is because loading small data to memory and using GPU for calculation is overkill, so the CPU has an advantage in this case. But if you have large data dimensions, the GPU can compute efficiently and surpass the CPU.\nThis is well known with GPUs: they are only faster if you put a large computational load. It is not specific to pytorch or to MPS…\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html",
    "href": "labs/02Examples/pytorch/pytorch_102.html",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Based on Bourkes’s https://www.learnpytorch.io/\nHere’s a standard PyTorch workflow.\n\n\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#pytorch-workflows",
    "href": "labs/02Examples/pytorch/pytorch_102.html#pytorch-workflows",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Based on Bourkes’s https://www.learnpytorch.io/\nHere’s a standard PyTorch workflow.\n\n\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#prepare-the-data",
    "href": "labs/02Examples/pytorch/pytorch_102.html#prepare-the-data",
    "title": "Intro to PyTorch",
    "section": "2.1. Prepare the data",
    "text": "2.1. Prepare the data\nHere we will generate our own data, a straight line, then use PyTorch to find the slope (weight) and intercept (bias).\n\n# Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend   = 1\nstep  = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\n\nTrain, validate, test split\n\ntraining set -&gt; model learns from this data\nvalidation ste -&gt; model is tuned on this data\ntest set -&gt; model is evluated on this data\n\n\n# Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test,  y_test  = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\nCreate a function to visualize the data.\n\ndef plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n      \"\"\"\n      Plots training data, test data and compares predictions.\n      \"\"\"\n      plt.figure(figsize=(10, 7))\n\n      # Plot training data in blue\n      plt.scatter(train_data, train_labels, c=\"b\", s=20, label=\"Training data\")\n\n      # Plot test data in green\n      plt.scatter(test_data, test_labels, c=\"orange\", s=20, label=\"Testing data\")\n\n      if predictions is not None:\n        # Plot the predictions in red (predictions were made on the test data)\n        plt.scatter(test_data, predictions, c=\"r\", s=20, label=\"Predictions\")\n\n      # Show the legend\n      plt.legend(prop={\"size\": 14});\n\n\nplot_predictions()"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#build-the-model",
    "href": "labs/02Examples/pytorch/pytorch_102.html#build-the-model",
    "title": "Intro to PyTorch",
    "section": "2.2 Build the model",
    "text": "2.2 Build the model\nThere are 4 essential modules for creating any NN\n\ntorch.nn contains all the building blocks of the computational graph\ntorch.optim contains the different optimization algorithms\ntorch.utils.data.Dataset selects data\ntorch.utils.data.DataLoader loads the data\n\nThe NN itself, defined by torch.nn, contains the following sub-modules\n\nnn.Module has the layers\nnn.Parameter has the weights and biases\n\nFinally, all the nn.Module subclasses require a forward() method that defines the flow of the computation, or structure of the NN.\nWe create a standard linear regression class.\n\n# Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                    dtype=torch.float), # &lt;- PyTorch uses float32 by default\n                                    requires_grad=True) # &lt;- update this value with gradient descent\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                dtype=torch.float), # &lt;- PyTorch uses float32 by default\n                                requires_grad=True) # &lt;- update this value with gradient descent\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n\nNow, let’s create an instance of the model and check its prameters.\n\n# Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model \n# (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n\n[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]\n\n\nWe can retrieve the state of the model, with .stat_dict()\n\n# List named parameters \nmodel_0.state_dict()\n\nOrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#make-predictions-with-torch.inference_mode",
    "href": "labs/02Examples/pytorch/pytorch_102.html#make-predictions-with-torch.inference_mode",
    "title": "Intro to PyTorch",
    "section": "Make predictions with torch.inference_mode()",
    "text": "Make predictions with torch.inference_mode()\nBefore optimizing, we can already make (bad) predictions with this randomly initialized model. We will use the torch.inference_mode() function that streamlines the inference process.\n\n# Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n\n# Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n\nplot_predictions(predictions=y_preds)\n\nNumber of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n\n\n\n\n\n\n\n\n\nThese predicitions are way off, as expected, since they are based on a random initialization. Let us compute the errors.\n\n# errors\ny_test - y_preds\n\ntensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#train-the-model",
    "href": "labs/02Examples/pytorch/pytorch_102.html#train-the-model",
    "title": "Intro to PyTorch",
    "section": "Train the model",
    "text": "Train the model\nWe need to define\n\na loss function\nan optimizer\n\nHere we will use\n\nMAE torch.nn.L1Loss()\nSGD torch.optim.SGD(params, lr)\n\nparams are the model parameters that we want to adjust optimally\nlr is the learning rate (step-size) of the gradient descent, a hyperparameter\n\n\n.\n\n# Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate \n                                     # (how much the optimizer should change parameters at each \n                                     # step, higher=more (less stable), lower=less (might take a long time))\n\nFinally, we need the training and testing loops.\n\nTraining loop\n\nHere are the 5 basic steps:\n\nforward pass through the training data -&gt; model(x_train)\ncompute the loss -&gt; loss=loss_fn(y_pred,y_train)\nset gradients to zero -&gt; optimizer.zero_grad()\ndo backprop on the loss to compute gradient -&gt; loss.backward()\nupdate the parametes with the gradient -&gt; optimizer.step()\n\n\n\nTesting loop\n\nIn 3 steps:\n\nforward pass -&gt; model(x_test)\ncompute the loss -&gt; loss=loss_fn(y_pred,y_test)\ncompute evaluation metrics/scores\n\nWe put it all together, and train for 1000 epochs of the SGD.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Advance the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening every 10 steps\n      if epoch % 20 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n\nEpoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \n\n\nFinally, plot the loss curves.\n\n# Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\nNow, inspect the model’s state_dict() to see how close we got.\n\n# Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nThe original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#use-trained-model-for-predictions",
    "href": "labs/02Examples/pytorch/pytorch_102.html#use-trained-model-for-predictions",
    "title": "Intro to PyTorch",
    "section": "Use trained model for predictions",
    "text": "Use trained model for predictions\nTo do inference with a PyTorch model:\n\nSet model in evaluation mode -&gt; model.eval()\nMake predictions using the inference mode context manager -&gt; with torch.inference_mode()\nAll predictions should be on objects on the same device - GPU/CPU\n\n\n# 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n\n# plot the result\nplot_predictions(predictions=y_preds)\n\n\n\n\n\n\n\n\nFinal error plot, showing noise “ball” limit.\n\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.yscale(\"log\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#saving-and-loading-trained-models",
    "href": "labs/02Examples/pytorch/pytorch_102.html#saving-and-loading-trained-models",
    "title": "Intro to PyTorch",
    "section": "Saving and loading trained models",
    "text": "Saving and loading trained models\nThree main methods:\n\ntorch.save uses pickle to save anything\ntorch.load unpickles\ntorch.nn.Module.load_state_dict loads a model’s parameter dictionary (model_save_dict())\n\n\nfrom pathlib import Path\n\n# 1. Create 'models' directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_0.pth\n\n\n\n# Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n\n-rw-r--r--@ 1 markasch  staff  1207 Feb 14 14:15 models/01_pytorch_workflow_model_0.pth\n\n\n\nLoad a saved model\nWe have saved the model’s state dictionary at a given path. We can now load it using\n\ntorch.nn.Module.load_state_dict(torch.load(f=))\n\nTo test this, we create anew instance of the LinearRegressionModel(), which being a subclass of torch.nn.Module has all its built-in methods, and in particular load_state_dict().\n\n# Instantiate a new instance of our model (this will be instantiated \n# with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new \n# instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n&lt;All keys matched successfully&gt;\n\n\nNow, we are ready to perform inference.\n\n# 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n    \n# Compare previous model predictions with loaded model predictions \n# (these should be the same)\ny_preds == loaded_model_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#device-agnostic-version-of-pytorch-ml-workflow",
    "href": "labs/02Examples/pytorch/pytorch_102.html#device-agnostic-version-of-pytorch-ml-workflow",
    "title": "Intro to PyTorch",
    "section": "Device Agnostic Version of pyTorch ML Workflow",
    "text": "Device Agnostic Version of pyTorch ML Workflow\nUsing all the above snippets, we can now write a complete, device agnostic workflow.\n\n# Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'\n\n\n\n# Setup device agnostic code\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nUsing device: mps\n\n\n\n# Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:5], y[:5]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560]]))\n\n\n\n# Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\n\nplot_predictions(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\nBuild the pyTorch Linear Model\nInstead of manually defining weight and bias parmeters by “hand”, using nn.Parameter(), we will use e pre-built torch.nn module,\n\nnn.Linear(dim_in_features, dim_out_features)\n\n\n\n# Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need \n# but is used for demonstrative purposes, try commenting it out and \n# seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n\n(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))\n\n\nNow, we can place the model onto the gpu device (after checking)\n\n# Check model device\nnext(model_1.parameters()).device\n\ndevice(type='cpu')\n\n\n\n# Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\"/\"mps\" \n                   # if available or \"cpu\" if not\nnext(model_1.parameters()).device\n\ndevice(type='mps', index=0)\n\n\n\n\nTraining\nWe use the same functions and hyperparameters as before\n\nnn.L1Loss()\ntorch.optim.SGD()\n\n\n# Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n\nBefore training on the gpu, we must place the data there too.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n\n/Users/markasch/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nEpoch: 0 | Train loss: 0.5551779270172119 | Test loss: 0.5739762783050537\nEpoch: 100 | Train loss: 0.0062156799249351025 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 300 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 400 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 500 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 600 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 700 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 800 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 900 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\n\n\n\n# Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='mps:0')),\n             ('linear_layer.bias', tensor([0.3025], device='mps:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n\n\n\n\nPredictions\nUse inference mode.\n\n# Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n\ntensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='mps:0')\n\n\n\n# plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#save-and-load",
    "href": "labs/02Examples/pytorch/pytorch_102.html#save-and-load",
    "title": "Intro to PyTorch",
    "section": "Save and Load",
    "text": "Save and Load\nFinally, save, load and perform inference.\n\nfrom pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_1.pth\n\n\n\n# Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n\nLoaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\nmps:0\n\n\n\n# Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='mps:0')"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#to-go-further",
    "href": "labs/02Examples/pytorch/pytorch_102.html#to-go-further",
    "title": "Intro to PyTorch",
    "section": "To go further…",
    "text": "To go further…\nA more complete tutorial on torch.nn is available on the official pytorch website\n\nhttps://pytorch.org/tutorials/beginner/nn_tutorial.html"
  },
  {
    "objectID": "labs/01intro/underfitting_overfitting.html",
    "href": "labs/01intro/underfitting_overfitting.html",
    "title": "Underfitting vs. Overfitting",
    "section": "",
    "text": "%matplotlib inline\n\n============================\nThis example shows how underfitting and overfitting arise when using polynomial regression to approximate a nonlinear function, \\(y =1.5 \\cos (\\pi x).\\)\nThe plots shows the function \\(y(x)\\) and the estimated curves of of different degrees.\nWe observe the following:\n\nThe linear function (polynomial with degree 1) is not sufficient to fit the training samples—this is underfitting.\nA polynomial of degree 4 approximates the true function almost perfectly and gives the smallest MSE.\nFor higher degrees, the model overfits the training data, and the mean-squared errors (MSE) become very large–the model is learning the noise in the training data.\n\nWe evaluate quantitatively overfitting / underfitting by using cross-validation and then calculating the mean squared error (MSE) on the validation set. The higher the value, the less likely the model generalizes correctly from the training data since it is brittle.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 10, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 10))\nfor i in range(len(degrees)):\n    ax = plt.subplot(2, 2, i + 1) \n    plt.setp(ax, xticks=(), yticks=())\n    polynomial_features = PolynomialFeatures(degree=degrees[i],                                            include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using cross-validation\n    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 04",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nStatistical Inverse Problems\n\n\nBayesian Inference\n\n\nMon, Jan 29\n\n\n\n\nData Assimilation\n\n\nstatistical data assimilation\n\n\nWed, Jan 31\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nDifferentiable Programming\n\n\nFri, Sep 16\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 8 — A toolbox for digital twins section 8.8\n\n\nChapter 8 — A toolbox for digital twins section 9.4\n\n\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 02",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nAdjoint state\n\n\nInverse Problems\n\n\nWed, Jan 17\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nPyTorch intro\n\n\nFri, Jan 19\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 8 — A toolbox for digital twins section 8.7\n\n\n\n\n\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 01",
    "section": "",
    "text": "Topic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nWelcome to Digital Twins for Physical Systems\n\n\nIntroduction\n\n\nMon, Jan 08\n\n\n\n\nBasics\n\n\nData Assimilation\n\n\nWed, Jan 10\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weeks/week-01.html#footnotes",
    "href": "weeks/week-01.html#footnotes",
    "title": "Week 01",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is a lot of material there that you may want to read through. Use material presented in class as a guide what is important.↩︎"
  },
  {
    "objectID": "hw/w2-hw01.html",
    "href": "hw/w2-hw01.html",
    "title": "HW 01: Pet Names",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC-BY"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "",
    "text": "Course overview from CSE : Digital Twins for Physical Systems\nIBM defines “A digital twin is a virtual representation of an object or system that spans its lifecycle, is updated from real-time data, and uses simulation, machine learning and reasoning to help decision-making.” During this course, we will explore these concepts and their significance in addressing the challenges of monitoring and control of physical systems described by partial-differential equations. After introducing deterministic & statistical data assimilation techniques, the course switches gears towards scientific machine learning to introduce the technique of simulation-based inference, during which uncertainty is captured with generative conditional neural networks, and neural operators where Fourier Neural Operators act as surrogates for solutions of partial-differential equations. The course concludes by incorporating these techniques into uncertainty-aware Digital Twins that can be used to monitor and control complicated processes such as underground storage of CO2 or management of batteries.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "",
    "text": "Course overview from CSE : Digital Twins for Physical Systems\nIBM defines “A digital twin is a virtual representation of an object or system that spans its lifecycle, is updated from real-time data, and uses simulation, machine learning and reasoning to help decision-making.” During this course, we will explore these concepts and their significance in addressing the challenges of monitoring and control of physical systems described by partial-differential equations. After introducing deterministic & statistical data assimilation techniques, the course switches gears towards scientific machine learning to introduce the technique of simulation-based inference, during which uncertainty is captured with generative conditional neural networks, and neural operators where Fourier Neural Operators act as surrogates for solutions of partial-differential equations. The course concludes by incorporating these techniques into uncertainty-aware Digital Twins that can be used to monitor and control complicated processes such as underground storage of CO2 or management of batteries.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#class-meetings",
    "href": "index.html#class-meetings",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nHowey Physics N210\nMon & Wed 5:00 - 6:15PM",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Prerequisites",
    "text": "Prerequisites\nNumerical Linear Algebra, Statistics, Machine Learning, Experience w/ Python, or Julia",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nFelix J. Herrmann (Instructor)\nTBD\nZoom\n\n\nRafael Orozco (TA)\nTBD\nZoom",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#access-to-piazza",
    "href": "index.html#access-to-piazza",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Access to Piazza",
    "text": "Access to Piazza\nStudents are encouraged to post their questions on Piazza on Canvas or Piazza direct, which will be monitored by Rafael Orozco.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "slides/week-01/02-basics.html#text-book",
    "href": "slides/week-01/02-basics.html#text-book",
    "title": "Basics",
    "section": "Text book",
    "text": "Text book"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section",
    "href": "slides/week-01/02-basics.html#section",
    "title": "Basics",
    "section": "",
    "text": "INTRODUCTION"
  },
  {
    "objectID": "slides/week-01/02-basics.html#what-is-data-assimilation",
    "href": "slides/week-01/02-basics.html#what-is-data-assimilation",
    "title": "Basics",
    "section": "What is data assimilation",
    "text": "What is data assimilation\nSimplest view: a method of combining observations with model output.\nWhy do we need data assimilation? Why not just use the observations? (cf. Regression)\nWe want to predict the future!\n\nFor that we need models.\nBut when models are not constrained periodically by reality, they are of little value.\nTherefore, it is necessary to fit the model state as closely as possible to the observations, before a prediction is made."
  },
  {
    "objectID": "slides/week-01/02-basics.html#data-assimilation-methods",
    "href": "slides/week-01/02-basics.html#data-assimilation-methods",
    "title": "Basics",
    "section": "Data assimilation methods",
    "text": "Data assimilation methods\nThere are two major classes of methods:\n\nVariational methods where we explicitly minimize a cost function using optimization methods.\nStatistical methods where we compute the best linear unbiased estimate (BLUE) by algebraic computations using the Kalman filter.\n\nThey provide the same result in the linear case, which is the only context where their optimality can be rigorously proved.\nThey both have difficulties in dealing with non-linearities and large problems.\nThe error statistics that are required by both, are in general poorly known."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-approaches",
    "href": "slides/week-01/02-basics.html#introduction-approaches",
    "title": "Basics",
    "section": "Introduction: approaches",
    "text": "Introduction: approaches\nDA is an approach for solving a specific class of inverse, or parameter estimation problems, where the parameter we seek is the initial condition.\nAssimilation problems can be approached from many directions (depending on your background/preferences):\n\ncontrol theory;\nvariational calculus;\nstatistical estimation theory;\nprobability theory,\nstochastic differential equations.\n\nNewer approaches (discussed later): nudging methods, reduced methods, ensemble methods and hybrid methods that combine variational and statistical approaches, Machine/Deep Learning based approaches."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-approaches-1",
    "href": "slides/week-01/02-basics.html#introduction-approaches-1",
    "title": "Basics",
    "section": "Introduction: approaches",
    "text": "Introduction: approaches\n\nNavigation: important application of the Kalman filter.\nRemote sensing: satellite data.\nGeophysics: seismic exploration, geophysical prospecting, earthquake prediction.\nAir and noise pollution, source estimation\nWeather forecasting.\nClimatology. Global warming.\nEpidemiology.\nForest fire evolution.\nFinance."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-nonlinearity",
    "href": "slides/week-01/02-basics.html#introduction-nonlinearity",
    "title": "Basics",
    "section": "Introduction: nonlinearity",
    "text": "Introduction: nonlinearity\nThe problems of data assimilation (in particular) and inverse problems in general arise from:\n\nThe nonlinear dynamics of the physical model equations.\nThe nonlinearity of the inverse problem."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-iterative-process",
    "href": "slides/week-01/02-basics.html#introduction-iterative-process",
    "title": "Basics",
    "section": "Introduction: iterative process",
    "text": "Introduction: iterative process\n\nClosely related to\n\nthe inference cycle\nmachine learning…"
  },
  {
    "objectID": "slides/week-01/02-basics.html#forward-and-inverse-problems",
    "href": "slides/week-01/02-basics.html#forward-and-inverse-problems",
    "title": "Basics",
    "section": "Forward and inverse problems",
    "text": "Forward and inverse problems\n\n\n\n\n\n\n\nConsider a parameter-dependent dynamical system, \\[\\frac{dz}{dt}=g(t,z;\\theta),\\qquad z(t_{0})=z_{0},\\] with \\(g\\) known, \\(\\theta\\in\\Theta,\\) \\(z(t)\\in\\mathbb{R}^{k}.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-1",
    "href": "slides/week-01/02-basics.html#section-1",
    "title": "Basics",
    "section": "",
    "text": "Forward problem: Given \\(\\theta,\\) \\(z_{0},\\)find \\(z(t)\\) for \\(t\\ge t_{0}.\\)\nInverse problem: Given \\(z(t)\\)for \\(t\\ge t_{0},\\) find \\(\\theta\\in\\Theta.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#observations",
    "href": "slides/week-01/02-basics.html#observations",
    "title": "Basics",
    "section": "Observations",
    "text": "Observations\nObservation equation: \\[f(t,\\theta)=\\mathcal{H}z(t,\\theta),\\] where \\(\\mathcal{H}\\) is the observation operator — to account for the fact that observations are never completely known (in space-time).\nUsually we have a finite number of discrete (space-time) observations \\[\\left\\{\\widetilde{y}_{j}\\right\\} _{j=1}^{n},\\] where \\[\\widetilde{y}_{j}\\approx f(t_{j},\\theta).\\]"
  },
  {
    "objectID": "slides/week-01/02-basics.html#noise-free-and-noise-data",
    "href": "slides/week-01/02-basics.html#noise-free-and-noise-data",
    "title": "Basics",
    "section": "Noise-free and noise data",
    "text": "Noise-free and noise data\nNoise-free: \\[\\widetilde{y}_{j}=f(t_{j},\\theta)\\]\nNoisy Data: \\[\\widetilde{y}_{j}=f(t_{j},\\theta)+\\varepsilon_{j},\\] where \\(\\varepsilon_{j}\\) is error and requires that we introduce variability/uncertainty into the modeling and analysis."
  },
  {
    "objectID": "slides/week-01/02-basics.html#well-posedness",
    "href": "slides/week-01/02-basics.html#well-posedness",
    "title": "Basics",
    "section": "Well-posedness",
    "text": "Well-posedness\n\nExistence\nUniqueness\nContinuous dependence of solutions on observations.\n\n✓ The existence and uniqueness together are also known as “identifiability”.\n✓ The continuous dependence is related to the “stability” of the inverse problem."
  },
  {
    "objectID": "slides/week-01/02-basics.html#well-posednessmath",
    "href": "slides/week-01/02-basics.html#well-posednessmath",
    "title": "Basics",
    "section": "Well-posedness—math",
    "text": "Well-posedness—math\n\nDefinition 2 Let \\(X\\) and \\(Y\\) be two normed spaces and let \\(K\\::\\:X\\rightarrow Y\\) be a linear or nonlinear map between the two. The problem of finding \\(x\\) given \\(y\\) such that \\[Kx=y\\] is well-posed if the following three properties hold:\n\n\nWP1\n\nExistence—for every \\(y\\in Y\\) there is (at least) one solution \\(x\\in X\\) such that \\(Kx=y.\\)\n\nWP2\n\nUniqueness—for every \\(y\\in Y\\) there is at most one \\(x\\in X\\) such that \\(Kx=y.\\)\n\nWP3\n\nStability— the solution \\(x\\) depends continuously on the data \\(y\\) in that for every sequence \\(\\left\\{ x_{n}\\right\\} \\subset X\\) with \\(Kx_{n}\\rightarrow Kx\\) as \\(n\\rightarrow\\infty,\\) we have that \\(x_{n}\\rightarrow x\\) as \\(n\\rightarrow\\infty.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-2",
    "href": "slides/week-01/02-basics.html#section-2",
    "title": "Basics",
    "section": "",
    "text": "This concept of ill-posedness will help us to understand and distinguish between direct and inverse models.\nIt will provide us with basic comprehension of the methods and algorithms that will be used to solve inverse problems.\nFinally, it will assist us in the analysis of “what went wrong?” when we attempt to solve the inverse problems."
  },
  {
    "objectID": "slides/week-01/02-basics.html#ill-posedness-of-inverse-problems",
    "href": "slides/week-01/02-basics.html#ill-posedness-of-inverse-problems",
    "title": "Basics",
    "section": "Ill-posedness of inverse problems",
    "text": "Ill-posedness of inverse problems\nMany inverse problems are ill-posed!\nSimplest case: one observation \\(\\tilde{y}\\) for \\(f(\\theta)\\) and we need to find the pre-image \\[\\theta^{*}=f^{-1}(\\widetilde{y})\\] for a given \\(\\widetilde{y}.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#simplest-case",
    "href": "slides/week-01/02-basics.html#simplest-case",
    "title": "Basics",
    "section": "Simplest case",
    "text": "Simplest case\n\nNon-existence: there is no \\(\\theta_{3}\\) such that \\(f(\\theta_{3})=y_{3}\\)\nNon-uniqueness: \\(y_{j}=f(\\theta_{j})=f(\\widetilde{\\theta}_{j})\\) for \\(j=1,2.\\)\nLack of continuity of inverse map: \\(\\left|y_{1}-y_{2}\\right|\\) small \\(\\nRightarrow\\left|f^{-1}(y_{1})-f^{-1}(y_{2})\\right|=\\left|\\theta_{1}-\\widetilde{\\theta}_{2}\\right|\\) small."
  },
  {
    "objectID": "slides/week-01/02-basics.html#why-is-this-so-important",
    "href": "slides/week-01/02-basics.html#why-is-this-so-important",
    "title": "Basics",
    "section": "Why is this so important?",
    "text": "Why is this so important?\nCouldn’t we just apply a good least squares algorithm (for example) to find the best possible solution?\n\nDefine \\(J(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}\\) for a given \\(y_{1}\\)\nApply a standard iterative scheme, such as direct search or gradient-based minimization, to obtain a solution\nNewton’s method: \\[\\theta^{k+1}=\\theta^{k}-\\left[J'(\\theta^{k})\\right]^{-1}J(\\theta^{k})\\]\nLeads to highly unstable behavior because of ill-posedness"
  },
  {
    "objectID": "slides/week-01/02-basics.html#what-went-wrong",
    "href": "slides/week-01/02-basics.html#what-went-wrong",
    "title": "Basics",
    "section": "What went wrong",
    "text": "What went wrong\n✗ This behavior is not the fault of steepest descent algorithms.\n✗ It is a manifestation of the inherent ill-posedness of the problem.\n✗ How to fix this problem is the subject of much research over the past 50 years!!!\nMany remedies (fortunately) exist…\n\nexplicit and implicit constrained optimizations\nregularization and penalization\nmachine learning…"
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularization",
    "href": "slides/week-01/02-basics.html#tikhonov-regularization",
    "title": "Basics",
    "section": "Tikhonov regularization",
    "text": "Tikhonov regularization\nIdea: is to replace the ill-posed problem for \\(J(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}\\) by a “nearby” problem for \\[J_{\\beta}(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}+\\beta\\left|\\theta-\\theta_{0}\\right|^{2}\\] where \\(\\beta\\) is “suitably chosen” regularization/penalization parametersee below for details.\n\nWhen it is done correctly, TR provides convexity and compactness.\nEven when done correctly, it modifies the problem and new solutions may be far from the original ones.\nIt is not trivial to regularize correctly or even to know if you have succeeded…"
  },
  {
    "objectID": "slides/week-01/02-basics.html#non-uniqueness-seismic-travel-time-tomography",
    "href": "slides/week-01/02-basics.html#non-uniqueness-seismic-travel-time-tomography",
    "title": "Basics",
    "section": "Non uniqueness: seismic travel-time tomography",
    "text": "Non uniqueness: seismic travel-time tomography\n\n\n\n\n\nA signal seismic ray passes through a 2-parameter block model.\n\nunknowns are the 2 block slownesses (inverse of seismic velocity) \\(\\left(\\Delta s_{1},\\Delta s_{2}\\right)\\)\ndata is the observed travel time of the ray, \\(\\Delta t_{1}\\)\nmodel is the linearized travel time equation, \\(\\Delta t_{1}=l_{1}\\Delta s_{1}+l_{2}\\Delta s_{2}\\) where \\(l_{j}\\) is the length of the ray in the \\(j\\)-th block.\n\nClearly we have one equation for two unknowns and hence there is no unique solution."
  },
  {
    "objectID": "slides/week-01/02-basics.html#inverse-problems-general-formulation",
    "href": "slides/week-01/02-basics.html#inverse-problems-general-formulation",
    "title": "Basics",
    "section": "Inverse Problems: General Formulation",
    "text": "Inverse Problems: General Formulation\nAll inverse problems share a common formulation.\nLet the model parameters1 be a vector (in general, a multivariate random variable), \\(\\mathbf{m},\\) and the data be \\(\\mathbf{d},\\) \\[\\begin{aligned}\n    \\mathbf{m} & =\\left(m_{1},\\ldots,m_{p}\\right)\\in\\mathcal{M},\\\\\n    \\mathbf{d} & =\\left(d_{1},\\ldots,d_{n}\\right)\\in\\mathcal{D},\n    \\end{aligned}\\]\nwhere \\(\\mathcal{M}\\) and \\(\\mathcal{D}\\) are the corresponding model parameter space and data space.\nThe mapping \\(G\\colon\\mathcal{M}\\rightarrow\\mathcal{D}\\) is defined by the direct(or forward) model\n\\[\\mathbf{d}=g(\\mathbf{m}), \\qquad(1)\\] where\nApplied mathematicians often call the equation \\(G(m)=d\\) a mathematical model and \\(m\\) the parameters. Other scientists call \\(G\\) the forward operator and \\(m\\) the model. We will adopt the more mathematical convention, where \\(m\\) will be referred to as the model parameters, \\(G\\) the model and \\(d\\) the data."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-3",
    "href": "slides/week-01/02-basics.html#section-3",
    "title": "Basics",
    "section": "",
    "text": "– \\(g\\in G\\) is an operator that describes the “physical” model and can take numerous forms, such as algebraic equations, differential equations, integral equations, or linear systems.\n\nThen we can add the observationsor predictions, \\(\\mathbf{y}=(y_{1},\\ldots,y_{r}),\\) corresponding to the mapping from data space into observation space, \\(H\\colon\\mathcal{D}\\rightarrow\\mathcal{Y},\\) and described by \\[\\mathbf{y}=h(\\mathbf{d})=h\\left(g(\\mathbf{m})\\right),\\] where\n\\(h\\in H\\) is the observation operator, usually some projection into an observable subset of \\(\\mathcal{D}.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-4",
    "href": "slides/week-01/02-basics.html#section-4",
    "title": "Basics",
    "section": "",
    "text": "Note\n\n\nIn addition, there will be some random noise in the system, usually modeled as additive noise, giving the more realistic, stochastic direct model \\[\\mathbf{d}=g(\\mathbf{m})+\\mathbf{\\epsilon},\\label{eq:stat-inv-pb} \\qquad(2)\\] where \\(\\mathbf{\\epsilon}\\) is a random vector."
  },
  {
    "objectID": "slides/week-01/02-basics.html#inverse-problemsclassification",
    "href": "slides/week-01/02-basics.html#inverse-problemsclassification",
    "title": "Basics",
    "section": "Inverse Problems—Classification",
    "text": "Inverse Problems—Classification\nWe can now classify inverse problems as:\n\ndeterministic inverse problems that solve 1 for \\(\\mathbf{m},\\)\nstatistical inverse problems that solve 2 for \\(\\mathbf{m}.\\)\n\nThe first class will be treated by linear algebra and optimizationmethods.\nThe latter can be treated by a Bayesian (filtering) approach, and by weighted least-squares, maximum likelihood and DA techniques\nBoth classes can be further broken down into:\n\nLinear inverse problems, where 1 or 2 are linear equations. These include linear systems that are often the result of discretizing (partial) differential equations and integral equations.\nNonlinear inverse problems where the algebraic or differential operators are nonlinear."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-5",
    "href": "slides/week-01/02-basics.html#section-5",
    "title": "Basics",
    "section": "",
    "text": "Finally, since most inverse problems cannot be solved explicitly, computational methods are indispensable for their solution see Asch (2022)\nAlso note that we will be inverting here between the model and data spaces, that are usually both of high dimension and thus this model-based inversion will invariably be computationally expensive.\nThis will motivate us to employ\n\ninversion between the data and observation spaces in a purely data-driven approach, using machine learning methods"
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularizationintroduction",
    "href": "slides/week-01/02-basics.html#tikhonov-regularizationintroduction",
    "title": "Basics",
    "section": "Tikhonov Regularization—Introduction",
    "text": "Tikhonov Regularization—Introduction\nTikhonov regularization (TR) is probably the most widely used method for regularizing ill-posed, discrete and continuous inverse problems.\n\n\n\n\n\n\nNote\n\n\nNote that the LASSO and ridge regression methods—see ML Lectures—are special cases of TR.\n\n\n\nThe theory is the subject of entire books...\nRecall:\n\nthe objective of TR is to reduce, or remove, ill-posedness in optimization problems by modifying the objective function.\nthe three sources of ill-posedness: non-existence, non-uniqueness and sensitivity to perturbations.\nTR, in principle, addresses and alleviates all three sources of ill-posedness and is thus a vital tool for the solution of inverse problems."
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularizationformulation",
    "href": "slides/week-01/02-basics.html#tikhonov-regularizationformulation",
    "title": "Basics",
    "section": "Tikhonov Regularization—Formulation",
    "text": "Tikhonov Regularization—Formulation\nThe most general TR objective function is \\[\\mathcal{T}_{\\alpha}(\\mathbf{m};\\mathbf{d})=\\rho\\left(G(\\mathbf{m}),\\mathbf{d}\\right)+\\alpha J(\\mathbf{m}),\\] where\n\n\\(\\rho\\) is the data discrepancy functional that quantifies the difference between the model output and the measured data;\n\\(J\\) is the regularization functional that represents some desired quality of the sought for model parameters, usually smoothness;\n\\(\\alpha\\) is the regularization parameter that needs to be tuned, and determines the relative importance of the regularization term.\n\nEach domain, each application and each context will require specific choicesof these three items, and often we will have to rely either on previous experience, or on some sort of numerical experimentation (trial-and-error) to make a good choice.\nIn some cases there exist empirical algorithms, in particular for the choice of \\(\\alpha.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularizationdiscrepancy",
    "href": "slides/week-01/02-basics.html#tikhonov-regularizationdiscrepancy",
    "title": "Basics",
    "section": "Tikhonov Regularization—Discrepancy",
    "text": "Tikhonov Regularization—Discrepancy\nThe most common discrepancy functions are:\n\nleast-squares, \\[\\rho_{\\mathrm{LS}}(\\mathbf{d}_{1},\\mathbf{d}_{2})=\\frac{1}{2}\\Vert\\mathbf{d}_{1}-\\mathbf{d}_{2}\\Vert_{2}^{2},\\]\n1-norm, \\[\\rho_{1}(\\mathbf{d}_{1},\\mathbf{d}_{2})=\\vert\\mathbf{d}_{1}-\\mathbf{d}_{2}\\vert,\\]\nKullback-Leibler distance, \\[\\rho_{\\mathrm{KL}}(d_{1},d_{2})=\\left\\langle d_{1},\\log(d_{1}/d_{2})\\right\\rangle ,\\] where \\(d_{1}\\) and \\(d_{2}\\) are considered here as probability density functions. This discrepancy is valid in the Bayesian context."
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularization-1",
    "href": "slides/week-01/02-basics.html#tikhonov-regularization-1",
    "title": "Basics",
    "section": "Tikhonov Regularization",
    "text": "Tikhonov Regularization\nThe most common regularization functionals are derivatives of order one or two.\n\nGradient smoothing: \\[J_{1}(\\mathbf{m})=\\frac{1}{2}\\Vert\\nabla\\mathbf{m}\\Vert_{2}^{2},\\] where \\(\\nabla\\) is the gradient operator of first-order derivatives of the elements of \\(\\mathbf{m}\\) with respect to each of the independent variables.\nLaplacian smoothing: \\[J_{2}(\\mathbf{m})=\\frac{1}{2}\\Vert\\nabla^{2}\\mathbf{m}\\Vert_{2}^{2},\\] where \\(\\nabla^{2}=\\nabla\\cdot\\nabla\\) is the Laplacian operator defined as the sum of all second-order derivatives of \\(\\mathbf{m}\\) with respect to each of the independent variables."
  },
  {
    "objectID": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter",
    "href": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter",
    "title": "Basics",
    "section": "TR—Computing the Regularization Parameter",
    "text": "TR—Computing the Regularization Parameter\nOnce the data discrepancy and regularization functionals have been chosen, we need to tune the regularization parameter, \\(\\alpha.\\)\nWe have here, similarly to the bias-variance trade-off of ML Lectures, a competition between the discrepancy error and the magnitude of the regularization term.\nWe need to choose, the best compromise between the two.\nWe will briefly present three frequently used approaches:\n\nL-curve method.\nDiscrepancy principle.\nCross-validation and LOOCV."
  },
  {
    "objectID": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter-1",
    "href": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter-1",
    "title": "Basics",
    "section": "TR—Computing the Regularization Parameter",
    "text": "TR—Computing the Regularization Parameter\n\n\n\n\n\n\nFigure 1\n\n\n\nThe L-curve criterion is an empirical method for picking a value of \\(\\alpha.\\)\n\nsince \\(e_{m}(\\alpha)=\\Vert\\mathbf{m}\\Vert_{2}\\) is a strictly decreasing function of \\(\\alpha\\) and \\(e_{d}(\\alpha)=\\Vert G\\mathbf{m}-\\mathbf{d}\\Vert_{2}\\) is a strictly increasing one,\nwe plot \\(\\log e_{m}\\) against \\(\\log e_{d}\\) we will always obtain an L-shaped curve that has an “elbow” at the optimal value of \\(\\alpha=\\alpha_{L},\\) or at least at a good approximation of this optimal value see Figure 1."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-6",
    "href": "slides/week-01/02-basics.html#section-6",
    "title": "Basics",
    "section": "",
    "text": "This trade-off curve gives us a visual recipe for choosing the regularization parameter, reminiscent of the bias-variance trade off\nThe range of values of \\(\\alpha\\) for which one should plot the curve has to be determined by either trial-and-error, previous experience, or a balancing of the two terms in the TR functional.\n\nThe discrepancy principle\n\nchoose the value of \\(\\alpha=\\alpha_{D}\\) such that the residual error (first term) is equal to an a priori bound, \\(\\delta,\\) that we would like to attain.\non the L-curve, this corresponds to the intersection with the vertical line at this bound, as shown in Figure 1.\na good approximation for the bound is to put \\(\\delta=\\sigma\\sqrt{n},\\) where \\(\\sigma^{2}\\) is the variance and \\(n\\) the number of observations.1 This can be thought of as the noise level of the data.\n\nThis is strictly valid under the hypothesis of i.i.d. Gaussian noise."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-7",
    "href": "slides/week-01/02-basics.html#section-7",
    "title": "Basics",
    "section": "",
    "text": "the discrepancy principle is also related to regularization by the truncated singular value decomposition (TSVD), in which case the truncation level implicitly defines the regularization parameter.\n\nCross-validation, as we explained in ML Lectures, is a way of using the observations themselves to estimate a parameter.\n\nWe then employ the classical approach of either LOOCV or \\(k\\)-fold cross validation, and choose the value of \\(\\alpha\\) that minimizes the RSS (Residual Sum of Squares) of the test sets.\nIn order to reduce the computational cost, a generalized cross validation (GCV) method can be used."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-8",
    "href": "slides/week-01/02-basics.html#section-8",
    "title": "Basics",
    "section": "",
    "text": "DATA ASSIMILATION"
  },
  {
    "objectID": "slides/week-01/02-basics.html#definitions-and-notation",
    "href": "slides/week-01/02-basics.html#definitions-and-notation",
    "title": "Basics",
    "section": "Definitions and notation",
    "text": "Definitions and notation\nAnalysis is the process of approximating the true state of a physical system at a given time\nAnalysis is based on:\n\nobservational data,\na model of the physical system,\nbackground information on initial and boundary conditions.\n\nAn analysis that combines time-distributed observations and a dynamic model is called data assimilation."
  },
  {
    "objectID": "slides/week-01/02-basics.html#standard-notation",
    "href": "slides/week-01/02-basics.html#standard-notation",
    "title": "Basics",
    "section": "Standard notation",
    "text": "Standard notation\nA discrete model for the evolution of a physical (atmospheric, oceanic, etc.) system from time \\(t_{k}\\) to time \\(t_{k+1}\\) is described by a dynamic, state equation\n\\[\\mathbf{x}^{f}(t_{k+1})=M\\left[\\mathbf{x}^{f}(t_{k})\\right], \\qquad(3)\\]\n\n\\(\\mathbf{x}\\) is the model’s state vector of dimension \\(n,\\)\n\\(M\\) is the corresponding dynamics operator (finite difference or finite element discretization).\n\nThe error covariance matrix associated with \\(\\mathbf{x}\\) is given by \\(\\mathbf{P}\\) since the true state will differ from the simulated state ([eq:state]) by random or systematic errors.\nObservations, or measurements, at time \\(t_{k}\\) are defined by"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-9",
    "href": "slides/week-01/02-basics.html#section-9",
    "title": "Basics",
    "section": "",
    "text": "\\[\\mathbf{y}_{k}^{\\mathrm{o}}=H_{k}\\left[\\mathbf{x}^{t}(t_{k})\\right]+\\varepsilon_{k},\\]\n\n\\(H\\) is an observation operator\n\\(\\varepsilon\\) is a white noise process zero mean and covariance matrix \\(\\mathbf{R}\\) (instrument errors and representation errors due to the discretization)\nobservation vector \\(\\mathbf{y}_{k}^{\\mathrm{o}}=\\mathbf{y}^{\\mathrm{o}}(t_{k})\\) has dimension \\(p_{k}\\) (usually \\(p_{k}\\ll n.\\) )\n\nSubscripts are used to denote the discrete time index, the corresponding spatial indices or the vector with respect to which an error covariance matrix is defined.\nSuperscripts refer to the nature of the vectors/matrices\n\n“a” for analysis,\n“b” for background (or ‘initial/first guess’),\n“f” for forecast,\n“o” for observation and\n“t” for the (unknown) true state."
  },
  {
    "objectID": "slides/week-01/02-basics.html#standard-notationcontinuous-system",
    "href": "slides/week-01/02-basics.html#standard-notationcontinuous-system",
    "title": "Basics",
    "section": "Standard notation—continuous system",
    "text": "Standard notation—continuous system\nNow let us introduce the continuous system. In fact, continuous time simplifies both the notation and the theoretical analysis of the problem. For a finite-dimensional system of ordinary differential equations, the sate and observation equations become \\[\\dot{\\mathbf{x}}^{\\mathrm{f}}=\\mathcal{M}(\\mathbf{x}^{\\mathrm{f}},t)%\\mathbf{\\dot{\\xf}}=\\mathcal{M}(\\xf,t)\\] and \\[\\mathbf{y}^{\\mathrm{o}}(t)=\\mathcal{H}(\\mathbf{x}^{\\mathrm{t}},t)+\\boldsymbol{\\epsilon},\\] where \\(\\dot{\\left(\\,\\right)}=\\mathrm{d}/\\mathrm{d}t,\\) \\(\\mathcal{M}\\) and \\(\\mathcal{H}\\) are nonlinear operators in continuous time for the model and the observation respectively.\nThis implies that \\(\\mathbf{x},\\) \\(\\mathbf{y},\\) and \\(\\boldsymbol{\\epsilon}\\) are also continuous-in-time functions.\n\nFor PDEs, where there is in addition a dependence on space, attention must be paid to the function spaces, especially when performing variational analysis."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-10",
    "href": "slides/week-01/02-basics.html#section-10",
    "title": "Basics",
    "section": "",
    "text": "With a PDE model, the field (state) variable is commonly denoted by \\(\\boldsymbol{u}(\\mathbf{x},t),\\) where \\(\\mathbf{x}\\) represents the space variables (no longer the state variable as above!), and the model dynamics is now a nonlinear partial differential operator, \\[\\mathcal{M}=\\mathcal{M}\\left[\\partial_{\\mathbf{x}}^{\\alpha},\\boldsymbol{u}(\\mathbf{x},t),\\mathbf{x},t\\right]\\] with \\(\\partial_{\\mathbf{x}}^{\\alpha}\\) denoting the partial derivatives with respect to the space variables of order up to \\(\\left|\\alpha\\right|\\le m\\) where \\(m\\) is usually equal to two and in general varies between one and four."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-11",
    "href": "slides/week-01/02-basics.html#section-11",
    "title": "Basics",
    "section": "",
    "text": "CONCLUSIONS"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-12",
    "href": "slides/week-01/02-basics.html#section-12",
    "title": "Basics",
    "section": "",
    "text": "Data assimilation requires not only the observations and a background, but also knowledge of:\n\nerror statistics(background, observation, model, etc.)\nphysics (forecast model, model relating observed to retrieved variables, etc.).\n\nThe challenge of data assimilation is in combining our stochastic knowledge with our physical knowledge."
  },
  {
    "objectID": "slides/week-01/02-basics.html#open-source-software",
    "href": "slides/week-01/02-basics.html#open-source-software",
    "title": "Basics",
    "section": "Open-source software",
    "text": "Open-source software\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen."
  },
  {
    "objectID": "slides/week-01/02-basics.html#references",
    "href": "slides/week-01/02-basics.html#references",
    "title": "Basics",
    "section": "References",
    "text": "References\n\nK. Law, A. Stuart, K. Zygalakis. Data Assimilation. A Mathematical Introduction. Springer, 2015.\nG. Evensen. Data assimilation, The Ensemble Kalman Filter, 2nd ed., Springer, 2009.\nA. Tarantola. Inverse problem theory and methods for model parameter estimation. SIAM. 2005.\nO. Talagrand. Assimilation of observations, an introduction. J. Meteorological Soc. Japan, 75, 191, 1997.\nF.X. Le Dimet, O. Talagrand. Variational algorithms for analysis and assimilation of meteorological observations: theoretical aspects. Tellus, 38(2), 97, 1986.\nJ.-L. Lions. Exact controllability, stabilization and perturbations for distributed systems. SIAM Rev., 30(1):1, 1988.\nJ. Nocedal, S.J. Wright. Numerical Optimization. Springer, 2006.\nF. Tröltzsch. Optimal Control of Partial Differential Equations. AMS, 2010.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. SIAM.\n\n\n\n\n\n🔗 https://flexie.github.io/CSE-8803-Twin/"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#bayesian-filters",
    "href": "slides/week-05/08-DA-Bayes.html#bayesian-filters",
    "title": "Data Assimilation",
    "section": "Bayesian filters",
    "text": "Bayesian filters\nWe begin by defining a probabilistic state-space, or nonlinear filtering model, of the form \\[\\begin{aligned}\n    \\mathbf{x}_{k} & \\sim p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\label{eq:filter1}\\\\\n    \\mathbf{y}_{k} & \\sim p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}),\\quad k=0,1,2,\\ldots,\\label{eq:filter2}\n    \\end{aligned} \\tag{1}\\] where\n\n\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) is the state vector at time \\(k,\\)\n\\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}\\) is the observation vector at time \\(k,\\)\nthe conditional probability, \\(p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\) represents the stochasticdynamics model, and can be a probability density or a discrete probability function, or a mixture of both,\nthe conditional probability, \\(p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}),\\) represents themeasurement model and its inherent noise."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section",
    "href": "slides/week-05/08-DA-Bayes.html#section",
    "title": "Data Assimilation",
    "section": "",
    "text": "In addition, we assume that the model is Markovian, such that \\[p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{1:k-1},\\mathbf{y}_{1:k-1})=p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\] and that the observations are conditionally independent of state and measurement histories, \\[p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{1:k},\\mathbf{y}_{1:k-1})=p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}).\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#example-of-gaussian-random-walk",
    "href": "slides/week-05/08-DA-Bayes.html#example-of-gaussian-random-walk",
    "title": "Data Assimilation",
    "section": "Example of Gaussian Random Walk",
    "text": "Example of Gaussian Random Walk\nTo fix ideas and notation, we begin with a very simple, scalar case, the Gaussian random walk model. This model can then easily be generalized.\nConsider the scalar system \\[\\begin{aligned}\n    x_{k} & =x_{k-1}+w_{k-1},\\quad w_{k-1}\\sim\\mathcal{N}(0,Q),\\label{eq:GRW-x}\\\\\n    y_{k} & =x_{k}+v_{k},\\quad v_{k}\\sim\\mathcal{N}(0,R),\\label{eq:GRW-y}\n    \\end{aligned} \\tag{2}\\] where \\(x_{k}\\) is the (hidden) state and \\(y_{k}\\) is the (known) measurement.\nNoting that \\(x_{k}-x_{k-1}=w_{k-1}\\) and that \\(y_{k}-x_{k}=v_{k},\\) we can immediately rewrite this system in terms of the conditional probability densities, \\[\\begin{aligned}\n    p(x_{k}\\mid x_{k-1}) & =\\mathcal{N}\\left(x_{k}\\mid x_{k-1},Q\\right)\\\\\n     & =\\dfrac{1}{\\sqrt{2\\pi Q}}\\exp\\left[-\\frac{1}{2Q}\\left(x_{k}-x_{k-1}\\right)^{2}\\right]\n    \\end{aligned}\\] and"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-1",
    "href": "slides/week-05/08-DA-Bayes.html#section-1",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    p(y_{k}\\mid x_{k}) & =\\mathcal{N}\\left(y_{k}\\mid x_{k},R\\right)\\\\\n     & =\\dfrac{1}{\\sqrt{2\\pi R}}\\exp\\left[-\\frac{1}{2R}\\left(y_{k}-x_{k}\\right)^{2}\\right].\n    \\end{aligned}\\]\nA realization of the model is shown in Figure 1.\n\n\nFigure 1: Gaussian random walk state space model equations 2. State, \\(x_k\\) is solid blue curve, measurements, \\(y_k\\), are red circles. Fixed values of noise variance are \\(Q=1\\) and \\(R=1\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#code",
    "href": "slides/week-05/08-DA-Bayes.html#code",
    "title": "Data Assimilation",
    "section": "Code",
    "text": "Code\n% Simulate a Gaussian random walk.\n% initialize\nrandn('state',123)\nR=1; Q=1; K=100;\n% simulate\nX_init = sqrt(Q)\\*randn(K,1);\nX = cumsum(X_init);\nW = sqrt(R)\\*randn(K,1);\nY = X + W;\n% plot\nplot(1:K,X,1:K,Y(1:K,1),'ro')\nxlabel('k'), ylabel('x_k')"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#nonlinear-filter-model",
    "href": "slides/week-05/08-DA-Bayes.html#nonlinear-filter-model",
    "title": "Data Assimilation",
    "section": "Nonlinear Filter Model",
    "text": "Nonlinear Filter Model\nUsing the nonlinear filtering model 1 and the Markov property, we can express thejoint prior of the states, \\(\\mathbf{x}_{0:T}=\\{\\mathbf{x}_{0},\\ldots,\\mathbf{x}_{T}\\},\\) and the joint likelihood of the measurements, \\(\\mathbf{y}_{1:T}=\\{\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{T}\\},\\) as the products \\[p(\\mathbf{x}_{0:T})=p(\\mathbf{x}_{0})\\prod_{k=1}^{T}p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1})\\] and \\[p(\\mathbf{y}_{1:T}\\mid\\mathbf{x}_{0:T})=\\prod_{k=1}^{T}p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})\\] respectively."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-2",
    "href": "slides/week-05/08-DA-Bayes.html#section-2",
    "title": "Data Assimilation",
    "section": "",
    "text": "Then, applying Bayes’ law, we can compute the complete posterior distribution of the states as \\[p(\\mathbf{x}_{0:T}\\mid\\mathbf{y}_{1:T})=\\frac{p(\\mathbf{y}_{1:T}\\mid\\mathbf{x}_{0:T})p(\\mathbf{x}_{0:T})}{p(\\mathbf{y}_{1:T})}.\\label{eq:Bayes-post-state-eq}\\]\nBut this type of complete characterization is not feasible to compute in real-time, or near real-time, since the number of computations per time-step increases as measurements arrive.\nWhat we need is afixed number of computations per time-step.\n\nThis can be achieved by a recursive estimation that, step by step, produces the filtering distribution defined above.\nIn this light, we can now define the general Bayesian filtering problem, of which Kalman filters will be a special case."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-3",
    "href": "slides/week-05/08-DA-Bayes.html#section-3",
    "title": "Data Assimilation",
    "section": "",
    "text": "Bayesian filtering is the recursive computation of the marginal posterior distribution, \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\] known as the filtering distribution, of the state \\(\\mathbf{x}_{k}\\) at each time step \\(k,\\) given the measurements up to time \\(k.\\)\n\nNow, based on Bayes’ rule, we can formulate the Bayesian filtering theorem (Särkkä and Svensson 2023) .\n\nTheorem 1 (Bayesian Filter). The recursive equations, known as the Bayesian filter, for computing the filtering distribution \\(p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\) and the predicted distribution \\(p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})\\) at the time step \\(k,\\) are given by the three-stage process:\nInitialization: Define the prior distribution \\(p(\\mathbf{x}_{0}).\\)\nPrediction: Compute the predictive distribution \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})=\\int p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1})p(\\mathbf{x}_{k-1}\\mid\\mathbf{y}_{1:k-1})\\,\\mathrm{d}\\mathbf{x}_{k-1}.\\]\nCorrection: Compute the posterior distribution by Bayes’ rule,*"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-4",
    "href": "slides/week-05/08-DA-Bayes.html#section-4",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})=\\frac{p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})}{\\int p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})}\\,\\mathrm{d}\\mathbf{x}_{k}.\\]\nNow, if we assume that the dynamic and measurement models are linear, with i.i.d. Gaussian noise, then we obtain the closed-form solution for the Kalman filter, already derived in the Basic Course. We recall the linear, Gaussian state-space model, \\[\\begin{aligned}\n\\mathbf{x}_{k} & =\\mathbf{M}_{k-1}\\mathbf{x}_{k-1}+\\mathbf{w}_{k-1},\\label{eq:kf-1}\\\\\n\\mathbf{y}_{k} & =\\mathbf{H}_{k}\\mathbf{x}_{k}+\\mathbf{v}_{k},\\label{eq:kf-2}\n\\end{aligned} \\tag{3}\\] for the Kalman filter, where\n\n\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) is the state,\n\\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}\\) is the measurement,\n\\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1})\\) is the process noise,\n\\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k})\\) is the measurement noise,\n\\(\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{m}_{0},\\mathbf{P}_{0})\\) is the Gaussian distributed initial state, with mean \\(\\mathbf{m}_{0}\\) and covariance \\(\\mathbf{P}_{0},\\)\n\\(\\mathbf{M}_{k-1}\\) is the time-dependent transition matrix of the dynamic model at time \\(k-1,\\) and\n\\(\\mathbf{H}_{k}\\) is the time-dependent measurement model matrix."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-5",
    "href": "slides/week-05/08-DA-Bayes.html#section-5",
    "title": "Data Assimilation",
    "section": "",
    "text": "This model can be very elegantly rewritten in terms of conditional probabilities as \\[\\begin{aligned}\np(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}) & =\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{M}_{k-1}\\mathbf{x}_{k-1},\\mathbf{Q}_{k-1}\\right),\\\\\np(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}) & =\\mathcal{N}\\left(\\mathbf{y}_{k}\\mid\\mathbf{H}_{k}\\mathbf{x}_{k},\\mathbf{R}_{k}\\right).\n\\end{aligned}\\]\n\nTheorem 2 (Kalman Filter). The Bayesian filtering equations for the linear, Gaussian model Equation 3 can be explicitly computed and the resulting conditional probability distributions are Gaussian. The prediction distribution is \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})=\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\hat{\\mathbf{m}}_{k},\\hat{\\mathbf{P}}_{k}\\right),\\] the filtering distribution is \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})=\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right)\\] and the smoothing distribution is \\[p(\\mathbf{y}_{k}\\mid\\mathbf{y}_{1:k-1})=\\mathcal{N}\\left(\\mathbf{y}_{k}\\mid\\mathbf{H}_{k}\\mathbf{\\hat{m}}_{k},\\mathbf{S}_{k}\\right).\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-6",
    "href": "slides/week-05/08-DA-Bayes.html#section-6",
    "title": "Data Assimilation",
    "section": "",
    "text": "The parameters of these distributions can be computed by the three-stage Kalman filter loop:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)*\nPrediction: Compute the predictive distribution mean and covariance,* \\[\\begin{aligned}\n\\mathbf{\\hat{m}}_{k} & =\\mathbf{M}_{k-1}\\mathbf{m}_{k-1},\\\\\n\\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{k-1}\\mathbf{P}_{k-1}\\mathbf{M}_{k-1}^{\\mathrm{T}}+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]\nCorrection: Compute the filtering distribution mean and covariance by first defining* \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{\\hat{m}}_{k},\\quad\\textrm{the innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{k}\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k},\\quad\\textrm{the measurement covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{k}^{\\mathrm{T}}\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\] then finally updating the filter mean and covariance,"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-7",
    "href": "slides/week-05/08-DA-Bayes.html#section-7",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]\n\nProof. The proof see Särkkä and Svensson (2023) is a direct application of classical results for the joint, marginal, and conditional distributions of two Gaussian random variables, \\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}.\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#kf-for-gaussian-random-walk",
    "href": "slides/week-05/08-DA-Bayes.html#kf-for-gaussian-random-walk",
    "title": "Data Assimilation",
    "section": "KF for Gaussian Random Walk",
    "text": "KF for Gaussian Random Walk\nWe now return to the Gaussian random walk model seen above in the Example, and formulate a Kalman filter for estimating its state from noisy measurements.\n\nExample 1 Kalman Filter for Gaussian Random Walk. Suppose that we have measurements of the scalar \\(y_{k}\\) from the Gaussian random walk model \\[\\begin{aligned}\nx_{k} & =x_{k-1}+w_{k-1},\\quad w_{k-1}\\sim\\mathcal{N}(0,Q),\\label{eq:GRW-x2}\\\\\ny_{k} & =x_{k}+v_{k},\\quad v_{k}\\sim\\mathcal{N}(0,R).\\label{eq:GRW-y2}\n\\end{aligned} \\tag{4}\\] This very basic system is found in many applications where\n\n\\(x_{k}\\) represents a slowly varying quantity that we measure directly.\nprocess noise, \\(w_{k},\\) takes into account fluctuations in the state \\(x_{k}.\\)\nmeasurement noise, \\(v_{k},\\) accounts for measurement instrument errors.\n\n\nWe want to estimate the state \\(x_{k}\\) over time, taking into account the measurements \\(y_{k}.\\) That is, we would like to compute the filtering density,"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-8",
    "href": "slides/week-05/08-DA-Bayes.html#section-8",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[p({x}_{k}\\mid{y}_{1:k})=\\mathcal{N}\\left({x}_{k}\\mid{m}_{k},{P}_{k}\\right).\\] We proceed by simply writing down the three stages of the Kalman filter, noting that \\(M_{k}=1\\) and \\(H_{k}=1\\) for this model. We obtain:\nInitialization: Define the prior mean \\({m}_{0}\\) and prior covariance \\({P}_{0}.\\)\nPrediction: \\[\\begin{aligned}\n{\\hat{m}}_{k} & ={m}_{k-1},\\\\\n\\hat{{P}}_{k} & ={P}_{k-1}+{Q}.\n\\end{aligned}\\]\nCorrection: Define \\[\\begin{aligned}\n{d}_{k} & ={y}_{k}-{\\hat{m}}_{k},\\quad\\textrm{the innovation},\\\\\n{S}_{k} & =\\hat{{P}}_{k}+{R},\\quad\\textrm{the measurement covariance},\\\\\n{K}_{k} & =\\hat{{P}}_{k}{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-9",
    "href": "slides/week-05/08-DA-Bayes.html#section-9",
    "title": "Data Assimilation",
    "section": "",
    "text": "then update, \\[\\begin{aligned}\n{m}_{k} & ={\\hat{m}}_{k}+K_{k}{d}_{k},\\\\\nP_{k} & =\\hat{P}_{k}-\\frac{\\hat{P}_{k}^{2}}{S_{k}}.\n\\end{aligned}\\]\nIn Figure 2, we show simulations with system noise standard deviation of \\(1\\) and measurement noise standard deviation of \\(0.5.\\) We observe that the KF tracks the random walk very efficiently.\n\n\nFigure 2: Kalman filter for tracking a Gaussian random walk state space model 4“. State, \\(x_k\\), is solid blue curve; measurements, \\(y_k\\), are red circles; Kalman filter estimate is green curve and 95% quantiles are shown. Fixed values of noise variances are \\(Q=1\\) and \\(R=0.5^2\\). Results computed by kf_gauss_state.m."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#code-1",
    "href": "slides/week-05/08-DA-Bayes.html#code-1",
    "title": "Data Assimilation",
    "section": "Code",
    "text": "Code\n\n\n% Kalman Filter for scalar Gaussian random walk\n% Set parameters\nsig_w = 1; sig_v = 0.5;\nM = 1;\nQ = sig_w\\^2;\nH = 1;\nR = sig_v\\^2;\n% Initialize\nm0 = 0;\nP0 = 1;\n% Simulate data\nrandn('state',1234);\nsteps = 100; T = \\[1:steps\\];\nX = zeros(1,steps);\nY = zeros(1,steps);\nx = m0;\nfor k=1:steps\n  w = Q'\\*randn(1);\n  x = M\\*x + w;\n  y = H\\*x + sig_v\\*randn(1);\n  X(k) = x;\n  Y(k) = y;\nend\nplot(T,X,'-',T,Y,'.');\nlegend('Signal','Measurements');\nxlabel('{k}'); ylabel('{x}\\_k');\n% Kalman filter\nm = m0;\nP = P0;\nfor k=1:steps\n  m = M\\*m;\n  P = M\\*P\\*M' + Q;\n  d = Y(:,k) - H\\*m;\n  S = H\\*P\\*H' + R;\n  K = P\\*H'/S;\n  m = m + K\\*d;\n  P = P - K\\*S\\*K';\n  kf_m(k) = m;\n  kf_P(k) = P;\nend\n% Plot   \nclf; hold on\nfill(\\[T fliplr(T)\\],\\[kf_m+1.96\\*sqrt(kf_P) \\...\n  fliplr(kf_m-1.96\\*sqrt(kf_P))\\],1, \\...\n  'FaceColor',\\[.9 .9 .9\\],'EdgeColor',\\[.9 .9 .9\\])\nplot(T,X,'-b',T,Y,'or',T, kf_m(1,:),'-g')\nplot(T,kf_m+1.96\\*sqrt(kf_P),':r',T,kf_m-1.96\\*sqrt(kf_P),':r');\nhold off\n\n\n\n\n\n\n\nNote\n\n\n\nLine 3: by modifying these noise amplitudes, one can better understand how the KF operates.\nLines 31-32 and 34-38: the complete filter is coded in only 7 lines, exactly as prescribed by Theorem 5. This is the reason for the excellent performance of the KF, in particular in real-time systems. In higher dimensions, when the matrices become large, more attention must be paid to the numerical linear algebra routines used. The inversion of the measurement covariance matrix, \\(S,\\) in line 36, is particularly challenging and requires highly tuned decomposition methods."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-10",
    "href": "slides/week-05/08-DA-Bayes.html#section-10",
    "title": "Data Assimilation",
    "section": "",
    "text": "EXTENDED KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#extended-kalman-filters",
    "href": "slides/week-05/08-DA-Bayes.html#extended-kalman-filters",
    "title": "Data Assimilation",
    "section": "Extended Kalman filters",
    "text": "Extended Kalman filters\nIn real applications, we are usually confronted with nonlinearity in the model and in the measurements.\n\nMoreover, the noise is not necessarily additive.\n\nTo deal with these nonlinearities, one possible approach is to linearize about the current mean and covariance, which produces the extended Kalman filter (EKF).\nThis filter is widely accepted as the standard for navigation and GPS systems, among others.\nRecall the nonlinear problem, \\[\\begin{aligned}\n\\mathbf{x}_{k} & = & \\mathcal{M}_{k-1}(\\mathbf{x}_{k-1})+\\mathbf{w}_{k-1},\\label{eq:state_nl}\\\\\n\\mathbf{y}_{k} & = & \\mathcal{H}_{k}(\\mathbf{x}_{k})+\\mathbf{v}_{k},\\label{eq:obs_nl}\n\\end{aligned} \\tag{5}\\] where"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-11",
    "href": "slides/week-05/08-DA-Bayes.html#section-11",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n},\\) \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m},\\) \\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1}),\\) \\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k}),\\)\nand now \\(\\mathcal{M}_{k-1}\\) and \\(\\mathcal{H}_{k}\\) are nonlinear functions of \\(\\mathbf{x}_{k-1}\\) and \\(\\mathbf{x}_{k}\\) respectively.\n\nThe EKF is then based on Gaussian approximationsof the filtering densities, \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\approx\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right),\\] where these approximations are derived from the first-order truncation of the corresponding Taylor series in terms of the statistical moments of the underlying random variables.\nLinearization in the Taylor series expansions will require evaluation of the Jacobian matrices, defined as \\[\\mathbf{M}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{M}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}\\] and"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-12",
    "href": "slides/week-05/08-DA-Bayes.html#section-12",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\mathbf{H}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}.\\]\n\nTheorem 3 The first-order extended Kalman filter with additive noise for the nonlinear system 5 can be computed by the three-stage process:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)\nPrediction: Compute the predictive distribution mean and covariance,* \\[\\begin{aligned}\n\\mathbf{\\hat{m}}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{m}_{k-1}),\\\\\n\\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{\\mathbf{x}}(\\mathbf{m}_{k-1})\\mathbf{P}_{k-1}\\mathbf{M}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]\nCorrection: Compute the filtering distribution mean and covariance by first defining* \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathcal{H}_{k}(\\mathbf{\\hat{m}}_{k}),\\quad\\textrm{the innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{\\mathbf{x}}(\\mathbf{\\hat{m}}_{k})\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})+\\mathbf{R}_{k},\\thinspace\\textrm{the measurement covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-13",
    "href": "slides/week-05/08-DA-Bayes.html#section-13",
    "title": "Data Assimilation",
    "section": "",
    "text": "then finally updating the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]\n\nProof. The proof see Särkkä and Svensson (2023) is once again a direct application of classical results for the joint, marginal and conditional distributions of two Gaussian random variables, \\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}.\\) In addition, use is made of the Taylor series approximations to compute the Jacobian matrices \\(\\mathbf{M}_{\\mathbf{x}}\\) and \\(\\mathbf{H}_{\\mathbf{x}}\\) evaluated at \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k-1}\\) and \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k}\\) respectively."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#extended-kalman-filter-non-additive-noise",
    "href": "slides/week-05/08-DA-Bayes.html#extended-kalman-filter-non-additive-noise",
    "title": "Data Assimilation",
    "section": "Extended Kalman filter — non-additive noise",
    "text": "Extended Kalman filter — non-additive noise\nFor non-additive noise, the model is now \\[\\begin{aligned}\n\\mathbf{x}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{x}_{k-1},\\mathbf{w}_{k-1}),\\label{eq:state_nl_na}\\\\\n\\mathbf{y}_{k} & =\\mathcal{H}_{k}(\\mathbf{x}_{k},\\mathbf{v}_{k}),\\label{eq:obs_nl_na}\n\\end{aligned} \\tag{6}\\] where \\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1}),\\) and \\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k})\\) are system and measurement Gaussian noises.\nIn this case the overall three-stage scheme is the same, with necessary modifications to take into account the additional functional dependence on \\(\\mathbf{w}\\) and \\(\\mathbf{v}.\\)\n\nInitialization:\n\nDefine the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)\n\nPrediction:\n\nCompute the predictive distribution mean and covariance,"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-14",
    "href": "slides/week-05/08-DA-Bayes.html#section-14",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\mathbf{\\hat{m}}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{m}_{k-1},\\mathbf{0}),\\\\\n    \\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{\\mathbf{x}}(\\mathbf{m}_{k-1})\\mathbf{P}_{k-1}\\mathbf{M}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})\\\\\n     & +\\mathbf{M}_{\\mathbf{w}}(\\mathbf{m}_{k-1})\\mathbf{Q}_{k-1}\\mathbf{M}_{\\mathbf{w}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})+\\mathbf{Q}_{k-1}.\n    \\end{aligned}\\]\n\nCorrection:\n\nCompute the filtering distribution mean and covariance by first defining \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathcal{H}_{k}(\\mathbf{\\hat{m}}_{k},\\mathbf{0}),\\,\\textrm{the}\\,\\textrm{innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{\\mathbf{x}}(\\mathbf{\\hat{m}}_{k})\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\\\\n& +\\mathbf{H}_{\\mathbf{v}}(\\mathbf{\\hat{m}}_{k})\\mathbf{R}_{k}\\mathbf{H}_{\\mathbf{v}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k}),\\,\\textrm{the}\\,\\textrm{measurement\\,covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\mathbf{S}_{k}^{-1},\\,\\textrm{the}\\,\\textrm{Kalman\\,gain,}\n\\end{aligned}\\] then finally updating the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#ekf-pros-and-cons",
    "href": "slides/week-05/08-DA-Bayes.html#ekf-pros-and-cons",
    "title": "Data Assimilation",
    "section": "EKF — pros and cons",
    "text": "EKF — pros and cons\nPros:\n\nRelative simplicity, based on well-known linearization methods.\nMaintains the simple, elegant, and computationally efficient KF update equations.\nGood performance for such a simple method.\nAbility to treat nonlinear process and observation models.\nAbility to treat both additive and more general nonlinear Gaussian noise.\n\nCons:\n\nPerformance can suffer in presence of strong nonlinearity because of the local validity of the linear approximation (valid for small perturbations around the linear term).\nCannot deal with non-Gaussian noise, such as discrete-valued random variables."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-15",
    "href": "slides/week-05/08-DA-Bayes.html#section-15",
    "title": "Data Assimilation",
    "section": "",
    "text": "Requires differentiable process and measurement operators and evaluation of Jacobian matrices, which might be problematic in very high dimensions.\n\nIn spite of this, the EKF remains a solid filter and, as mentioned earlier, remains the basis of most GPS and navigation systems."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#ekf-example-nonlinear-oscillator",
    "href": "slides/week-05/08-DA-Bayes.html#ekf-example-nonlinear-oscillator",
    "title": "Data Assimilation",
    "section": "EKF Example — nonlinear oscillator",
    "text": "EKF Example — nonlinear oscillator\nConsider the nonlinear ODE model for the oscillations of a noisy pendulum with unit mass and length \\(L,\\) \\[\\frac{\\mathrm{d}^{2}\\theta}{\\mathrm{d}t^{2}}+\\frac{g}{L}\\sin\\theta+w(t)=0\\] where\n\n\\(\\theta\\) is the angular displacement of the pendulum,\n\\(g\\) is the gravitational constant,\n\\(L\\) is the pendulum’s length, and\n\\(w(t)\\) is a white noise process.\n\nThis is rewritten in state space form, \\[\\dot{\\mathbf{x}}+\\mathcal{M}(\\mathbf{x})+\\mathbf{w}=0,\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-16",
    "href": "slides/week-05/08-DA-Bayes.html#section-16",
    "title": "Data Assimilation",
    "section": "",
    "text": "where\n\\[\\begin{aligned}\n    \\mathbf{x} & =\\left[\\begin{array}{c}\n    x_{1}\\\\\n    x_{2}\n    \\end{array}\\right]=\\left[\\begin{array}{c}\n    \\theta\\\\\n    \\dot{\\theta}\n    \\end{array}\\right],\\quad\\mathcal{M}(\\mathbf{x})=\\left[\\begin{array}{c}\n    x_{2}\\\\\n    -\\dfrac{g}{L}\\sin x_{1}\n    \\end{array}\\right],\\\\\n    \\mathbf{w} & =\\left[\\begin{array}{c}\n    0\\\\\n    w(t)\n    \\end{array}\\right].\n    \\end{aligned}\\]\nSuppose that we have discrete, noisy measurements of the horizontal component of the position, \\(\\sin(\\theta).\\)\n\nThen the measurement equation is scalar, \\[y_{k}=\\sin\\theta_{k}+v_{k},\\] where \\(v_{k}\\) is a zero-mean Gaussian random variable with variance \\(R.\\)\n\nThe system is thus nonlinear in both state and measurement and the state-space system is of the general form 5."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-17",
    "href": "slides/week-05/08-DA-Bayes.html#section-17",
    "title": "Data Assimilation",
    "section": "",
    "text": "A simple discretization, based on the simplest Euler’s method, produces \\[\\begin{aligned}\n    \\mathbf{x}_{k} & =\\mathcal{M}(\\mathbf{x}_{k-1})+\\mathbf{w}_{k-1}\\\\\n    {y}_{k} & =\\mathcal{H}_{k}(\\mathbf{x}_{k})+{v}_{k},\n    \\end{aligned}\\] where \\[\\begin{aligned}\n    \\mathbf{x}_{k} & =\\left[\\begin{array}{c}\n    x_{1}\\\\\n    x_{2}\n    \\end{array}\\right]_{k},\\\\\n    \\mathcal{M}(\\mathbf{x}_{k-1}) & =\\left[\\begin{array}{c}\n    x_{1}+\\Delta tx_{2}\\\\\n    x_{2}-\\Delta t\\dfrac{g}{L}\\sin x_{1}\n    \\end{array}\\right]_{k-1},\\\\\n    \\mathcal{H}(\\mathbf{x}_{k}) & =[\\sin x_{1}]_{k}.\n    \\end{aligned}\\]\nThe noise terms have distributions \\[\\mathbf{w}_{k-1}\\sim\\mathcal{N}(\\mathbf{0},Q),\\quad v_{k}\\sim\\mathcal{N}(0,R),\\] where the process covariance matrix is"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-18",
    "href": "slides/week-05/08-DA-Bayes.html#section-18",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[Q=\\left[\\begin{array}{cc}\n    q_{11} & q_{12}\\\\\n    q_{21} & q_{22}\n    \\end{array}\\right],\\] with components (see remark below the example), \\[q_{11}=q_{c}\\frac{\\Delta t^{3}}{3},\\quad q_{12}=q_{21}=q_{c}\\frac{\\Delta t^{2}}{2},\\quad q_{22}=q_{c}\\Delta t,\\] and \\(q_{c}\\) is the continuous process noise spectral density.\nFor the first-order EKFhigher orders are possiblewe will need the Jacobian matrices of \\(\\mathcal{M}(\\mathbf{x})\\) and \\(\\mathcal{H}(\\mathbf{x})\\) evaluated at \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k-1}\\) and \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k}\\) . These are easily obtained here, in an explicit form, \\[\\mathbf{M}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{M}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}=\\left[\\begin{array}{cc}\n    1 & \\Delta t\\\\\n    -\\Delta t\\dfrac{g}{L}\\cos x_{1} & 1\n    \\end{array}\\right]_{k-1},\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-19",
    "href": "slides/week-05/08-DA-Bayes.html#section-19",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\mathbf{H}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}=\\left[\\begin{array}{cc}\n\\cos x_{1} & 0\\end{array}\\right]_{k}.\\]\nFor the simulations, we take:\n\n500 time steps with \\(\\Delta t=0.01.\\)\nNoise levels \\(q_{c}=0.01\\) and \\(R=0.1.\\)\nInitial angle \\(x_{1}=1.8\\) and initial angular velocity \\(x_{2}=0.\\)\nInitial diagonal state covariance of \\(0.1.\\)\n\nResults are plotted in Figure Figure 3.\n\nWe notice that despite the very noisy, nonlinear measurements, the EKF rapidly approaches the true state and then tracks it extremely well."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-20",
    "href": "slides/week-05/08-DA-Bayes.html#section-20",
    "title": "Data Assimilation",
    "section": "",
    "text": "Figure 3: Extended Kalman filter for tracking a noisy pendulum model, where horizontal position is measured. State, \\(x_k\\), $is solid blue curve; measurements, \\(y_k\\), are red circles; extended Kalman filter estimate is green curve. Results computed by EKfPendulum.m\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the above example, we have used a rather special form for the process noise covariance, \\(Q.\\) It cannot be computed exactly for nonlinear systems and some kind of approximations are needed.1 One way is to use an Euler-Maruyama method from SDEs, but this leads to singular dynamics where the particle smoothers will not work. Another approach, which was used here, is to first construct an approximate model and then compute the covariance using that model. In this case the approximate model was taken as \\[\\ddot{x}=w(t),\\] which is maybe overly simple, but works. Then the matrix \\(Q\\) is propagated through this simplified dynamics using an integration factor (exponential) solution and the corresponding power series expression of the transition matrix. Details of this can be found in [Grewal; Andrews].\n\n\n\nThanks to Simo Särkkä (private communication) for suggesting this explanation."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#unscented-kalman-filters",
    "href": "slides/week-05/08-DA-Bayes.html#unscented-kalman-filters",
    "title": "Data Assimilation",
    "section": "Unscented Kalman filters",
    "text": "Unscented Kalman filters\nThe unscented Kalman filter (UKF) was developed to overcome two shortcomings of the EKF:\n\nits difficulty to treat strong nonlinearities and\nits reliance on the computation of Jacobians.\n\nThe UKF is based on the unscented transform (UT), a method for approximating the distribution of a transformed variable, \\[\\mathbf{y}=g(\\mathbf{x}),\\] where \\(\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{m},\\mathbf{P}),\\) without linearizing the function \\(g.\\)\nThe UT is computed as follows:"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-21",
    "href": "slides/week-05/08-DA-Bayes.html#section-21",
    "title": "Data Assimilation",
    "section": "",
    "text": "Choose a collection of so-called\\(\\sigma\\)-pointsthat reproduce the mean and covariance of the distribution of \\(\\mathbf{x}\\).\nApply the nonlinearfunction to the \\(\\sigma\\)-points.\nEstimate themean and variance of the transformed random variable.\n\nThis is a deterministic sampling approach, as opposed to Monte Carlo, particle filters, and ensemble filters that all use randomly sampled points. Note that the first two usually require orders of magnitude more points than the UKF.\nSuppose that the random variable \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) with mean \\(\\mathbf{m}\\) and covariance \\(\\mathbf{P}.\\) Compute \\(N=2n+1\\) \\(\\sigma\\)-points and their corresponding weights \\[\\{\\mathbf{x}^{(\\pm i),},w^{(\\pm i)}\\},\\quad i=0,1,\\ldots,N\\] by the formulas"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-22",
    "href": "slides/week-05/08-DA-Bayes.html#section-22",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\mathbf{x}^{(0)} & =\\mathbf{m},\\\\\n\\mathbf{x}^{(\\pm i)} & =\\mathbf{m}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}^{(i)},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda},\\\\\nw^{(\\pm i)} & =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n,\n\\end{aligned}\\] where\n\n\\(\\mathbf{p}_{i}\\) is the \\(i\\)-th column of the square root of \\(\\mathbf{P},\\) which is the matrix \\(S\\) such that \\(SS^{\\mathrm{T}}=\\mathbf{P},\\) sometimes denoted as \\(\\mathbf{P}^{1/2},\\)\n\\(\\lambda\\) is a scaling parameter, defined as \\[\\lambda=\\alpha^{2}(n+\\kappa)-n,\\quad0&lt;\\alpha&lt;1,\\]\n\\(\\alpha\\) and \\(\\kappa\\) describe the spread of the \\(\\sigma\\)-points around the mean, with \\(\\kappa=3-n\\) usually,\n\\(\\beta\\) is used to include prior information on non-Gaussian distributions of \\(\\mathbf{x}.\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-23",
    "href": "slides/week-05/08-DA-Bayes.html#section-23",
    "title": "Data Assimilation",
    "section": "",
    "text": "For the covariance matrix, the weight \\(w^{(0)}\\) is modified to \\[w^{(0)}=\\frac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right).\\] These points and weights ensure that the means and covariances are consistently captured by the UT."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#ukf-algorithm",
    "href": "slides/week-05/08-DA-Bayes.html#ukf-algorithm",
    "title": "Data Assimilation",
    "section": "UKF — algorithm",
    "text": "UKF — algorithm\n\nTheorem 4 The UKF for the nonlinear system 5 computes a Gaussian approximation of the filtering distribution \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\approx\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right),\\] based on the UT, following the three-stage process:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0},\\) prior covariance \\(\\mathbf{P}_{0}\\) and the parameters \\(\\alpha,\\) \\(\\beta,\\) \\(\\kappa.\\)\nPrediction:\nCompute the \\(\\sigma\\)-points and weights, \\[\\begin{aligned}\n\\mathbf{x}_{k-1}^{(0)} & =\\mathbf{m}_{k-1},\\\\\n\\mathbf{x}_{k-1}^{(\\pm i)} & =\\mathbf{m}_{k-1}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}_{k-1}^{(i)},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda},\\quad w^{(\\pm i)}  =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-24",
    "href": "slides/week-05/08-DA-Bayes.html#section-24",
    "title": "Data Assimilation",
    "section": "",
    "text": "Propagate the \\(\\sigma\\)-points through the dynamic model \\[\\tilde{\\mathbf{x}}_{k}^{(i)}=\\mathcal{M}_{k-1}\\left(\\mathbf{x}_{k-1}^{(i)}\\right).\\]\nCompute the predictive distribution mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k}^{-} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\tilde{\\mathbf{x}}_{k}^{(i)}\\\\\n\\mathbf{P}_{k}^{-} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left(\\tilde{\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)\\left(\\tilde{\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)^{\\mathrm{T}}+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-25",
    "href": "slides/week-05/08-DA-Bayes.html#section-25",
    "title": "Data Assimilation",
    "section": "",
    "text": "Correction:\nCompute the updated \\(\\sigma\\)-points and weights, \\[\\begin{aligned}\n\\mathbf{x}_{k}^{(0)} & =\\mathbf{m}_{k}^{-},\\\\\n\\mathbf{x}_{k}^{(\\pm i)} & =\\mathbf{m}_{k}^{-}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}_{k}^{(i)-},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right),\\\\\nw^{(\\pm i)} & =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n.\n\\end{aligned}\\]\nPropagate the updated \\(\\sigma\\)-points through the measurement model \\[\\tilde{\\mathbf{y}}_{k}^{(i)}=\\mathcal{H}_{k}\\left(\\mathbf{x}_{k}^{(i)}\\right).\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-26",
    "href": "slides/week-05/08-DA-Bayes.html#section-26",
    "title": "Data Assimilation",
    "section": "",
    "text": "Compute the predicted mean and innovation, predicted measurement covariance, state-measurement cross-covariance, and filter gain, \\[\\begin{aligned}\n\\boldsymbol{\\mu}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\tilde{\\mathbf{y}}_{k}^{(i)},\\quad\\textrm{the mean,}\\\\\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\boldsymbol{\\mu}_{k},\\quad\\textrm{the innovation,}\\\\\n\\mathbf{S}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}}+\\mathbf{R}_{k},\\textrm{ measur cov}\\\\\n\\mathbf{C}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left({\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}},\\textrm{ s-m cross-cov},\\\\\n\\mathbf{K}_{k} & =\\mathbf{C}_{k}\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain.}\n\\end{aligned}\\]\nFinally, update the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-27",
    "href": "slides/week-05/08-DA-Bayes.html#section-27",
    "title": "Data Assimilation",
    "section": "",
    "text": "Just as was the case with the EKF, the UKF can also be applied to the non-additive noise model 6.\n\nThis is achieved by applying a non-additive version of the UT. Details can be found in (Särkkä and Svensson 2023)."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#particle-filters",
    "href": "slides/week-05/08-DA-Bayes.html#particle-filters",
    "title": "Data Assimilation",
    "section": "Particle filters",
    "text": "Particle filters\nWhat happens if both the models are nonlinear and the pdfs are non Gaussian?\nThe Kalman filter and its extensions are no longer optimal and, more importantly, can easily fail the estimation process. Another approach must be used.\nA promising candidate is the particle filter (PF)\nThe particle filter (Doucet, Johansen, et al. 2009) (and references therein) works sequentially in the spirit of the Kalman filter, but unlike the latter, it handles an ensemble of states (the particles) whose distribution approximates the pdf of the true state.\nBayes’ rule and the marginalization formula, \\[p(x)=\\int p(x\\mid y)p(y)\\,\\mathrm{d}y,\\] are explicitly used in the estimation process.\nThe linear and Gaussian hypotheses can then be ruled out, in theory."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-28",
    "href": "slides/week-05/08-DA-Bayes.html#section-28",
    "title": "Data Assimilation",
    "section": "",
    "text": "In practice though, the particle filter cannot yet be applied to very high dimensional systems (this is often referred to as “the curse of dimensionality”). Though recent work by [Friedemann, Raffin2023] has improved this by sophisticated parallel computing.\nParticle filters are methods for obtaining Monte Carlo approximationsof the solutions of the Bayesian filtering equations.\nRather than trying to compute the exact solution of the Bayesian filtering equations, the transformations of such filtering (Bayes’ rule for the analysis, model propagation for the forecast) are applied to the members of the sample.\n\nThe statistical moments are meant to be those of the targeted pdf.\nObviously this sampling strategy can only be exact in the asymptotic limit; that is, in the limit where the number of members (or particles) goes to infinity.\n\nThe most popular and simple algorithm of Monte Carlo type that solves the Bayesian filtering equations is called the bootstrap particle filter. It is computed by a three-stage process."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-29",
    "href": "slides/week-05/08-DA-Bayes.html#section-29",
    "title": "Data Assimilation",
    "section": "",
    "text": "Sampling\n\nWe consider a sample of particles \\(\\left\\{ \\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{M}\\right\\}\\). The related probability density function at time \\(t_{k}\\) is \\(p_{k}(\\mathbf{x}),\\) where \\[p_{k}(\\mathbf{x})\\simeq\\sum_{i=1}^{M}\\omega_{i}^{k}\\delta(\\mathbf{x}-\\mathbf{x}_{k}^{i})\\] and \\(\\delta\\) is the Dirac mass and the sum is meant to be an approximation of the exact density that the samples emulate. A positive scalar, \\(\\omega_{k}^{i},\\) weights the importance of particle \\(i\\) within the ensemble. At this stage, we assume that the weights \\(\\omega_{i}^{k}\\) are uniform and \\(\\omega_{i}^{k}=1/M\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-30",
    "href": "slides/week-05/08-DA-Bayes.html#section-30",
    "title": "Data Assimilation",
    "section": "",
    "text": "Forecast\n\nAt the forecast step, the particles are propagated by the model without approximation, \\[p_{k+1}(\\mathbf{x})\\simeq\\sum_{i=1}^{M}\\omega_{k}^{i}\\delta(\\mathbf{x}-\\mathbf{x}_{k+1}^{i}),\\] with \\(\\mathbf{x}_{k+1}^{i}=\\mathcal{M}_{k+1}(\\mathbf{x}_{k}).\\) A stochastic noise can optionally be added to the dynamics of each particle."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-31",
    "href": "slides/week-05/08-DA-Bayes.html#section-31",
    "title": "Data Assimilation",
    "section": "",
    "text": "Analysis\n\nThe analysis step of the particle filter is extremely simple and elegant. The rigorous implementation of Bayes’ rule ascribes to each particle a statistical weight that corresponds to the likelihood of the particle given the data. The weight of each particle is updated according to \\[\\omega_{k+1}^{\\mathrm{a},i}\\propto\\omega_{k+1}^{\\mathrm{f},i}p(\\mathbf{y}_{k+1}|\\mathbf{x}_{k+1}^{i})\\,.\\] It is remarkable that the analysis is carried out with only a few multiplications. It does not involve inverting any system or matrix, as opposed for instance to the Kalman filter.\n\n\nOne usually has to choose between\n\nlinear Kalman filters\nensemble Kalman filters\nnonlinear filters\nhybrid variational-filter methods."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-32",
    "href": "slides/week-05/08-DA-Bayes.html#section-32",
    "title": "Data Assimilation",
    "section": "",
    "text": "These questions are resumed in the following Table:\n\n\n\nTable 1: Decision matrix for choice of Kalman filters.\n\n\n\n\n\nEstimator\nModel type\npdf\nCPU-time\n\n\n\n\nKF\nlinear\nGaussian\nlow\n\n\nEKF\nlocally linear\nGaussian\nlow-medium\n\n\nUKF\nnonlinear\nGaussian\nmedium\n\n\nEnKF\nnonlinear\nGaussian\nmedium-high\n\n\nPF\nnonlinear\nnon-Gaussian\nhigh"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#codes",
    "href": "slides/week-05/08-DA-Bayes.html#codes",
    "title": "Data Assimilation",
    "section": "Codes",
    "text": "Codes\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen."
  },
  {
    "objectID": "slides/week-03/05-variational.html#outline",
    "href": "slides/week-03/05-variational.html#outline",
    "title": "Variational Data Assimilation",
    "section": "Outline",
    "text": "Outline\nAdjoint methods and variational data assimilation (4h)\n\nIntroduction to data assimilation: setting, history, overview, definitions.\nAdjoint method.\nVariational data assimilation methods:\n\n3D-Var,\n4D-Var.\n\n\nStatistical estimation, Kalman filters and sequential data assimilation (4h)\n\n\nIntroduction to statistical DA.\nStatistical estimation.\nThe Kalman filter.\nNonlinear extensions and ensemble filters."
  },
  {
    "objectID": "slides/week-03/05-variational.html#reference-tet-books",
    "href": "slides/week-03/05-variational.html#reference-tet-books",
    "title": "Variational Data Assimilation",
    "section": "Reference Tet Books",
    "text": "Reference Tet Books"
  },
  {
    "objectID": "slides/week-03/05-variational.html#overview",
    "href": "slides/week-03/05-variational.html#overview",
    "title": "Variational Data Assimilation",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nFrom Asch (2022)"
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-daformulation",
    "href": "slides/week-03/05-variational.html#variational-daformulation",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—formulation",
    "text": "Variational DA—formulation\nIn variational data assimilation we describe the state of the system by\n\na state variable, \\(\\mathbf{x}(t)\\in\\mathcal{X},\\) a function of space and time that\nrepresents the physical variables of interest, such as current velocity (in oceanography), temperature, sea-surface height, salinity, biological species concentration, chemical concentration, etc.\n\nEvolution of the state is described by a system of (in general nonlinear) differential equations in a region \\(\\Omega,\\) \\[\\left\\{ \\begin{aligned} & \\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t}=\\mathcal{M}(\\mathbf{x})\\quad\\mathrm{in}\\;\\Omega\\times\\left[0,T\\right],\\\\\n     & \\mathbf{x}(t=0)=\\mathbf{x}_{0},\n    \\end{aligned}\n    \\right. \\tag{1}\\] where the initial condition is unknown (or poorly known)."
  },
  {
    "objectID": "slides/week-03/05-variational.html#section",
    "href": "slides/week-03/05-variational.html#section",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "Suppose that we are in possession of observations \\(\\mathbf{y}(t)\\in\\mathcal{O}\\) and an observation operator \\(\\mathcal{H}\\) that describes the available observations.\nThen, to characterize the difference between the observations and the state, we define the objective (or cost) function, \\[J(\\mathbf{x}_{0})=\\frac{1}{2}\\int_{0}^{T}\\left\\Vert \\mathbf{y}(t)-\\mathcal{H}\\left(\\mathbf{x}(\\mathbf{x}_{0},t)\\right)\\right\\Vert _{\\mathcal{O}}^{2}\\mathrm{d}t+\\frac{1}{2}\\left\\Vert \\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}\\right\\Vert _{\\mathcal{X}}^{2} \\tag{2}\\] where\n\n\\(\\mathbf{x}^{\\mathrm{b}}\\) is the background (or first guess)\nand the second term plays the role of a regularization (in the sense of Tikhonov see previous Lecture.\nThe two norms under the integral, in the finite-dimensional case, will be represented by the error covariance matrices \\(\\mathbf{R}\\) and \\(\\mathbf{B}\\) respectively, and will be described below.\nNote that for mathematical rigor we have indicated, as subscripts, the relevant functional spaces on which the norms are defined."
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-1",
    "href": "slides/week-03/05-variational.html#section-1",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "In the continuous context, the data assimilation problem is formulated as follows:\n\n\n\n\n\n\nImportant\n\n\nFind the analyzed state \\(\\mathbf{x}^a_0\\) that minimizes \\(J\\) and satisfies\n\\[\\mathbf{x}^a_0=\\mathop{\\mathrm{arg\\,min}}_{\\mathbf{x}_0} J(\\mathbf{x}_0)\\]\n\n\n\nThe necessary condition for the existence of a (local) minimum is (as usual...) \\[\\nabla J(\\mathbf{x}_{0}^{\\mathrm{a}})=0.\\]\nVariational DA is based on anadjoint approach that is explained inthe previous Lecture.\nThe particularity here is that the adjoint is used to solve an inverse problem for the unknown initial condition."
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-da3d-var",
    "href": "slides/week-03/05-variational.html#variational-da3d-var",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—3D Var",
    "text": "Variational DA—3D Var\nUsually, 3D-Var and 4D-Var are introduced in a finite dimensional or discrete context this approach will be used in this section.\nFor the infinite dimensional or continuous case, we must use the calculus of variations and (partial) differential equations, as was done in the previous Lectures.\nWe start out with the finite-dimensional version of the cost function 2, \\[\\begin{aligned}\n    J(\\mathbf{x}) & =\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}^{\\mathrm{b}}\\right)^{\\mathrm{T}}\\mathbf{B}^{-1}\\left(\\mathbf{x}-\\mathbf{x}^{\\mathrm{b}}\\right)\\\\\n     & +\\frac{1}{2}\\left(\\mathbf{Hx}-\\mathbf{y}\\right)^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{Hx}-\\mathbf{y}\\right),\n    \\end{aligned} \\tag{3}\\] where\n\n\\(\\mathbf{x},\\) \\(\\mathbf{x}^{\\mathrm{b}},\\) and \\(\\mathbf{y}\\) are the state, the background state, and the measured state respectively;"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-2",
    "href": "slides/week-03/05-variational.html#section-2",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\(\\mathbf{H}\\) is the observation matrix (a linearization of the observation operator \\(\\mathcal{H}\\) );\n\\(\\mathbf{R}\\) and \\(\\mathbf{B}\\) are the observation and background error covariance matrices respectively.\n\nThis quadratic function attempts to strike a balance between some a priori knowledge about a background (or historical) state and the actual measured, or observed, state.\nIt also assumes that we know and that we can invertthe matrices \\(\\mathbf{R}\\) and \\(\\mathbf{B}\\). This, as we will be pointed out below, is not always obvious.\nFurthermore, it represents the sum of the (weighted) background deviations and the (weighted) observation deviations. The basic methodology is presented in the Algorithm below, which is nothing more than a classical gradient descent algorithm."
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-3",
    "href": "slides/week-03/05-variational.html#section-3",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\(j=0\\), \\(x=x_{0}\\)\nwhile \\(\\left\\Vert \\nabla J\\right\\Vert &gt;\\epsilon\\) or \\(j\\le j_{\\mathrm{max}}\\)\n    compute \\(J\\)\n    compute \\(\\nabla J\\)\n    gradient descent and update of \\(x_{j+1}\\)\n    \\(j=j+1\\)\nend\n\n\n\n\n\nWe note that when\n\nthe background \\(\\mathbf{x}^{\\mathrm{b}}=\\mathbf{x}^{\\mathrm{b}}+\\epsilon^{\\mathrm{b}}\\) is available at some time \\(t_{k},\\) together with\nobservations of the form \\(\\mathbf{y}=\\mathbf{Hx}^{\\mathrm{t}}+\\epsilon^{\\mathrm{o}}\\) that have been acquired at the same time (or over a short enough interval of time when the dynamics can be considered stationary),\nthen the minimization of 3 will produce an estimate of the system state at time \\(t_{k}.\\)"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-4",
    "href": "slides/week-03/05-variational.html#section-4",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "In this case, the analysis is called “three-dimensional variational analysis” and is abbreviated by 3D-Var.\nBorrowing from control theory see Asch (2022) the optimal gain can be shown to take the form \\[\\mathbf{K}=\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R})^{-1},\\] where \\(\\mathbf{B}\\) and \\(\\mathbf{R}\\) are the covariance matrices.\nWe obtain the analyzed state, \\[\\mathbf{x}^{\\mathrm{a}}=\\mathbf{x}^{\\mathrm{b}}+\\mathbf{K}(\\mathbf{y}-\\mathbf{H}(\\mathbf{x}^{\\mathrm{b}})).\\]\nThis is the state that minimizes the 3D-Var cost function.\nWe can verify this by taking the gradient, term by term, of the cost function Equation 3 and equating to zero, \\[\\nabla J(\\mathbf{x}^{\\mathrm{a}})=\\mathbf{B}^{-1}\\left(\\mathbf{x}^{\\mathrm{a}}-\\mathbf{x}^{\\mathrm{b}}\\right)-\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{a}}\\right)=0,\\label{eq:gradJ3DVar}\\]"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-5",
    "href": "slides/week-03/05-variational.html#section-5",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "where \\[\\mathbf{x}^{\\mathrm{a}}=\\mathop{\\mathrm{arg\\,min}}J(\\mathbf{x}).\\]\nSolving the equation, we find \\[\\begin{aligned}\n    \\mathbf{B}^{-1}\\left(\\mathbf{x}^{\\mathrm{a}}-\\mathbf{x}^{\\mathrm{b}}\\right) & = & \\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{a}}\\right)\\nonumber \\\\\n    \\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)\\mathbf{x}^{\\mathrm{a}}& = & \\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{y}+\\mathbf{B}^{-1}\\mathbf{x}^{\\mathrm{b}}\\nonumber \\\\\n    \\mathbf{x}^{\\mathrm{a}}& = & \\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\left(\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{y}+\\mathbf{B}^{-1}\\mathbf{x}^{\\mathrm{b}}\\right)\\nonumber \\\\\n     & = & \\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\left(\\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)\\mathbf{x}^{\\mathrm{b}}\\right.\\nonumber \\\\\n     & ~ & \\left.-\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\mathbf{x}^{\\mathrm{b}}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{y}\\right)\\nonumber \\\\\n     & = & \\mathbf{x}^{\\mathrm{b}}+\\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{b}}\\right)\\nonumber \\\\\n     & = & \\mathbf{x}^{\\mathrm{b}}+\\mathbf{K}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{b}}\\right),\\label{eq:lincontrol}\n    \\end{aligned} \\tag{4}\\]"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-6",
    "href": "slides/week-03/05-variational.html#section-6",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "where we have simply added and subtracted the term \\(\\left(\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)\\mathbf{x}^{\\mathrm{b}}\\) in the third-last line and in the last line we have brought out what are known as the innovation term, \\[\\mathbf{d}=\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{b}},\\] and the gain matrix, \\[\\mathbf{K}=\\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}.\\]\nThis matrix can be rewritten as \\[\\mathbf{K}=\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\left(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\right)^{-1} \\tag{5}\\]\nusing a well-known Sherman-Morrison-Woodbury formula of linear algebra that completely avoids the direct computation of the inverse of the matrix \\(\\mathbf{B}.\\)"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-7",
    "href": "slides/week-03/05-variational.html#section-7",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "The linear combination in 4 of a background term plus a multiple of the innovation is a classical result of linear-quadratic (LQ) control theory and shows how nicely DA fits in with and corresponds to (optimal) control theory\nThe form of the gain matrix 5 can be explained quite simply.\n\nThe term \\(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) is the background covariance transformed to the observation space.\nThe “denominator” term \\(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) expresses the sum of observation and background covariances.\nThe “numerator” term \\(\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) takes the ratio of \\(\\mathbf{B}\\) and \\(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) back to the model space.\n\nThis recalls (and is completely analogous to) the variance ratio"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-8",
    "href": "slides/week-03/05-variational.html#section-8",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\frac{\\sigma_{\\mathrm{b}}^{2}}{\\sigma_{b}^{2}+\\sigma_{\\mathrm{o}}^{2}}\\] that appears in the optimal BLUE (Best Linear Unbiased Estimate) that will be derived later in the statistical DA Lecture.\nThis corresponds to the case of a single observation \\(y^{\\mathrm{o}}=x^{\\mathrm{o}}\\) of a quantity \\(x,\\) \\[\\begin{aligned}\n    x^{\\mathrm{a}} & = & x^{\\mathrm{b}}+\\frac{\\sigma_{\\mathrm{b}}^{2}}{\\sigma_{\\mathrm{b}}^{2}+\\sigma_{\\mathrm{o}}^{2}}(x^{\\mathrm{o}}-x^{\\mathrm{b}})\\\\\n     & = & x^{\\mathrm{b}}+\\frac{1}{1+\\alpha}(x^{\\mathrm{o}}-x^{\\mathrm{b}}),\n    \\end{aligned}\\]\nwhere \\[\\alpha=\\frac{\\sigma_{\\mathrm{o}}^{2}}{\\sigma_{\\mathrm{b}}^{2}}.\\]"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-9",
    "href": "slides/week-03/05-variational.html#section-9",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "In other words, the best way to estimate the state is to take a weighted average of the background (or prior) and the observations of the state. And the best weight is the ratio of the mean squared errors (variances).\nThe statistical viewpoint is thus perfectly reproduced in the 3D-Var framework."
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-da-4d-var",
    "href": "slides/week-03/05-variational.html#variational-da-4d-var",
    "title": "Variational Data Assimilation",
    "section": "Variational DA — 4D Var",
    "text": "Variational DA — 4D Var\nA more realistic, but complicated situation arises when one wants to assimilate observations that are acquired over a time interval, during which the system dynamics (flow, for example) cannot be neglected.\nSuppose that the measurements are available at a succession of instants, \\(t_{k},\\) \\(k=0,1,\\ldots,K\\) and are of the form \\[\\mathbf{y}_{k}=\\mathbf{H}_{k}\\mathbf{x}_{k}+\\boldsymbol{\\epsilon}^{\\mathrm{o}}_{k}, \\tag{6}\\] where\n\n\\(\\mathbf{H}_{k}\\) is a linear observation operator and\n\\(\\boldsymbol{\\epsilon}^{\\mathrm{o}}_{k}\\) is the observation errorwith covariance matrix \\(\\mathbf{R}_{k},\\)\nand suppose that these observation errors are uncorrelated in time."
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-11",
    "href": "slides/week-03/05-variational.html#section-11",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    J(\\mathbf{x}_{0}) & =\\frac{1}{2}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}^{\\mathrm{b}}_{0}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)\\\\\n     & +\\frac{1}{2}\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right).\n    \\end{aligned} \\tag{8}\\]\nThe minimization of \\(J(\\mathbf{x}_{0})\\) will provide the initial condition of the model that fits the data most closely.\nThis analysis is called “strong constraint four-dimensional variational assimilation,” abbreviated as strong constraint 4D-Var. The term “strong constraint” implies that the model found by the state equation 7 must be exactly satisfied by the sequence of estimated state vectors.\nIn the presence of model uncertainty, the state equation becomes \\[\\mathbf{x}_{k+1}^{\\mathrm{t}}=\\mathbf{M}_{k+1}\\mathbf{x}_{k}^{\\mathrm{t}}+\\boldsymbol{\\eta}_{k+1},\\label{eq:mod_uncert}\\] where"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-12",
    "href": "slides/week-03/05-variational.html#section-12",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "the model noise \\(\\boldsymbol{\\eta}_{k}\\) has covariance matrix \\(\\mathbf{Q}_{k},\\)\nwhich we suppose to be uncorrelated in time and uncorrelated with the background and observation errors.\n\nThe objective function for the best, linear unbiased estimator (BLUE) for the sequence of states \\[\\left\\{ \\mathbf{x}_{k},\\,k=0,1,\\ldots,K\\right\\}\\] is of the form \\[\\begin{aligned}\n    J(\\mathbf{x}_{0},\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{K})=\\frac{1}{2}\\left(\\mathbf{x}_{0}-\\mathbf{x}_{0}^{\\mathrm{b}}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}_{0}^{\\mathrm{b}}\\right)\\nonumber \\\\\n    +\\frac{1}{2}\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right)\\nonumber \\\\\n    +\\frac{1}{2}\\sum_{k=0}^{K-1}\\left(\\mathbf{x}_{k+1}-\\mathbf{M}_{\\mathit{k}+1}\\mathbf{x}_{k}\\right)^{\\mathrm{T}}\\mathbf{Q}_{k+1}^{-1}\\left(\\mathbf{x}_{k+1}-\\mathbf{M}_{\\mathit{k}+1}\\mathbf{x}_{k}\\right).\n    \\end{aligned} \\tag{9}\\]"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-13",
    "href": "slides/week-03/05-variational.html#section-13",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "This objective function has become a function of the complete sequence of states \\[\\left\\{ \\mathbf{x}_{k},\\,k=0,1,\\ldots,K\\right\\} ,\\] and its minimization is known as “weak constraint four-dimensional variational assimilation,” abbreviated as weak constraint 4D-Var.\nEquations 8 and 9, with an appropriate reformulation of the state and observation spaces, are special cases of the BLUE objective function.\nAll the above forms of variational assimilation, as defined by Equations 3, 8 and 9, have been used for real-world data assimilation, in particular in meteorology and oceanography.\nHowever, these methods are directly applicable to a vast array of other domains, among which we can cite\n\ngeophysics and environmental sciences,\nseismology,"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-14",
    "href": "slides/week-03/05-variational.html#section-14",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "atmospheric chemistry, and terrestrial magnetism.\nMany other examples exist.\n\nWe remark that in real-world practice, variational assimilation is performed on nonlinear models. If the extent of the nonlinearity is sufficiently small (in some sense), then variational assimilation, even if it does not solve the correct estimation problem, will still produce useful results."
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-da4d-var-implementation",
    "href": "slides/week-03/05-variational.html#variational-da4d-var-implementation",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—4D Var — implementation",
    "text": "Variational DA—4D Var — implementation\nNow, our problem reduces to\n\nquantifying the covariance matrices and then, of course,\ncomputing the analyzed state.\n\nThe quantification of the covariance matrices must result from extensive data studies, or the use of a Kalman filter approach see below.\nThe computation of the analyzed state will be described next this will not be done directly, but rather by an adjoint approach for minimizing the cost functions.\nThere is of course the inverse of \\(\\mathbf{B}\\) or \\(\\mathbf{P}^{\\mathrm{b}}\\) to compute, but we remark that there appear onlymatrix-vector products of \\(\\mathbf{B}^{-1}\\) and \\(\\left(\\mathbf{P}^{\\mathrm{b}}\\right)^{-1}\\) and we can thus define operators (or routines) that compute these efficiently without the need for large storage capacities."
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-da4d-var-adjoint",
    "href": "slides/week-03/05-variational.html#variational-da4d-var-adjoint",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—4D Var — adjoint",
    "text": "Variational DA—4D Var — adjoint\nWe explain the adjoint approach in the case of strong constraint 4D-Var, taking into account a completely general nonlinear setting for the model and for the observation operators.\nLet \\(\\mathbf{M}_{k}\\) and \\(\\mathbf{H}_{k}\\) be the nonlinear model and observation operators respectively.\nWe reformulate 7 and 8 in terms of the nonlinear operators as \\[\\begin{aligned}\n    J(\\mathbf{x}_{0}) & = & \\frac{1}{2}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)\\nonumber \\\\\n     &  & +\\frac{1}{2}\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right),\n    \\end{aligned} \\tag{10}\\]\nwith the dynamics \\[\\mathbf{x}_{k+1}=\\mathbf{M}_{k+1}\\left(\\mathbf{x}_{k}\\right),\\quad k=0,1,\\ldots,K-1. \\tag{11}\\]"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-15",
    "href": "slides/week-03/05-variational.html#section-15",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "The minimization problem requires that we now compute the gradient of \\(J\\) with respect to \\(\\mathbf{x}_{0}.\\)\nThe gradient is determined from the property that, for a given perturbation \\(\\delta\\mathbf{x}_{0}\\) of \\(\\mathbf{x}_{0},\\) the corresponding first-order variation of \\(J\\) is \\[\\delta J=\\left(\\nabla_{\\mathbf{x}_{0}}J\\right)^{\\mathrm{T}}\\delta\\mathbf{x}_{0}. \\tag{12}\\]\nThe perturbation is propagated by the tangent linear equation, \\[\\delta\\mathbf{x}_{k+1}=\\mathbf{M}_{k+1}\\delta\\mathbf{x}_{k},\\quad k=0,1,\\ldots,K-1, \\tag{13}\\] obtained by differentiation of the state equation 11, where \\(\\mathbf{M}_{k+1}\\) is the Jacobian matrix (of first-order partial derivatives) of \\(\\mathbf{x}_{k+1}\\) with respect to \\(\\mathbf{x}_{k}.\\)\nThe first-order variation of the cost function is obtained similarly by differentiation of 10,"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-16",
    "href": "slides/week-03/05-variational.html#section-16",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\delta J & =\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}^{\\mathrm{b}}_{0}\\right)^{-1}\\delta\\mathbf{x}_{0}\\label{eq:4DvarVarObj}\\\\\n     & +\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\mathrm{H}_{k}\\delta\\mathbf{x}_{k},\n    \\end{aligned} \\tag{14}\\] where \\(\\mathrm{H}_{k}\\) is the Jacobian of \\(\\mathbf{H}_{k}\\) and \\(\\delta\\mathbf{x}_{k}\\) is defined by 13.\n\nThis variation is a compound function of \\(\\delta\\mathbf{x}_{0}\\) that depends on all the \\(\\delta\\mathbf{x}_{k}\\)’s.\nBut if we can obtain a direct dependence on \\(\\delta\\mathbf{x}_{0}\\) in the form of 12, eliminating the explicit dependence on \\(\\delta\\mathbf{x}_{k},\\) then we will (as in the previously seen examples) arrive at an explicit expression for the gradient \\(\\nabla_{\\mathbf{x}_{0}}J\\) of our cost function \\(J.\\)\nThis will be done, as we have done before, by introducing an adjoint state and requiring that it satisfy certain conditions namely, the adjoint equation. Let us now proceed with this program.\n\nWe begin by defining, for \\(k=0,1,\\ldots,K,\\) the adjoint state vectors \\(\\mathbf{p}_{k}\\) that belong to the dual of the state space."
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-17",
    "href": "slides/week-03/05-variational.html#section-17",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "Now we take the null products (according to the tangent state equation 13), \\[\\mathbf{p}_{k}^{\\mathrm{T}}\\left(\\delta\\ \\mathbf{x}_{k}-\\mathbf{M}_{k}\\delta\\mathbf{x}_{k-1}\\right),\\] and subtract them from the right-hand side of the cost function variation 14, \\[\\begin{aligned}\n    \\delta J & =\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}^{\\mathrm{b}}_{0}\\right)^{-1}\\delta\\\\\n     & \\mathbf{x}_{0}+\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\mathrm{H}_{k}\\delta\\mathbf{x}_{k}\\\\\n     & -\\sum_{k=0}^{K}\\mathbf{p}_{k}^{\\mathrm{T}}\\left(\\delta\\mathbf{x}_{k}-\\mathbf{M}_{k}\\delta\\mathbf{x}_{k-1}\\right).\n    \\end{aligned}\\]\nRearranging the matrix products, using the symmetry of \\(\\mathbf{R}_{k}\\) and regrouping terms in \\(\\delta\\mathbf{x}_{\\cdot},\\) we obtain,"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-18",
    "href": "slides/week-03/05-variational.html#section-18",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\delta J & = & \\left[\\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)+\\mathrm{H}_{0}^{\\mathrm{T}}\\mathbf{R}_{0}^{-1}\\left(\\mathbf{H}_{0}(\\mathbf{x}_{0})-\\mathbf{y}_{0}\\right)+\\mathbf{M}_{0}^{\\mathrm{T}}\\mathbf{p}_{1}\\right]\\delta\\mathbf{x}_{0}\\\\\n     &  & +\\left[\\sum_{k=1}^{K-1}\\mathrm{H}_{k}^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)-\\mathbf{p}_{k}+\\mathbf{M}_{k}^{\\mathrm{T}}\\mathbf{p}_{k+1}\\right]\\delta\\mathbf{x}_{k}\\\\\n     &  & +\\left[\\mathrm{H}_{K}^{\\mathrm{T}}\\mathbf{R}_{K}^{-1}\\left(\\mathbf{H}_{K}(\\mathbf{x}_{K})-\\mathbf{y}_{K}\\right)-\\mathbf{p}_{K}\\right]\\delta\\mathbf{x}_{k}.\n    \\end{aligned}\\]\nNotice that this expression is valid for any choice of the adjoint states \\(\\mathbf{p}_{k}\\) and, in order to “kill” all \\(\\delta\\mathbf{x}_{k}\\) terms, except \\(\\delta\\mathbf{x}_{0},\\) we must simply impose that, \\[\\begin{aligned}\n    \\mathbf{p}_{K} & = & \\mathrm{H}_{K}^{\\mathrm{T}}\\mathbf{R}_{K}^{-1}\\left(\\mathbf{H}_{K}(\\mathbf{x}_{K})-\\mathbf{y}_{K}\\right),\\label{eq:4DvarAdj1}\\\\\n    \\mathbf{p}_{k} & = & \\mathrm{H}_{k}^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)+\\mathbf{M}_{k}^{\\mathrm{T}}\\mathbf{p}_{k+1},\\quad k=K-1,\\ldots,1,\\label{eq:4DvarAdj2}\\\\\n    \\mathbf{p}_{0} & = & \\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}_{0}^{\\mathrm{b}}\\right)+\\mathrm{H}_{0}^{\\mathrm{T}}\\mathbf{R}_{0}^{-1}\\left(\\mathbf{H}_{0}(\\mathbf{x}_{0})-\\mathbf{y}_{0}\\right)+\\mathbf{M}_{0}^{\\mathrm{T}}\\mathbf{p}_{1}.\n    \\end{aligned} \\tag{15}\\]\nWe recognize the backward, adjoint equation for \\(\\mathbf{p}_{k}\\) and the only term remaining in the variation of \\(J\\) is then"
  },
  {
    "objectID": "slides/week-03/05-variational.html#section-19",
    "href": "slides/week-03/05-variational.html#section-19",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\delta J=\\mathbf{p}_{0}^{\\mathrm{T}}\\delta\\mathbf{x}_{0},\\] so that \\(\\mathbf{p}_{0}\\) is the sought for gradient, \\(\\nabla_{\\mathbf{x}_{0}}J\\), of the objective function with respect to the initial condition \\(\\mathbf{x}_{0}\\) according to 12.\nThe system of equations 15 is the adjoint of the tangent linear equation 13.\nThe term adjoint here corresponds to the transposes of the matrices \\(\\mathrm{H}_{k}^{\\mathrm{T}}\\) and \\(\\mathbf{M}_{k}^{\\mathrm{T}}\\) that, as we have seen before, are the finite-dimensional analogues of an adjoint operator."
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-da---4d-var-algorithm",
    "href": "slides/week-03/05-variational.html#variational-da---4d-var-algorithm",
    "title": "Variational Data Assimilation",
    "section": "Variational DA - 4D Var Algorithm",
    "text": "Variational DA - 4D Var Algorithm\nWe can now propose the “usual” algorithm for solving the optimization problem by the adjoint approach:\n\nFor a given initial condition \\(\\mathbf{x}_{0},\\) integrate forwards the (nonlinear) state equation 11 and store the solutions \\(\\mathbf{x}_{k}\\) (or use some sort of checkpointing).\nFrom the final condition, \\(\\mathbf{p}_K\\) (cf. 15), integrate backwards in time the adjoint equations for \\(\\mathbf{p}_k\\) (cf. 15)\nCompute directly the required gradient \\(\\mathbf{p}_0\\) (cf. 15).\nUse this gradient in an iterative optimization algorithm to find a (local) minimum.\n\nThe above description for the solution of the 4D-Var data assimilation problem clearly covers the case of 3D-Var, where we seek to minimize 3. In this case, we only need the transpose Jacobian \\(\\mathrm{H}^{\\mathrm{T}}\\) of the observation operator."
  },
  {
    "objectID": "slides/week-03/05-variational.html#variational-da---roles-of-r-and-b",
    "href": "slides/week-03/05-variational.html#variational-da---roles-of-r-and-b",
    "title": "Variational Data Assimilation",
    "section": "Variational DA - roles of R and B",
    "text": "Variational DA - roles of R and B\nThe relative magnitudes of the errors due to measurement and background provide us with important information as to how much “weight” to give to the different information sources when solving the assimilation problem.\nFor example, if background errors are larger than observation errors, then the analyzed state, solution to the DA problem, should be closer to the observations than to the background and vice-versa.\nThe background error covariance matrix, \\(\\mathbf{B},\\) plays an important role in DA. This is illustrated in the following examples."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#statistical-da-introduction",
    "href": "slides/week-04/07-DA-stat.html#statistical-da-introduction",
    "title": "Data Assimilation",
    "section": "Statistical DA: introduction",
    "text": "Statistical DA: introduction\nNow we will generalize the variational approach to deal with errors and noise in\n\nthe models,\nthe observations and\nthe initial conditions.\n\nThe variational results could of course be derived as a special case of statistical DA, in the limit where the noise disappears.\nEven the statistical results can be derived in a very general way, using SDEs and/or Bayesian analysis, and then specialized to the various Kalman-type filters that we will study here.\nPractical inverse problems and data assimilation problems involve measured data.\n\nThese data are inexact and are mixed with random noise."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section",
    "href": "slides/week-04/07-DA-stat.html#section",
    "title": "Data Assimilation",
    "section": "",
    "text": "Only statistical models can provide rigorous, effective means for dealing with this measurement error.\n\nWe want to estimate a scalar quantity, say the temperature or the ozone level, at a fixed point in space.\n\n\n\nSuppose we have:\n\na model forecast, \\(x^{\\mathrm{b}}\\) (background, or a priori value)\nand a measured value, \\(x^{\\mathrm{obs}}\\) (observation).\n\nThe simplest possible approach is to try a linear combination of the two, \\[x^{\\mathrm{a}}=x^{\\mathrm{b}}+w(x^{\\mathrm{obs}}-x^{\\mathrm{b}}),\\] where \\(x^{\\mathrm{a}}\\) denotes the analysisthat we seek and \\(0\\le w\\le1\\) is a weight factor. We subtract the (always unknown) true state \\(x^{\\mathrm{t}}\\) from both sides, \\[x^{\\mathrm{a}}-x^{\\mathrm{t}}=x^{\\mathrm{b}}-x^{\\mathrm{t}}+w(x^{\\mathrm{obs}}-x^{\\mathrm{t}}-x^{\\mathrm{b}}+x^{\\mathrm{t}})\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-1",
    "href": "slides/week-04/07-DA-stat.html#section-1",
    "title": "Data Assimilation",
    "section": "",
    "text": "and defining the three errors (analysis, background, observation) as \\[e^{\\mathrm{a}}=x^{\\mathrm{a}}-x^{\\mathrm{t}},\\quad\\mathrm{e^{b}}=x^{\\mathrm{b}}-x^{\\mathrm{t}},\\quad e^{\\mathrm{obs}}=x^{\\mathrm{obs}}-x^{\\mathrm{t}},\\] we obtain \\[e^{\\mathrm{a}}=e^{\\mathrm{b}}+w(e^{\\mathrm{obs}}-e^{\\mathrm{b}})=we^{\\mathrm{obs}}+(1-w)e^{\\mathrm{b}}.\\] If we have many realizations, we can take an ensemble average, or expectation, denoted by \\(\\left\\langle \\cdot\\right\\rangle ,\\) \\[\\left\\langle e^{\\mathrm{a}}\\right\\rangle =\\left\\langle e^{\\mathrm{b}}\\right\\rangle +w(\\left\\langle e^{\\mathrm{obs}}\\right\\rangle -\\left\\langle e^{\\mathrm{b}}\\right\\rangle ).\\] Now if these errors are centred (have zero mean, or the estimates of the true state are unbiased), then \\[\\left\\langle e^{\\mathrm{a}}\\right\\rangle =0\\] also. So we must look at the variance and demand that it be as small as possible. The variance is defined, using the above notation, as"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-2",
    "href": "slides/week-04/07-DA-stat.html#section-2",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\sigma^{2}=\\left\\langle \\left(e-\\left\\langle e\\right\\rangle \\right)^{2}\\right\\rangle .\\] Now, taking variances of the error equation, and using the zero-mean property, we obtain \\[\\sigma_{\\mathrm{a}}^{2}=\\sigma_{\\mathrm{b}}^{2}+w^{2}\\left\\langle \\left(e^{\\mathrm{obs}}-e^{\\mathrm{b}}\\right)^{2}\\right\\rangle +2w\\left\\langle e^{\\mathrm{b}}\\left(e^{\\mathrm{obs}}-e^{\\mathrm{b}}\\right)\\right\\rangle .\\] This reduces to \\[\\sigma_{\\mathrm{a}}^{2}=\\sigma_{\\mathrm{b}}^{2}+w^{2}\\left(\\sigma_{\\mathrm{o}}^{2}+\\sigma_{\\mathrm{b}}^{2}\\right)-2w\\sigma_{\\mathrm{b}}^{2}\\] if \\(e^{\\mathrm{o}}\\) and \\(e^{\\mathrm{b}}\\) are uncorrelated.\nNow, to compute a minimum, take the derivative with respect to \\(w\\) and equate to zero, to obtain \\[0=2w\\left(\\sigma_{\\mathrm{obs}}^{2}+\\sigma_{\\mathrm{b}}^{2}\\right)-2\\sigma_{\\mathrm{b}}^{2},\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-3",
    "href": "slides/week-04/07-DA-stat.html#section-3",
    "title": "Data Assimilation",
    "section": "",
    "text": "where we have ignored all cross terms (errors are assumed independent). Finally, solving this last equation, we can write the optimal weight, \\[w_{*}=\\frac{\\sigma_{\\mathrm{b}}^{2}}{\\sigma_{\\mathrm{obs}}^{2}+\\sigma_{\\mathrm{b}}^{2}}=\\frac{1}{1+\\sigma_{\\mathrm{o}}^{2}/\\sigma_{\\mathrm{b}}^{2}}\\] which depends on the ratio of the background and the observation errors. Clearly \\(0\\le w_{*}\\le1\\) and\n\nif the observation is perfect, \\(\\sigma_{\\mathrm{obs}}^{2}=0\\) and thus \\(w_{*}=1,\\) the maximum weight;\nif the background is perfect, \\(\\sigma_{\\mathrm{b}}^{2}=0\\) and \\(w_{*}=0,\\) so the observation will not be taken into account.\n\nWe can now rewrite the analysis error variance as,"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-4",
    "href": "slides/week-04/07-DA-stat.html#section-4",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\sigma_{\\mathrm{a}}^{2} & =w_{*}^{2}\\sigma_{\\mathrm{obs}}^{2}+(1-w_{*})^{2}\\sigma_{\\mathrm{b}}^{2}\\\\\n& =\\frac{\\sigma_{\\mathrm{b}}^{2}\\sigma_{\\mathrm{obs}}^{2}}{\\sigma_{\\mathrm{obs}}^{2}+\\sigma_{\\mathrm{b}}^{2}}\\\\\n& =(1-w_{*})\\sigma_{\\mathrm{b}}^{2}\\\\\n& =\\frac{1}{\\sigma_{\\mathrm{obs}}^{-2}+\\sigma_{\\mathrm{b}}^{-2}},\n\\end{aligned}\\] where we suppose that \\(\\sigma_{\\mathrm{b}}^{2},\\;\\sigma_{\\mathrm{o}}^{2}&gt;0.\\) In other words, \\[\\frac{1}{\\sigma_{\\mathrm{a}}^{2}}=\\frac{1}{\\sigma_{\\mathrm{o}}^{2}}+\\frac{1}{\\sigma_{\\mathrm{b}}^{2}}.\\] This is a very fundamental result, implying that the overall precision, \\(\\tau=1/\\sigma^{2},\\) (reciprocal of the variance) is the sum of the background and measurement precisions. Finally, the analysis equation becomes\n\\[x^{\\mathrm{a}}=x^{\\mathrm{b}}+\\frac{1}{1+\\alpha}(x^{\\mathrm{obs}}-x^{\\mathrm{b}}),\\] where \\(\\alpha=\\sigma_{\\mathrm{obs}}^{2}/\\sigma_{\\mathrm{b}}^{2}.\\) This is called the BLUE- Best Linear Unbiased Estimator - because it gives an unbiased, optimal weighting for a linear combination of two independent measurements."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#statistical-da-3-special-cases-and-conclusions",
    "href": "slides/week-04/07-DA-stat.html#statistical-da-3-special-cases-and-conclusions",
    "title": "Data Assimilation",
    "section": "Statistical DA: 3 special cases and conclusions",
    "text": "Statistical DA: 3 special cases and conclusions\nWe can isolate three special cases:\n\nif the observation is very accurate, \\(\\sigma_{\\mathrm{obs}}^{2}\\ll\\sigma_{\\mathrm{b}}^{2},\\) \\(\\alpha\\ll1\\) and thus \\(x^{\\mathrm{a}}\\approx x^{\\mathrm{obs}}\\)\nif the background is accurate, \\(\\alpha\\gg1\\) and \\(x^{\\mathrm{a}}\\approx x^{\\mathrm{b}}\\)\nand finally, if observation and background varaiances are approximately equal, \\(\\alpha\\approx1\\) and \\(x^{\\mathrm{a}}\\) is the arithmetic average of \\(x^{\\mathrm{b}}\\) and \\(x^{\\mathrm{obs}}.\\)\n\nConclusion: this simple, linear model does indeed capture the full range of possible solutions in a statistically rigorous manner, thus providing us with an “enriched” solution when compared with a non-probabilistic, scalar response such as the arithmetic average of observation and background, which would correspond to only the last of the above three special cases."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-5",
    "href": "slides/week-04/07-DA-stat.html#section-5",
    "title": "Data Assimilation",
    "section": "",
    "text": "KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters---background-and-history",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters---background-and-history",
    "title": "Data Assimilation",
    "section": "Kalman Filters - background and history",
    "text": "Kalman Filters - background and history\nDA is concerned with dynamic systems, where (noisy) observations are acquired over time.\nQuestion: Is there some statistically optimal way to combine the dynamic model and the observations?\n\nOne answer is provided by Kalman filters\nThey are linear models for state estimation of noisy dynamic systems.\nThey have been the de facto standard in many robotics and tracking/prediction applications because they are well-suited for systems where there is uncertainty about an observable dynamic process.\nThey are also the basis of many data assimilation systems.\nThey use a paradigm of “observe, predict, correct” to extract information from a noisy signal.\n\nThe Kalman filter was invented1 in 1960 by R. E. Kálmán to solve this sort of problem in a mathematically optimal way.\n1 Apparently, following a prior invention by Stratonovich, one year earlier."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-6",
    "href": "slides/week-04/07-DA-stat.html#section-6",
    "title": "Data Assimilation",
    "section": "",
    "text": "Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains.\n\nThere are Kalman filters in aircraft and autonomous vehicles, on submarines, and, in cruise missiles.\nWall Street uses them to track the market.\nThey are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments.\nChemical plants use them to control and monitor reactions.\nThey are used to perform medical imaging and to remove noise from cardiac signals.\nWeather forecasting is based on Kalman filters.\nThey can effectively be used for modeling in epidemiology.\n\nIn summary, if it involves a sensor and/or time-series data, a Kalman filter or a close relative of the Kalman filter is usually involved."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters-formulation",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters-formulation",
    "title": "Data Assimilation",
    "section": "Kalman Filters — formulation",
    "text": "Kalman Filters — formulation\nConsider a dynamical system that evolves in time and we would like toestimatea series of true states, \\(\\mathbf{x}^{\\mathrm{t}}_{k}\\) (a sequence of random vectors) where discrete time is indexed by the letter \\(k.\\)\nThese times are those when the observations or measurements are taken, as shown in the Figure.\n\n\nFigure 1: Sequential assimilation: a computed model trajectory, observations, and their error bars."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-7",
    "href": "slides/week-04/07-DA-stat.html#section-7",
    "title": "Data Assimilation",
    "section": "",
    "text": "The assimilation starts with an unconstrained model trajectory from \\(t_{0},t_{1},\\ldots,t_{k-1},t_{k},\\ldots,t_{n}\\) and aims to provide anoptimal fitto the available observations/measurements given their uncertainties (error bars).\n\nFor example, in current, synoptic scale weather forecasts, \\(t_{k}-t_{k-1}=6\\) hours and is less for the convective scale.\nIn robotics, or autonomous vehicles, the time intervals are of the order of the instrumental frequency, which can be a few milliseconds."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters---stochastic-model",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters---stochastic-model",
    "title": "Data Assimilation",
    "section": "Kalman Filters - stochastic model",
    "text": "Kalman Filters - stochastic model\nWe seek to estimate the state \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) of a discrete-time dynamic process that is governed by the linear stochastic difference equation \\[\\mathbf{x}_{k+1}=\\mathbf{M}_{k+1}\\mathbf{x}_{k}+\\mathbf{w}_{k} \\tag{1}\\]\nwith a measurement/observation \\(\\mathbf{y}\\in\\mathbb{R}^{m},\\) \\[\\mathbf{y}_{k}=\\mathbf{H}_{k}\\mathbf{x}_{k}+\\mathbf{v}_{k}. \\tag{2}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\mathbf{M}_{k+1}\\) and \\(\\mathbf{H}_{k}\\) are considered linear, here.\nThe random vectors, \\(\\mathbf{w}_{k}\\) and \\(\\mathbf{v}_{k},\\) represent the process/modeling and measurement/observation errors respectively.\nThey are assumed to be independent, white noise processes with Gaussian/normal probability distributions, \\[\\begin{aligned}\n    \\mathbf{w}_{k} & \\sim & \\mathcal{N}(0,\\mathbf{Q}_{k}),\\\\\n    \\mathbf{v}_{k} & \\sim & \\mathcal{N}(0,\\mathbf{R}_{k}),\n    \\end{aligned}\\] where \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) are the covariance matrices (supposed known) of the modeling and observation errors respectively."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-8",
    "href": "slides/week-04/07-DA-stat.html#section-8",
    "title": "Data Assimilation",
    "section": "",
    "text": "All these assumptions about unbiased and uncorrelated errors (in time and between each other) are not limiting, since extensions of the standard Kalman filter can be developed should any of these not be valid see next lecture.\nWe note that, for a broader mathematical view on the above system, we could formulate all of statistical DA in terms of stochastic differential equations (SDEs).\n\nThen the theory of Itô provides a detailed solution of the problem of optimal filtering as well as rigorous existence and uniqueness results… see (Law, Stuart, and Zygalakis 2015; Särkkä and Svensson 2023)."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters-sequential-assimilation-scheme",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters-sequential-assimilation-scheme",
    "title": "Data Assimilation",
    "section": "Kalman Filters — sequential assimilation scheme",
    "text": "Kalman Filters — sequential assimilation scheme\nThe typical assimilation scheme is made up of two major steps:\n\na prediction/forecast step, and\na correction/analysis step.\n\n\n\nFigure 2: Sequential assimilation scheme for the Kalman filter. The x-axis denotes time, the y-axis denotes the values of the state and observations vectors."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-9",
    "href": "slides/week-04/07-DA-stat.html#section-9",
    "title": "Data Assimilation",
    "section": "",
    "text": "At time \\(t_{k}\\) we have the result of a previous forecast, \\(\\mathbf{x}^{\\mathrm{f}}_{k},\\) (the analogue of the background state \\(\\mathbf{x}^{\\mathrm{b}}_{k}\\)) and the result of an ensemble of observations in \\(\\mathbf{y}_{k}.\\)\nBased on these two vectors, we perform an analysis that produces \\(\\mathbf{x}^{\\mathrm{a}}_{k}.\\)\nWe then use the evolution model to obtain a prediction of the state at time \\(t_{k+1}.\\)\nThe result of the forecast is denoted \\(\\mathbf{x}^{\\mathrm{f}}_{k+1},\\) and becomes the background, or initial guess, for the next time-step see Figure 2).\nThe Kalman filter problem can be resumed as follows:\n\ngiven a prior/background estimate \\(\\mathbf{x}^{\\mathrm{f}}\\) of the system state at time \\(t_{k},\\)\nwhat is the best update/analysis \\(\\mathbf{x}^{\\mathrm{a}}_{k}\\) based on the currently available measurements \\(\\mathbf{y}_{k}?\\)"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters-the-filter",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters-the-filter",
    "title": "Data Assimilation",
    "section": "Kalman Filters — the filter",
    "text": "Kalman Filters — the filter\nThe goal of the Kalman filter is:\n\nto compute an optimal a posteriori estimate \\(\\mathbf{x}_{k}^{\\mathrm{a}}\\)\nthat is a linear combination of an a priori estimate \\(\\mathbf{x}_{k}^{\\mathrm{f}}\\) and a weighted difference between the actual measurement \\(\\mathbf{y}_{k}\\) and the measurement prediction \\(\\mathbf{H}_{k}\\mathbf{x}^{\\mathrm{f}}_{k}.\\)\n\nThis is none other than the BLUE that we have seen above.\nThe filter is thus of the linear, recursive form \\[\\mathbf{x}_{k}^{\\mathrm{a}}=\\mathbf{x}_{k}^{\\mathrm{f}}+\\mathbf{K}_{k}\\left(\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\right). \\tag{3}\\]\nThe difference \\(\\mathbf{d}_{k}=\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\) is called the innovation and reflects the discrepancy between the actual and the predicted measurements at time \\(t_{k}.\\)"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-10",
    "href": "slides/week-04/07-DA-stat.html#section-10",
    "title": "Data Assimilation",
    "section": "",
    "text": "Note\n\n\nNote that, for generality, the matrices are shown with a time-dependence. When this is not the case, the subscripts \\(k\\) can be dropped.\n\n\n\nThe Kalman gain matrix, \\(\\mathbf{K},\\) minimizes the a posteriori error covariance Equation 4.\n\nWe define forecast (a priori) and analysis (a posteriori) estimate errors as \\[\\begin{aligned}\n    \\mathbf{e}_{k}^{\\mathrm{f}} & = & \\mathbf{x}_{k}^{\\mathrm{f}}-\\mathbf{x}_{k}^{\\mathrm{t}},\\\\\n    \\mathbf{e}_{k}^{\\mathrm{a}} & = & \\mathbf{x}_{k}^{\\mathrm{a}}-\\mathbf{x}_{k}^{\\mathrm{t}},\n    \\end{aligned}\\] where \\(\\mathbf{x}_{k}^{\\mathrm{t}}\\) is the (unknown) true state. Respective error covariance matrices are \\[\\begin{aligned}\n    \\mathbf{P}_{k}^{\\mathrm{f}} & = & \\mathop{\\mathrm{Cov}}(\\mathbf{e}_{k}^{\\mathrm{f}})=\\mathrm{E}\\left[\\mathbf{e}_{k}^{\\mathrm{f}}(\\mathbf{e}_{k}^{\\mathrm{f}})^{\\mathrm{T}}\\right],\\nonumber \\\\\n    \\mathbf{P}_{k}^{\\mathrm{a}} & = & \\mathop{\\mathrm{Cov}}(\\mathbf{e}_{k}^{\\mathrm{a}})=\\mathrm{E}\\left[\\mathbf{e}_{k}^{\\mathrm{a}}(\\mathbf{e}_{k}^{\\mathrm{a}})^{\\mathrm{T}}\\right].\n    \\end{aligned} \\tag{4}\\]\n\nOptimal gain requires a careful derivation, that is beyond our scope here (see (Asch 2022; Asch, Bocquet, and Nodet 2016))."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters-optimal-gain",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters-optimal-gain",
    "title": "Data Assimilation",
    "section": "Kalman Filters — optimal gain",
    "text": "Kalman Filters — optimal gain\nThe Kalman gainmatrix, \\(\\mathbf{K},\\) is chosen to minimize the a posteriori error covariance Equation 4.\nThe resulting \\(\\mathbf{K}\\) that minimizes Equation 4 is given by \\[\\mathbf{K}_{k}=\\mathbf{P}_{k}^{\\mathrm{f}}\\mathbf{H}_{k}^{\\mathrm{T}}\\left(\\mathbf{H}_{k}\\mathbf{P}_{k}^{\\mathrm{f}}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k}\\right)^{-1} \\tag{5}\\] where we remark that \\(\\mathbf{H}\\mathbf{P}_{k}^{\\mathrm{f}}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k}=\\mathrm{E}\\left[\\mathbf{d}_{k}\\mathbf{d}_{k}^{\\mathrm{T}}\\right]\\) is the covariance of the innovation.\nLooking at this expression for \\(\\mathbf{K}_{k},\\) we see:\n\nwhen the measurement error covariance\\(\\mathbf{R}_{k}\\) approaches zero, the gain \\(\\mathbf{K}_{k}\\) weights the innovation more heavily, since \\[\\lim_{\\mathbf{R}\\rightarrow0}\\mathbf{K}_{k}=\\mathbf{H}_{k}^{-1}.\\]\nOn the other hand, as the a priori error estimate covariance \\(\\mathbf{P}_{k}^{\\mathrm{f}}\\) approaches zero,"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-11",
    "href": "slides/week-04/07-DA-stat.html#section-11",
    "title": "Data Assimilation",
    "section": "",
    "text": "the gain \\(\\mathbf{K}_{k}\\) weights the innovation less heavily, and \\[\\lim_{\\mathbf{P}_{k}^{\\mathrm{f}}\\rightarrow0}\\mathbf{K}_{k}=0.\\]\nAnother way of thinking about the weighting of \\(\\mathbf{K}\\) is that as the measurement error covariance \\(\\mathbf{R}\\) approaches zero, the actual measurement \\(\\mathbf{y}_{k}\\) is “trusted” more and more, while the predicted measurement \\(\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\) is trusted less and less.\nOn the other hand, as the a priori error estimate covariance \\(\\mathbf{P}_{k}^{\\mathrm{f}}\\) approaches zero, the actual measurement \\(\\mathbf{y}_{k}\\) is trusted less and less, while the predicted measurement \\(\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\) is “trusted” more and more this will be illustrated in the computational example below."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filters-2-step-procedure",
    "href": "slides/week-04/07-DA-stat.html#kalman-filters-2-step-procedure",
    "title": "Data Assimilation",
    "section": "Kalman Filters — 2-step procedure",
    "text": "Kalman Filters — 2-step procedure\n\n\nFigure 3: Kalman filter loop, showing the two phases, predict and correct, preceded by an initialization step.\nThe predictor-corrector loop is illustrated in Figure 3 and can be transposed, as is, into an operational algorithm."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kf-predictorforecast-step",
    "href": "slides/week-04/07-DA-stat.html#kf-predictorforecast-step",
    "title": "Data Assimilation",
    "section": "KF — predictor/forecast step",
    "text": "KF — predictor/forecast step\n\n\nStart from a previous analyzed state1, \\(\\mathbf{x}_{k}^{\\mathrm{a}},\\) or from the initial state if \\(k=0,\\) characterized by the Gaussian pdf \\(p(\\mathbf{x}_{k}^{\\mathrm{a}}\\mid\\mathbf{y}_{1:k}^{\\mathrm{o}})\\) of mean \\(\\mathbf{x}_{k}^{\\mathrm{a}}\\) and covariance matrix \\(\\mathbf{P}_{k}^{a}.\\)\nAn estimate of \\(\\mathbf{x}_{k+1}^{\\mathrm{t}}\\) is given by the dynamical model which defines the forecast as \\[\\begin{gather}\n    \\mathbf{x}_{k+1}^{\\mathrm{f}} & = & \\mathbf{M}_{k+1}\\mathbf{x}_{k}^{\\mathrm{a}},\\label{eq:fstate}\\\\\n    \\mathbf{P}_{k+1}^{\\mathrm{f}} & = & \\mathbf{M}_{k+1}\\mathbf{P}_{k}^{\\mathrm{a}}\\mathbf{M}_{k+1}^{\\mathrm{T}}+\\mathbf{Q}_{k+1},\n    \\end{gather} \\tag{6}\\] where the expression for \\(\\mathbf{P}^{\\mathrm{f}}_{k+1}\\) is obtained from the dynamics equation and the definition of the model noise covariance, \\(\\mathbf{Q}.\\)\n\n\n\n\nWe use here the classical notation \\(\\mathbf{y}_{i:j}=(\\mathbf{y}_{i},\\mathbf{y}_{i+1},\\ldots,\\mathbf{y}_{j})\\) for \\(i\\le j\\) that denotes conditioning on all the observations in the interval."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kf---correctoranalysis-step",
    "href": "slides/week-04/07-DA-stat.html#kf---correctoranalysis-step",
    "title": "Data Assimilation",
    "section": "KF - corrector/analysis step",
    "text": "KF - corrector/analysis step\n\n\nAt time \\(t_{k+1},\\) the pdf \\(p(\\mathbf{x}_{k+1}^{\\mathrm{f}}\\mid\\mathbf{y}_{1:k}^{\\mathrm{o}})\\) is known, thanks to the mean \\(\\mathbf{x}_{k+1}^{\\mathrm{f}}\\) and covariance matrix \\(\\mathbf{P}_{k+1}^{\\mathrm{f}}\\) just calculated, as well as the assumption of a Gaussian distribution.\nThe analysis step then consists of correcting this pdf using the observation available at time \\(t_{k+1}\\) in order to compute \\(p(\\mathbf{x}_{k+1}^{\\mathrm{a}}\\mid\\mathbf{y}_{1:k+1}^{\\mathrm{o}}).\\) This comes from the BLUE in the dynamical context and gives\n\n\n\n\n\\[\\begin{gather}\n    \\mathbf{K}_{k+1} & = & \\mathbf{P}_{k+1}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}\\left(\\mathbf{H}\\mathbf{P}_{k+1}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R}_{k+1}\\right)^{-1},\\label{eq:aK}\\\\\n    \\mathbf{x}_{k+1}^{\\mathrm{a}} & = & \\mathbf{x}_{k+1}^{\\mathrm{f}}+\\mathbf{K}_{k+1}\\left(\\mathbf{y}_{k+1}-\\mathbf{H}\\mathbf{x}_{k+1}^{\\mathrm{f}}\\right),\\label{eq:astate}\\\\\n    \\mathbf{P}_{k+1}^{\\mathrm{a}} & = & \\left(\\mathbf{I}-\\mathbf{K}_{k+1}\\mathbf{H}\\right)\\mathbf{P}_{k+1}^{\\mathrm{f}}.\\label{eq:acov}\n    \\end{gather} \\tag{7}\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#overall-picture",
    "href": "slides/week-04/07-DA-stat.html#overall-picture",
    "title": "Data Assimilation",
    "section": "Overall Picture",
    "text": "Overall Picture\n\n\n\nPrinciple: as we move forward in time, the uncertainty of the analysis is reduced, and the forecast is improved."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kf-relation-between-bayes-and-blue",
    "href": "slides/week-04/07-DA-stat.html#kf-relation-between-bayes-and-blue",
    "title": "Data Assimilation",
    "section": "KF — Relation Between Bayes and BLUE",
    "text": "KF — Relation Between Bayes and BLUE\nIf we know that the a priori and the observation data are both Gaussian, Bayes’ rule can be readily applied to compute the a posteriori pdf.\n\nThe a posteriori pdf is then Gaussian, and its parameters are given by the BLUE equations.\n\nHence with Gaussian pdfs and a linear observation operator, there is no need to use Bayes’ rule.\n\nThe BLUE equations can be used instead to compute the parameters of the resulting pdf.\nSince the BLUE provides the same result as Bayes’ rule, it is the best estimator of all.\n\nIn addition one can recognize the 3D-Var cost function.\n\nBy optimizing this cost function, 3D-Var finds the MAP (maximum a posteriori) estimate of the Gaussian pdf, which is equivalent to the MV (minimum variance) estimate found by the BLUE."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-12",
    "href": "slides/week-04/07-DA-stat.html#section-12",
    "title": "Data Assimilation",
    "section": "",
    "text": "ENSEMBLE KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#ensemble-kalman-filter-enkf",
    "href": "slides/week-04/07-DA-stat.html#ensemble-kalman-filter-enkf",
    "title": "Data Assimilation",
    "section": "Ensemble Kalman Filter — EnKF",
    "text": "Ensemble Kalman Filter — EnKF\nThe ensemble Kalman filter (EnKF) is an elegant approach that avoids\n\nthe steps of linearization in the classical Kalman Filter,\nand the need for adjoints in the variational approach.\n\nIt is still based on a Kalman filter, but an ensemble of realizations is used to compute an estimate of the population mean and variance, thus avoiding the need to compute inverses of potentially large matrices to obtain the posterior covariance, as was the case above in equations for \\(\\mathbf{K}_{k+1}\\) and \\(\\mathbf{P}_{k+1}^a\\) (Equation 7).\nThe EnKF and its variants have been successfully developed and implemented in meteorology and oceanography, including in operational weather forecasting systems. Because the method is simple to implement, it has been widely used in these fields."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-13",
    "href": "slides/week-04/07-DA-stat.html#section-13",
    "title": "Data Assimilation",
    "section": "",
    "text": "But it has spread out to other geoscience disciplines and beyond. For instance, to name a few domains, it has been applied in greenhouse gas inverse modeling, air quality forecasting, extra-terrestrial atmosphere forecasting , detection and attribution in climate sciences, geomagnetism re-analysis , and ice-sheet parameter estimation and forecasting. It has also been used in petroleum reservoir estimation, in adaptive optics for extra large telescopes, and highway traffic estimation.\nMore recently, the idea was proposed to exploit the EnKF as a universal approach for all inverse problems. The term EKI, Ensemble Kalman Inversion, is used to describe this approach."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#principle-of-the-enkf",
    "href": "slides/week-04/07-DA-stat.html#principle-of-the-enkf",
    "title": "Data Assimilation",
    "section": "Principle of the EnKF",
    "text": "Principle of the EnKF\nThe EnKF was originally proposed by G. Evensen in 1994 and amended in Evensen et al. (2009).\n\nDefinition 1 The ensemble Kalman filter (EnKF) is a Kalman filter that uses an ensemble of realizations to compute estimates of the population mean and covariance.\n\nSince it is based on Gaussian statistics (mean and covariance) it does not solve the Bayesian filtering problem in the limit of a large number of particles, as opposed to the more general particle filterseeAdvanced Course. Nonetheless, it turns out to be an excellent approximate algorithm for the filtering problem.\nAs in the particle filter, the EnKF is based on the concept of particles, a collection of state vectors, which are called the members of the ensemble.\n\nRather than propagating huge covariance matrices, the errors are emulated by scattered particles, a collection of state vectors whose variability is meant to be representative of the uncertainty of the system’s state resulting from the forecaster’s ignorance."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-14",
    "href": "slides/week-04/07-DA-stat.html#section-14",
    "title": "Data Assimilation",
    "section": "",
    "text": "Just like the particle filter, the members are propagated by the nonlinear model, without any linearization. Not only does this avoid the derivation of the tangent linear model, but it also circumvents the approximate linearization.\nFinally, as opposed to the particle filter, the EnKF does not irremediably suffer from the curse of dimensionality.\n\nTo sum up, here are the important remarks:\n\nthe EnKF avoids the linearization step of the KF;\nthe EnKF avoids the inversion of potentially large matrices;\nthe EnKF does not require any adjoint, as in variational assimilation;\nthe EnKF has been applied to a vast number of real-world problems."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#enkf-the-three-steps",
    "href": "slides/week-04/07-DA-stat.html#enkf-the-three-steps",
    "title": "Data Assimilation",
    "section": "EnKF — the Three Steps",
    "text": "EnKF — the Three Steps\n\nInitialization: generate an ensemble of \\(m\\) random states \\(\\left\\{ \\mathbf{x}_{i,0}^{\\mathrm{f}}\\right\\} _{i=1,\\ldots,m}\\) at time \\(t=0.\\)\nForecast: compute the prediction for each member of the ensemble.\nAnalysis: correct the prediction in light of the observations.\n\nPlease see the Algorithm (alg-EnKF?) below for details of each step.\n\n\n\n\n\n\nNote\n\n\n\nPropagation can equivalently be performed either at the end of the analysis step or at the beginning of the forecast step.\nThe Kalman gain is not computed directly, but estimated from the ensemble statistics.\nWith the important exception of the Kalman gain computation, all operations on the ensemble members are independent. As a result, parallelization is straightforward.\nThis is one of the main reasons for the success/popularity of the EnKF."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#enkf-analysis-step",
    "href": "slides/week-04/07-DA-stat.html#enkf-analysis-step",
    "title": "Data Assimilation",
    "section": "EnKF — Analysis Step",
    "text": "EnKF — Analysis Step\nThe EnKF seeks to mimic the analysis step of the Kalman filter but with an ensemble of limited size in place of the unwieldy covariance matrices.\nThe goal is to perform for each member of the ensemble an analysis of the form, \\[\\mathbf{x}_{i}^{{\\rm a}}=\\mathbf{x}_{i}^{{\\rm f}}+\\mathbf{K}\\left[\\mathbf{y}_{i}-\\mathcal{H}(\\mathbf{x}^{\\mathrm{f}}_{i})\\right], \\tag{8}\\] where\n\n\\(i=1,\\ldots,m\\) is the member index in the ensemble,\n\\(\\mathbf{x}^{\\mathrm{f}}_{i}\\) is the forecast state vector \\(i\\), which represents a background state or prior at the analysis time.\n\nTo mimic the Kalman filter, \\(\\mathbf{K}\\) must be identified with the Kalman gain \\[\\mathbf{K}=\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}{\\mathbf{H}\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R}}^{-1},\\label{eq:kalman-gain}\\] that we wish to estimate from the ensemble statistics."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-15",
    "href": "slides/week-04/07-DA-stat.html#section-15",
    "title": "Data Assimilation",
    "section": "",
    "text": "First, we compute the forecast error covariance matrix as a sum over the ensemble, \\[\\mathbf{P}^{\\mathrm{f}}=\\frac{1}{m-1}\\sum_{i=1}^{m}\\left({\\mathbf{x}_{i}^{{\\rm f}}-\\overline{\\mathbf{x}}^{{\\rm f}}}\\right)\\left({\\mathbf{x}_{i}^{{\\rm f}}-\\overline{\\mathbf{x}}^{{\\rm f}}}\\right)^{\\mathrm{T}},\\] with \\(\\overline{\\mathbf{x}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}_{i}^{{\\rm f}}.\\)\nThe forecast error covariance matrix can be factorized into \\[\\mathbf{P}^{\\mathrm{f}}=\\mathbf{X}_{\\mathrm{f}}\\mathbf{X}_{\\mathrm{f}}^{\\mathrm{T}},\\] where \\(\\mathbf{X}_{\\mathrm{f}}\\) is a \\(n\\times m\\) matrix whose columns are the normalized anomalies or normalized perturbations , i.e. for \\(i=1,\\ldots,m\\) \\[\\left[\\mathbf{X}_{\\mathrm{f}}\\right]_{i}=\\frac{\\mathbf{x}_{i}^{{\\rm f}}-\\overline{\\mathbf{x}}^{{\\rm f}}}{\\sqrt{m-1}}.\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-16",
    "href": "slides/week-04/07-DA-stat.html#section-16",
    "title": "Data Assimilation",
    "section": "",
    "text": "We can now obtain from Equation 8 a posterior ensemble \\(\\left\\{ \\mathbf{x}_{i}^{{\\rm a}}\\right\\} _{i=1,\\ldots,m}\\) from which we can compute the posterior statistics.\nHence, the posterior state and an ensemble of posterior perturbations can be estimated from \\[\\overline{\\mathbf{x}}^{{\\rm a}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}^{\\mathrm{a}}_{i}\\,,\\quad\\left[\\mathbf{X}_{\\mathrm{a}}\\right]_{i}=\\frac{\\mathbf{x}_{i}^{{\\rm a}}-\\overline{\\mathbf{x}}^{{\\rm a}}}{\\sqrt{m-1}}.\\]\nSince \\(\\mathbf{y}_{i}\\equiv\\mathbf{y}\\) was assumed, the normalized anomalies, \\(\\mathbf{X}_{i}^{{\\rm a}}\\equiv\\left[\\mathbf{X}_{\\mathrm{a}}\\right]_{i}\\), i.e. the normalized deviations of the ensemble members from the mean are obtained from Equation 8 minus the mean update, \\[\\mathbf{X}_{i}^{{\\rm a}}=\\mathbf{X}_{i}^{{\\rm f}}+\\mathbf{K}\\left(\\mathbf{0}-\\mathbf{H}\\mathbf{X}_{i}^{{\\rm f}}\\right)=\\left(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H}\\right)\\mathbf{X}_{i}^{{\\rm f}},\\label{eq:anomaly-update}\\]\nwhere \\(\\mathbf{X}_{i}^{{\\rm f}}\\equiv\\left[\\mathbf{X}_{\\mathrm{f}}\\right]_{i}\\), which yields the analysis error covariance matrix,"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-17",
    "href": "slides/week-04/07-DA-stat.html#section-17",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n        \\mathbf{P}^{{\\rm a}} & =\\mathbf{X}_{\\mathrm{a}}\\mathbf{X}_{\\mathrm{a}}^{\\mathrm{T}}\\\\\n         & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{X}_{\\mathrm{f}}\\mathbf{X}_{\\mathrm{f}}^{\\mathrm{T}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}\\\\\n         & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}.\n        \\end{aligned}\\]\nNote that such a computation is never carried out in practice. However, theoretically, in order to mimic the best linear unbiased estimator (BLUE) analysis of the Kalman filter, we should have obtained \\[\\begin{aligned}\n    \\mathbf{P}^{{\\rm a}} & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}+\\mathbf{K}\\mathbf{R}\\mathbf{K}^{\\mathrm{T}}\\\\\n     & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}.\n    \\end{aligned}\\]\n\nTherefore, the error covariances are underestimated since the second positive term, related to the observation errors, is ignored, which is likely to lead to the divergence of the EnKF when the scheme is cycled.\n\nAn elegant solution around this problem is to perturb the observation vector for each member: \\(\\mathbf{y}_{i}=\\mathbf{y}+\\mathbf{u}_{i}\\), where \\(\\mathbf{u}_{i}\\) is drawn from the Gaussian distribution \\(\\mathbf{u}_{i}\\sim N(\\mathbf{0},\\mathbf{R}).\\)"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-18",
    "href": "slides/week-04/07-DA-stat.html#section-18",
    "title": "Data Assimilation",
    "section": "",
    "text": "Let us define \\(\\overline{\\mathbf{u}}\\) the mean of the sampled \\(\\mathbf{u}_{i}\\), and the innovation perturbations \\[\\left[\\mathbf{Y}_{\\mathrm{f}}\\right]_{i}=\\frac{\\mathbf{H}\\mathbf{x}_{i}^{{\\rm f}}-\\mathbf{u}_{i}-\\mathbf{H}\\overline{\\mathbf{x}}^{{\\rm f}}+\\overline{\\mathbf{u}}}{\\sqrt{m-1}}.\\label{eq:innovation-pert}\\]\nThe posterior anomalies are modified accordingly, \\[\\mathbf{X}_{i}^{{\\rm a}}=\\mathbf{X}_{i}^{{\\rm f}}-\\mathbf{K}\\mathbf{Y}_{i}^{{\\rm f}}=(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{X}_{i}^{{\\rm f}}+\\frac{\\mathbf{K}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})}{\\sqrt{m-1}}.\\label{eq:anomaly-update-correction}\\]\n\nThese anomalies yield the analysis error covariance matrix,"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-19",
    "href": "slides/week-04/07-DA-stat.html#section-19",
    "title": "Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\mathbf{P}^{{\\rm a}}= & (\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}\n      +\\mathbf{K}\\left[\\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})^{\\mathrm{T}}\\right]\\mathbf{K}^{\\mathrm{T}}\\\\\n     & +\\frac{1}{\\sqrt{m-1}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})^{\\mathrm{T}}\\mathbf{K}^{\\mathrm{T}}\\\\\n     & +\\frac{1}{\\sqrt{m-1}}\\mathbf{K}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}},\n    \\end{aligned}\\] whose expectation over the random noise gives the proper expected posterior covariances, \\[\\begin{aligned}\n    \\mathrm{E}\\left[\\mathbf{P}^{{\\rm a}}\\right] & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}\n      +\\mathbf{K}\\mathrm{E}\\left[\\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})^{\\mathrm{T}}\\right]\\mathbf{K}^{\\mathrm{T}}\\\\\n     & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}+\\mathbf{K}\\mathbf{R}\\mathbf{K}\\\\\n     & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}.\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-20",
    "href": "slides/week-04/07-DA-stat.html#section-20",
    "title": "Data Assimilation",
    "section": "",
    "text": "Note that the gain can be formulated in terms of the anomaly matrices only, \\[\\mathbf{K}=\\mathbf{X}_{\\mathrm{f}}\\mathbf{Y}_{\\mathrm{f}}^{\\mathrm{T}}\\left(\\mathbf{Y}_{\\mathrm{f}}\\mathbf{Y}_{\\mathrm{f}}^{\\mathrm{T}}\\right)^{-1},\\label{eq:kalman-gain-pert}\\] since\n\n\\(\\mathbf{X}_{\\mathrm{f}}\\mathbf{Y}_{\\mathrm{f}}^{\\mathrm{T}}\\) is a sample estimate for \\(\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}\\) and\n\\(\\mathbf{Y}_{\\mathrm{f}}\\mathbf{Y}_{{\\rm f}}^{\\mathrm{T}}\\) is a sample estimate for \\(\\mathbf{H}\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R}.\\)\n\nIn this form, it is striking that the updated perturbations are linear combinations of the forecast perturbations. The new perturbations are sought within the ensemble subspace of the initial perturbations.\nSimilarly, the state analysis is sought within the affine space \\(\\overline{\\mathbf{x}}^{{\\rm f}}+\\mathrm{vec}\\left(\\mathbf{X}_{1}^{{\\rm f}},\\mathbf{X}_{2}^{{\\rm f}},\\ldots,\\mathbf{X}_{m}^{{\\rm f}}\\right).\\)"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#enkf-forecast-step",
    "href": "slides/week-04/07-DA-stat.html#enkf-forecast-step",
    "title": "Data Assimilation",
    "section": "EnKF — Forecast Step",
    "text": "EnKF — Forecast Step\nIn the forecast step, the updated ensemble obtained at the analysis step is propagated by the model over a time step, \\[\\mbox{for}\\quad i=1,\\ldots,m\\quad\\mathbf{x}_{i,k+1}^{{\\rm f}}=\\mathcal{M}_{k+1}(\\mathbf{x}^{\\mathrm{a}}_{i,k}).\\]\nA forecast can be computed from the mean of theforecast ensemble, while the forecast error covariances can be estimated from the forecast perturbations.\n\n\n\n\n\n\nNote\n\n\n\nThese are only optional diagnostics in the scheme and they are not required in the cycling of the EnKF.\nIt is important to observe that using the tangent linear model(TLM) operator, or any linearization thereof, was avoided.\nThis difference should particularly matter in a significantly nonlinear regime.\nHowever, as we shall see in the Advanced Course Lectures, in strongly nonlinear regimes, the EnKF is largely dominated by schemes known as the iterative EnKF and the iterative ensemble Kalman smoother"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#comparison-enkf-and-4d-var",
    "href": "slides/week-04/07-DA-stat.html#comparison-enkf-and-4d-var",
    "title": "Data Assimilation",
    "section": "Comparison: EnKf and 4D-Var",
    "text": "Comparison: EnKf and 4D-Var\n\n\n\nPrinciple of data assimilation: Having a physical model able to forecast the evolution of a system from time \\(t=t_{0}\\) to time\\(t=T_{f}\\) (cyan curve), the aim of DA is to use available observations (blue triangles) to correct the model projections and get closer to the (unknown) truth (dotted line).\nIn EnKFs, the initial system state and its uncertainty (green square and ellipsoid) are represented by \\(m\\) members."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-21",
    "href": "slides/week-04/07-DA-stat.html#section-21",
    "title": "Data Assimilation",
    "section": "",
    "text": "The members are propagated forward in time during \\(n_{1}\\) model time steps \\(dt\\) to \\(t=T_{1}\\) where observations are available (forecast phase, orange dashed lines).\nAt \\(t=T_{1}\\) the analysis uses the observations and their uncertainty (blue triangle and ellipsoid) to produce a new system state that is closer to the observations and with a lower uncertainty (red square and ellipsoid).\nA new forecastis issued from the analyszd state and this procedure is repeated until the end of the assimilation window at \\(t=T_{f}.\\)\nThe model state should get closer to the truth and with lower uncertainty as more observations are assimilated.\n\nTime-dependent variational methods (4D-Var) iterate over the assimilation window to find the trajectory that minimises the misfit (\\(J_{0}\\)) between the model and all observations available from \\(t_{0}\\) to \\(T_{f}\\) (violet curve).\nFor linear dynamics, Gaussian errors and infinite ensemble sizes, the states produced at the end of the assimilation window by the two methods should be equivalent (Li and Navon 2001)."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#enkf-the-algorithm",
    "href": "slides/week-04/07-DA-stat.html#enkf-the-algorithm",
    "title": "Data Assimilation",
    "section": "EnKF — the Algorithm",
    "text": "EnKF — the Algorithm\n\n\n\nGiven: For \\(k=0,\\ldots,K,\\) observation error cov. matrices\n            \\(\\mathbf{R}_{k}\\), observation models \\(\\mathcal{H}_{k}\\), forward models \\(\\mathcal{M}_{k}.\\)  \nCompute: the ensemble forecast \\(\\left\\{ \\mathbf{x}_{i,k}^{\\mathrm{f}}\\right\\} _{i=1,\\ldots,m,\\,k=1,\\ldots,K}\\) \n\\(\\left(\\mathbf{x}_{i,0}^{\\mathrm{f}}\\right)_{i=1,\\ldots,m}\\)            #Initialize the ensemble\nfor \\(k=0\\) to \\(K\\) do        #Loop over time\n  for  \\(i=1\\) to \\(m\\) do #Draw a stat. consistent obs. set\n    \\(\\mathbf{u}_{i}\\sim{\\cal N}(0,\\mathbf{R}_{k})\\)\n    \\(\\mathbf{y}_{i,k}=\\mathbf{y}_{k}+\\mathbf{u}_{i}\\)\n  end for\n\n\n\n#Compute the ensemble means\n  \\(\\overline{\\mathbf{x}}_{k}^{\\mathrm{f}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}^{\\mathrm{f}}_{i,k}\\,,\\overline{\\mathbf{u}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{u}_{i}\\) \n  \\(\\left[\\mathbf{X}_{\\mathrm{f}}\\right]_{i,k}=\\frac{\\mathbf{x}_{i,k}^{\\mathrm{f}}-\\overline{\\mathbf{x}}_{k}^{\\mathrm{f}}}{\\sqrt{m-1}},\\)   #Compute the normalized anomalies\n  \\(\\left[\\mathbf{Y}_{\\mathrm{f}}\\right]_{i,k}=\\frac{\\mathbf{H}_{k}\\mathbf{x}_{i,k}^{\\mathrm{f}}-\\mathbf{u}_{i}-\\mathbf{H}_{k}\\overline{\\mathbf{x}}_{k}^{\\mathrm{f}}+\\overline{\\mathbf{u}}}{\\sqrt{m-1}}\\)\n   \\(K_{k}=\\mathbf{X}_{k}^{\\mathrm{f}}\\left({\\mathbf{Y}_{k}^{\\mathrm{f}}}\\right)^{\\mathrm{T}}\\left(\\mathbf{Y}_{k}^{\\mathrm{f}}\\left({\\mathbf{Y}_{k}^{\\mathrm{f}}}\\right)^{\\mathrm{T}}\\right)^{-1}\\)    #Compute the gain\n  for \\(i=1\\) to \\(m\\) do              #Update the ensemble\n    \\(\\mathbf{x}_{i,k}^{{\\rm a}}=\\mathbf{x}_{i,k}^{\\mathrm{f}}+K_{k}\\left({\\mathbf{y}_{i,k}-\\mathcal{H}_{k}\\left(\\mathbf{x}^{\\mathrm{f}}_{i,k}\\right)}\\right)\\) \n    \\({\\displaystyle \\mathbf{x}_{i,k+1}^{\\mathrm{f}}=\\mathcal{M}_{k+1}\\left(\\mathbf{x}^{\\mathrm{a}}_{i,k}\\right)}\\)          #Compute the ensemble forecast\n  end for\nend for"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#localization-and-inflation",
    "href": "slides/week-04/07-DA-stat.html#localization-and-inflation",
    "title": "Data Assimilation",
    "section": "Localization and Inflation",
    "text": "Localization and Inflation\nWe have traded the extended Kalman filter for a seemingly considerably cheaper filter meant to achieve similar performances.\nBut this comes with significant drawbacks.\n\nFundamentally, one cannot hope to represent the full error covariance matrix of a complex high-dimensional system with only a few modes \\(m\\ll n\\), usually from a few dozens to a few hundreds.\nThis implies large sampling errors, meaning that the error covariance matrix is only sampled by a limited number of modes.\nThis rank-deficiencyis accompanied by spurious correlations at long distances that strongly affect the filter performance.\nEven though the unstable degrees of freedom of dynamical systems that we wish to control with a filter are usually far fewer than the dimension of the system, they often still represent a substantial fraction of the total degrees of freedom. Forecasting an ensemble of such size is usually not affordable."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-22",
    "href": "slides/week-04/07-DA-stat.html#section-22",
    "title": "Data Assimilation",
    "section": "",
    "text": "The consequence of this issue always is the divergence of the filter.\nHence, the EnKF is useful on the condition that efficient fixes are applied.\n\nTo make it a viable algorithm, one first needs to cope with the rank-deficiency of the filter and with its manifestations, i.e. sampling errors.\nFortunately, there are clever tricks to overcome this major issue, known as localization and inflation, which explains, ultimately, the broad success of the EnKF in geosciences and engineering."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#localization",
    "href": "slides/week-04/07-DA-stat.html#localization",
    "title": "Data Assimilation",
    "section": "Localization",
    "text": "Localization\nFor many systems with geographic spread, distant observables are weakly correlated.\nIn other words, two distant parts of the system are almost independent at least for short time scales.\nIt is possible to exploit this relative independence and spatially localizethe analysis. This has been naturally termed localization.\nThere are two types of localization:\n\nDomain localization, where instead of performing a global analysis valid at any location in the domain, we perform a local analysis to update the local state variables using local observations.\nCovariance localization focuses on the forecast error covariance matrix. It is based on the remark that the forecast error covariance matrix \\(\\mathbf{P}_{{\\rm f}}\\) is of low rank, at most \\(m-1,\\) and that this rank-deficiency could be cured by filtering these empirical covariances.\n\nFor implementation details, please consult (Asch, Bocquet, and Nodet 2016)."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#inflation",
    "href": "slides/week-04/07-DA-stat.html#inflation",
    "title": "Data Assimilation",
    "section": "Inflation",
    "text": "Inflation\nEven when the analysis is made local, the error covariance matrices are still evaluated with an ensemble of limited size.\n\nThis often leads to sampling errors and spurious correlations.\nWith a proper localization scheme, they might be significantly reduced.\nHowever small are the residual errors, they will accumulate and they will carry over to the next cycles of the sequential EnKF scheme. As a consequence, there is always a risk that the filter may ultimately diverge.\n\nOne way around is to inflate the error covariance matrix by a factor \\(\\lambda^{2}\\) slightly greater than \\(1\\) before or after the analysis.\n\nFor instance, after the analysis, \\[\\mathbf{P}^{\\rm a}\\longrightarrow\\lambda^{2}\\mathbf{P}^{\\mathrm{a}}.\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-23",
    "href": "slides/week-04/07-DA-stat.html#section-23",
    "title": "Data Assimilation",
    "section": "",
    "text": "Another way to achieve this is to inflate the ensemble, \\[\\mathbf{x}^{\\mathrm{a}}_{i}\\longrightarrow\\overline{\\mathbf{x}}^{{\\rm a}}+\\lambda{\\mathbf{x}^{\\mathrm{a}}_{i}-\\overline{\\mathbf{x}}^{{\\rm a}}},\\] which can alternatively be enforced on the prior (forecast) ensemble. This type of inflation is called multiplicative inflation.\n\nFor implementation details, please consult (Asch, Bocquet, and Nodet 2016)."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-24",
    "href": "slides/week-04/07-DA-stat.html#section-24",
    "title": "Data Assimilation",
    "section": "",
    "text": "EXAMPLES"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-25",
    "href": "slides/week-04/07-DA-stat.html#section-25",
    "title": "Data Assimilation",
    "section": "",
    "text": "As in the previous Lecture, we consider the same scalar 4D-Var example, but this time apply the Kalman filter to it.\nWe take the most simple linear forecast model, \\[\\frac{\\mathrm{d}x}{\\mathrm{d}t}=-\\alpha x,\\] with \\(\\alpha\\) a known positive constant.\nWe assume the same discrete dynamics considered in with a single observation at time step \\(3.\\)\nThe stochastic system 1-2 is \\[\\begin{aligned}\n    x_{k+1}^{\\mathrm{t}} & =M(x_{k}^{\\mathrm{t}})+w_{k},\\\\\n    y_{k+1} & =x_{k}^{\\mathrm{t}}+v_{k},\n    \\end{aligned}\\] where \\(w_{k}\\thicksim\\mathcal{N}(0,\\sigma_{Q}^{2}),\\) \\(v_{k}\\thicksim\\mathcal{N}(0,\\sigma_{R}^{2})\\) and \\(x_{0}^{\\mathrm{t}}-x_{0}^{\\mathrm{b}}\\thicksim\\mathcal{N}(0,\\sigma_{B}^{2}).\\)"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-26",
    "href": "slides/week-04/07-DA-stat.html#section-26",
    "title": "Data Assimilation",
    "section": "",
    "text": "The Kalman filter steps are\nForecast:\n\\[\\begin{aligned}\nx_{k+1}^{\\mathrm{f}} & =M(x_{k}^{\\mathrm{a}})=\\gamma x_{k},\\\\\nP_{k+1}^{\\mathrm{f}} & =\\gamma^{2}P_{k}^{\\mathrm{a}}+\\sigma_{Q}^{2}.\n\\end{aligned}\\]\nAnalysis:\n\\[\\begin{aligned}\nK_{k+1} & =P_{k+1}^{\\mathrm{f}}H\\left(H^{2}P_{k+1}^{\\mathrm{f}}+\\sigma_{R}^{2}\\right)^{-1},\\\\\nx_{k+1}^{\\mathrm{a}} & =x_{k+1}^{\\mathrm{f}}+K_{k+1}(x_{k+1}^{\\mathrm{o}}-Hx_{k+1}^{\\mathrm{f}}),\\\\\nP_{k+1}^{\\mathrm{a}} & =(1-K_{k+1}H)P_{k+1}^{\\mathrm{f}}=\\left(\\frac{1}{P_{k+1}^{\\mathrm{f}}}+\\frac{1}{\\sigma_{R}^{2}}\\right)^{-1},\\quad H=1.\n\\end{aligned}\\]\nInitialization:\n\\[\\begin{equation}\nx_{0}^{\\mathrm{a}}  =x_{0}^{\\mathrm{b}},\\quad\nP_{0}^{\\mathrm{a}}  =\\sigma_{B}^{2}.\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-27",
    "href": "slides/week-04/07-DA-stat.html#section-27",
    "title": "Data Assimilation",
    "section": "",
    "text": "We start with the initial state, at time step \\(k=0.\\) The initial conditions are as above. The forecast is \\[\\begin{aligned}\n    x_{1}^{\\mathrm{f}} & =M(x_{0}^{\\mathrm{a}})=\\gamma x_{0}^{\\mathrm{b}},\\\\\n    P_{1}^{\\mathrm{f}} & =\\gamma^{2}\\sigma_{B}^{2}+\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nSince there is no observation available, \\(H=0,\\) and the analysis gives,\n\\[\\begin{aligned}\n    K_{1} & =0,\\\\\n    x_{1}^{\\mathrm{a}} & =x_{1}^{\\mathrm{f}}=\\gamma x_{0}^{\\mathrm{b}},\\\\\n    P_{1}^{\\mathrm{a}} & =P_{1}^{\\mathrm{f}}=\\gamma^{2}\\sigma_{B}^{2}+\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nAt the next time step, \\(k=1,\\) and the forecast gives\n\\[\\begin{aligned}\n    x_{2}^{\\mathrm{f}} & =M(x_{1}^{\\mathrm{a}})=\\gamma^{2}x_{0}^{\\mathrm{b}},\\\\\n    P_{2}^{\\mathrm{f}} & =\\gamma^{2}P_{1}^{\\mathrm{a}}+\\sigma_{Q}^{2}=\\gamma^{4}\\sigma_{B}^{2}+(\\gamma^{2}+1)\\sigma_{Q}^{2}.\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-28",
    "href": "slides/week-04/07-DA-stat.html#section-28",
    "title": "Data Assimilation",
    "section": "",
    "text": "Once again there is no observation available, \\(H=0,\\) and the analysis yields \\[\\begin{aligned}\n    K_{2} & =0,\\\\\n    x_{2}^{\\mathrm{a}} & =x_{2}^{\\mathrm{f}}=\\gamma^{2}x_{0}^{\\mathrm{b}},\\\\\n    P_{2}^{\\mathrm{a}} & =P_{2}^{\\mathrm{f}}=\\gamma^{4}\\sigma_{B}^{2}+(\\gamma^{2}+1)\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nMoving on to \\(k=2,\\) we have the new forecast, \\[\\begin{aligned}\n    x_{3}^{\\mathrm{f}} & =M(x_{2}^{\\mathrm{a}})=\\gamma^{3}x_{0}^{\\mathrm{b}},\\\\\n    P_{3}^{\\mathrm{f}} & =\\gamma^{2}P_{2}^{\\mathrm{a}}+\\sigma_{Q}^{2}=\\gamma^{6}\\sigma_{B}^{2}+(\\gamma^{4}+\\gamma^{2}+1)\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nNow there is an observation, \\(x_{3}^{\\mathrm{o}},\\) available, so \\(H=1\\) and the analysis is \\[\\begin{aligned}\n    K_{3} & =P_{3}^{\\mathrm{f}}\\left(P_{3}^{\\mathrm{f}}+\\sigma_{R}^{2}\\right)^{-1},\\\\\n    x_{3}^{\\mathrm{a}} & =x_{3}^{\\mathrm{f}}+K_{3}(x_{3}^{\\mathrm{o}}-x_{3}^{\\mathrm{f}}),\\\\\n    P_{3}^{\\mathrm{a}} & =(1-K_{3})P_{3}^{\\mathrm{f}}.\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-29",
    "href": "slides/week-04/07-DA-stat.html#section-29",
    "title": "Data Assimilation",
    "section": "",
    "text": "Substituting and simplifying, we find \\[x_{3}^{\\mathrm{a}}=\\gamma^{3}x_{0}^{\\mathrm{b}}+\\frac{\\gamma^{6}\\sigma_{B}^{2}+(\\gamma^{4}+\\gamma^{2}+1)\\sigma_{Q}^{2}}{\\sigma_{R}^{2}+\\gamma^{6}\\sigma_{B}^{2}+(\\gamma^{4}+\\gamma^{2}+1)\\sigma_{Q}^{2}}\\left(x_{3}^{\\mathrm{o}}-\\gamma^{3}x_{0}^{\\mathrm{b}}\\right).\\label{eq:saclarKF_xa} \\tag{9}\\]\nCase 1: Assume we have a perfect model, then \\(\\sigma_{Q}^{2}=0\\) and the Kalman filter state 9 becomes \\[x_{3}^{\\mathrm{a}}=\\gamma^{3}x_{0}^{\\mathrm{b}}+\\frac{\\gamma^{6}\\sigma_{B}^{2}}{\\sigma_{R}^{2}+\\gamma^{6}\\sigma_{B}^{2}}\\left(x_{3}^{\\mathrm{o}}-\\gamma^{3}x_{0}^{\\mathrm{b}}\\right),\\] which is precisely the 4D-Var expression obtained before.\nCase 2: When the parameter \\(\\alpha\\) tends to zero, then \\(\\gamma\\) tends to one, the model is stationary and the Kalman filter state 9 becomes \\[x_{3}^{\\mathrm{a}}=x_{0}^{\\mathrm{b}}+\\frac{\\sigma_{B}^{2}+3\\sigma_{Q}^{2}}{\\sigma_{R}^{2}+\\sigma_{B}^{2}+3\\sigma_{Q}^{2}}\\left(x_{3}^{\\mathrm{o}}-x_{0}^{\\mathrm{b}}\\right),\\]"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-30",
    "href": "slides/week-04/07-DA-stat.html#section-30",
    "title": "Data Assimilation",
    "section": "",
    "text": "which, when \\(\\sigma_{Q}^{2}=0,\\) reduces to the 3D-Var solution, \\[x_{3}^{\\mathrm{a}}=x_{0}^{\\mathrm{b}}+\\frac{\\sigma_{B}^{2}}{\\sigma_{R}^{2}+\\sigma_{B}^{2}}\\left(x_{3}^{\\mathrm{o}}-x_{0}^{\\mathrm{b}}\\right),\\] that was obtained before.\nCase 3: When \\(\\alpha\\) tends to infinity, then \\(\\gamma\\) goes to zero, and we are in the case where there is no longer any memory with \\[x_{3}^{\\mathrm{a}}=\\frac{\\sigma_{Q}^{2}}{\\sigma_{R}^{2}+\\sigma_{Q}^{2}}x_{3}^{\\mathrm{o}}.\\] Then, if the model is perfect, \\(\\sigma_{Q}^{2}=0\\) and \\(x_{3}^{\\mathrm{a}}=0.\\) If the observation is perfect, \\(\\sigma_{R}^{2}=0\\) and \\(x_{3}^{\\mathrm{a}}=x_{3}^{\\mathrm{o}}.\\)\nThis example shows the complete chain, from the Kalman filter solution, through the 4D-Var, and finally reaching the 3D-Var one. Hopefully this clarifies the relationship between the three and demonstrates why the Kalman filter provides the most general solution possible."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-31",
    "href": "slides/week-04/07-DA-stat.html#section-31",
    "title": "Data Assimilation",
    "section": "",
    "text": "PRACTICAL GUIDELINES"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#general-guidelines",
    "href": "slides/week-04/07-DA-stat.html#general-guidelines",
    "title": "Data Assimilation",
    "section": "General Guidelines",
    "text": "General Guidelines\nWe briefly point out some important practical considerations. It should now be clear that there are four basic ingredients in any inverse or data assimilation problem:\n\nObservation or measured data.\nA forward or direct model of the real-world context.\nA backwards or adjoint model, in the variational case. A probabilistic framework, in the statistical case.\nAn optimization cycle.\n\nBut where does one start?\nThe traditional approach, often employed in mathematical and numerical modeling, is to begin with some simplified, or at least well-known, situation.\nOnce the above four items have been successfully implemented and tested on this instance, we then proceed to take into account more and more reality in the form of real data, more realistic models, more robust optimization procedures, etc."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-32",
    "href": "slides/week-04/07-DA-stat.html#section-32",
    "title": "Data Assimilation",
    "section": "",
    "text": "In other words, we introduce uncertainty, but into a system where we at least control some of the aspects.\nTwin experiments, or synthetic runs, are a basic and indispensable tool for all inverse problems. In order to evaluate the performance of a data assimilation system we invariably begin with the following methodology.\n\nFix all parameters and unknowns and define a reference trajectory, obtained from a run of the direct model call this the “truth”.\nDerive a set of (synthetic) measurements, or background data, from this “true” run.\nOptionally, perturb these observations in order to generate a more realistic observed state.\nRun the data assimilation or inverse problem algorithm, starting from an initial guess (different from the “true” initial state used above), using the synthetic observations.\nEvaluate the performance, modify the model/algorithm/observations, and cycle back to step 1."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-33",
    "href": "slides/week-04/07-DA-stat.html#section-33",
    "title": "Data Assimilation",
    "section": "",
    "text": "Twin experiments thus provide a well-structured methodological framework.\nWithin this framework we can perform different “stress tests” of our system.\nWe can modify the observation network,\n\nincrease or decrease (even switch off) the uncertainty,\ntest the robustness of the optimization method,\neven modify the model.\n\nIn fact, these experiments can be performed on the full physical model, or on some simpler (or reduced-order) model.\nToy models are, by definition, simplified models that we can play with. Yes, but these are of course “serious games.” In certain complex physical contexts, of which meteorology is a famous example, we have well-established toy models, often of increasing complexity. These can be substituted for the real model, whose computational complexity is often too large, and provide a cheaper test-bed."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#section-34",
    "href": "slides/week-04/07-DA-stat.html#section-34",
    "title": "Data Assimilation",
    "section": "",
    "text": "Some well-known examples of toy models are:\n\nLorenz models that are used as an avatar for weather simulations.\nVarious harmonic oscillators that are used to simulate dynamic systems.\nOther well-known models are the Ising model in physics, the Lotka-Volterra model in life sciences, and the Schelling model in social sciences.\n\nMachine Learning (ML) is becoming more and more present in our daily lives, and in scientific research. The use of ML in DA and Inverse modeling will be dealt with in the Advanced Course, where we will consider:\n\nML-based Surrogate Models.\nML-based Surrogate Models.\nScientific ML."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#kalman-filter-extensions",
    "href": "slides/week-04/07-DA-stat.html#kalman-filter-extensions",
    "title": "Data Assimilation",
    "section": "Kalman Filter — extensions",
    "text": "Kalman Filter — extensions\nThere are many variants, extensions and generalizations of the Kalman Filter.\nIn the Advanced Course, we will study in more detail:\nensemble Kalman Filters\n\nBayesian and nonlinear Kalman Filters: extended, unscented\nparticle filters\n\nOne usually has to choose between\n\nlinear Kalman filters\nensemble Kalman filters\nnonlinear filters\nhybrid variational-filter methods.\n\nThese questions will be addressed later."
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#where-are-we-in-the-inference-cycle",
    "href": "slides/week-04/07-DA-stat.html#where-are-we-in-the-inference-cycle",
    "title": "Data Assimilation",
    "section": "Where are we in the Inference cycle",
    "text": "Where are we in the Inference cycle\n\n\nFigure 4: DA is the inductive phase of DTs"
  },
  {
    "objectID": "slides/week-04/07-DA-stat.html#open-source-software",
    "href": "slides/week-04/07-DA-stat.html#open-source-software",
    "title": "Data Assimilation",
    "section": "Open-source software",
    "text": "Open-source software\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Spring 2024 syllabus: Digital Twins for Physical Systems — 33655 - CSE 8803 - DTP\nSyllabus updates:This syllabus is subject to change throughout the semester. We will try to keep these updates to a minimum.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be made familiar with …\n\nthe concept of Digital Twins and how they interact with their environment\nStatistical Inverse Problems and Bayesian Inference\ntechniques from Data Assimilation (DAT), Simulation-Based Inference (SBI), and Recursive Bayesian Inference (RBI), and Uncertainty Quantification [UQ]\n\nFor more on the Course outline, Topics, and Learning goals, see Goals.\nFor a motivational article see Digital Twins in the era of generative AI",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\n\n\n\nData assimilation: methods, algorithms, and applications\nAsch, Bocquet, and Nodet (2016), Mark\nSIAM, 2016\n\n\nA toolbox for digital twins: from model-based to data-driven\nAsch (2022), Mark\nSIAM, 2022",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#journal-papers",
    "href": "syllabus.html#journal-papers",
    "title": "Syllabus",
    "section": "Journal Papers",
    "text": "Journal Papers\n\n\n\nA comprehensive review of digital twin—part 1: modeling and twinning enabling technologies\nThelen et al. (2022)\nSpringer, 2022\n\n\nA comprehensive review of digital twin—part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives\nThelen et al. (2023)\nSpringer, 2022\n\n\nSequential Bayesian inference for uncertain nonlinear dynamic systems: A tutorial\nTatsis, Dertimanis, and Chatzi (2022)\nArxiv, 2022",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#lectures-computational-labs",
    "href": "syllabus.html#lectures-computational-labs",
    "title": "Syllabus",
    "section": "Lectures & computational labs",
    "text": "Lectures & computational labs\nIn part the course is an adaptation of open-source Basic and Advanced courses (CSU-IMU-2023) developed by Mark Asch. Codes will be drawn from the course and from the book. During the second half of the course, we will draw from the current literature.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Georgia Tech’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated.\n\n\nDiscrimination, Harassment, and Sexual Misconduct\nTo maintain a safe learning environment that fosters the dignity, respect, and success of students, faculty, and staff, Tech prohibits discriminatory harassment, which is unwelcome verbal, nonverbal, or physical conduct directed at any person or group based upon race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity, or veteran status that has the purpose or effect of creating an objectively hostile working or academic environment.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nDisability Services are available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website.\nAnnouncements will be emailed through Sakai Announcements periodically. Please check your email regularly to ensure you have the latest announcements for the course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\n\nComputational labs\nIn labs, you will apply the concepts discussed during lectures, with a focus on the computation.\n\n\nExams\nThere will be no exams.\n\n\nFinal Project\nThe purpose of the Final Project is to apply what you’ve learned throughout the semester to analyze an interesting data-driven research question.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nComp. Lab assignments\n30%\n\n\nPresentation Journal Paper\n20%\n\n\nFinal Project & Presentation\n50%",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 4: Phase II Registration\nJan 8: Classes begin\nJan 12: Deadline for Registration/Schedule Changes\nMarch 13: Last day to withdraw\nMarch 18-22: Spring break\nApril 24: Final Instructional Class",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nPlease abide by the following as you work on assignments in this course:\n\nYou may discuss individual lab assignments with other students; however, you may not directly share (or copy) code or write up with other students. For team assignments, you may collaborate freely within your team. You may discuss the assignment with other teams; however, you may not directly share (or copy) code or write up with another team. Unauthorized sharing (or copying) of the code or write up will be considered a violation for all students involved.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\n\n\nGuidance on the use of Generative AI\nAI-based assistance, such as ChatGPT and Copilot, can be used in the same way as collaboration with other people for the following activities: For lab assignments in understanding the lecture and textbook materials, and in developing your ideals for the project proposals. In these activities you are welcome to talk about your ideas and work with other people, both inside and outside the class, as well as with AI- based assistants. In the course, we will discuss how you can use these tools as a study tool, or as an assistant in helping to draft code or summaries.\n\nGenerating Research Ideas or Approaches:\n\nBrainstorming: You can use the AI tool as a brainstorming partner, where you exchange ideas whether the AI prompts you or you prompt the AI for ideas. Brainstorming is an iterative process that can be made more effective with the way that the queries are posted. For more samples or information, post this query: “How can I use AI to help me to brainstorm an idea?”\nSurveying Existing Approaches: Large Language Models, if trained broadly in a topic, can give a good initial overview of existing approaches or existing literature on a topic. Current research sources such as library or professional society databases are more reliable in terms of accuracy of peer-reviewed content.\nPrompt engineering is important: Practice the prompts used for Generative AI. The value of the response depends on the value of the prompt. If you provide a low quality or vague prompt, you will get vague results. Varying skill levels among users might exacerbate existing inequalities among students. For example, students for whom English is the second language might be at a disadvantage.\n\n\n\nAdvice on Usage:\n\nBe very skeptical of the results. Do not trust any outputs that you cannot evaluate yourself or trace back to original credible sources. There are many stories of generative AI giving citations of articles that do not exist (see the article in the Chronicles of Higher Education referenced below).\nBe scientific with your prompts (or queries): Prompting is not deterministic, so the same prompt at a different time may result in a different response. Small changes in the wording of the prompt may yield very different responses. Keep records, make small changes and see how it affects the outcome, etc.\nDon’t share any data or information that is confidential, proprietary, or have IP implications. Your uploaded data or ideas might be incorporated into the learning model to be available for others in your research area, prior to you having a chance to publish it. If you intend to pursue commercialization or other Intellectual Property avenues for your work, putting the information into an open AI platform may be considered as disclosure.\n\n\n\n\nLate work & extensions\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest lab assignments will be dropped to accommodate such circumstances.\n\n\nCampus Resources for Students\nLinks to Campus Resources for Students — e.g. Center for Academic Success, Communication Center, Office of Disability Services, OMED, Division of Student Life, etc. available at: https://ctl.gatech.edu/sites/default/files/documents/campus_resources_students.pdf\n\nQuick Guide on Student Mental Health and Wellness Resources:\nThe Center for Mental Health Care & Resources is here to offer confidential support and services to students in need of mental health care. During regular business hours, students who are not actively in counseling may call 404-894-2575 or walk-in to the office , 353 Ferst DR NW Atlanta GA 30313 (Flag building next to the Student Center). Any time outside of business hours, students may call 404-894-2575 and select the option to speak to the after-hours counselor.\n\n\nAdditional On Campus Options\nStudents who are experiencing an immediate life-threatening emergency on campus, can call the Georgia Tech Campus Police at 404-894-2500\n\n\nLocal and National crisis resources available 24/7:\n\nCrisis Text Line: Text HOME to 741741 https://www.crisistextline.org/\nGeorgia Crisis & Access Line: Call 1-800-715-4225\nNational Suicide Prevention Lifeline: Call 988 or 1-800-273-8255, or access text chat here https://suicidepreventionlifeline.org/chat/\n\n\n\nAdditional services offered by Satellite counselors’ (walk-in consultation hours):\nEach Satellite counselor provides weekly consultation appointments at varying times based on their schedules (see highlighted section below for my Fall ’23 hours). Anyone can access a Satellite counselor during these times, to set up a consultation email is preferable, once a message is received the Satellite counselor will reply with available dates/times and a link. Some Satellite counselors will accommodate in-person walk-ins at these times as well.\n\nWhat it is\n\nConsulting about a brief, non-emergency concern during the counselor’s posted walk-in consultation hours (similar to Let’s Talk). Consultations are approximately 15 minutes.\nProviding information to students about mental health resources on campus and how to get connected\nConsulting with faculty/staff about a student of concern\n\n\n\nWhat it isn’t\n\nNot for a walk-in counseling appointment or initial assessment\nNot for students who are seeking support in their counselor’s absence. Students who need additional/urgent support when their counselor is unavailable should visit the Center for Mental Health Care & Resources\nNot for crisis\nNot for case management\n\nTara Holdampf, MS, LPC, (she/her), Satellite Counselor’s Consultation Hours for the College of Sciences: Please email tara.holdampf@studentlife.gatech.edu for an appointment, or stop by my Office Location: MoSE 1120B. Monday-Friday 3:00 PM - 4:00 PM\n\n\nOther useful web pages to explore:\nGT Wellness Hub: https://gtwellnesshub.com (self-paced online resources for students)\n\nFree Headspace (for Students Only): https://gtwellnesshub.com/personal-guide-to-health- happiness/\nTogetherall (for Students Only): https://gtwellnesshub.com/one-on-one-personal-assistance/\nSilver cloud (for Students Only): https://gtwellnesshub.com/helpful-resources-right-at-your- fingertips/\n\nDivision of Student Engagement and Wellbeing: https://students.gatech.edu/\nHealth, Wellness & Recreation: https://students.gatech.edu/health-wellness-recreation\nTech Ends Suicide: https://mentalhealth.gatech.edu/end-suicide-initiative\nMental Health Services: https://mentalhealth.gatech.edu/about/services",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final Project",
    "section": "",
    "text": "Final Project\n\nGoal\nThe goal of the Final Project is to give students the opportunity to apply and further develop their understanding of Digital Twins to a topic close to their graduate research interests.\n\n\nRequirements\nThe Final Project consist of three parts\n\nProposal consisting of a “one pager” + “5 min pitch”\nProject presentation of 20 minutes with 5 minutes questions, presented at the end of the term\nA final report\n\n\nOne pager + 5 min pitch\nOn March 1th, everybody needs to give a 5 minute pitch on their project proposal. That same day, the “one pager” outlining the project is due by 6:00 PM. See description below. Your seminar will be evaluated with this form. Please send the slides for your presentation to the course instructor with CSE 8803 in the subject line or bring a memory stick. The 5 minutes presentations include time for questions. Since time is limited, the 5 minute limit will be strictly enforced. So please stick to the main message, the problem you want to tackle, what techniques you will be using, and why it is important.\n\n\nProposal\nConcise description of the work to be undertaken. The description should not exceed two pages and should contain\n\nTitle of the Final Project\nProblem Statement\nRelevance\nIdentification of methodology that will be used to solve the problem\n\nProposal should be submitted in PDF format or a link to an html page. The project proposals are due March 1 at 6:00 PM and presentations are scheduled for that same day during class. The presentations are five minutes each for each person.\n\n\nProject presentation at the end of the term\nEach project presentation will be allotted 20 minutes precisely with 5 minutes of questions. The seminar should reflect the current status of the project, which may be subject to change when the project is finalized.\nThe seminar will be evaluated using the following form2.\n\n\nFinal Report\nThe report needs to be written as a short journal paper. Students will be required to use the IEEE template in two-column mode. The paper should be four-to-six pages including abstract, body text, figures, and bibliography. Reports should consist of\n\nabstract\nintroduction\nproblem statement\nmethodology\nresults\nconclusions\nreferences\nappendices\n\n\n\n\nGrading (project alone)\n\n10% Proposal plus speed presentation\n30% Seminar\n60% Final Report\n\n\n\nLate policy\nLate submission is unacceptable and will lead to deductions in your grade. The deadline for submission will be communicate in class.\n\n\n\n\nCalendar for events\n\n\n\n\n\n\n\nReuseCC-BY",
    "crumbs": [
      "Final Project"
    ]
  }
]