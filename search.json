[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nDates\n\n\nTopic\n\n\n\n\n\n\nWeek 01\n\n\nJan 08 - Jan 12\n\n\nWelcome to Digital Twins for Physical Systems\n\n\n\n\nWeek 02\n\n\nJanuary 15 - 19\n\n\nAdjoint methods for inverse problems\n\n\n\n\nWeek 03\n\n\nJanuary 22- 26\n\n\nAutomatic Differentiation & Variational Data Assimilation\n\n\n\n\nWeek 04\n\n\nJan/Feb 29 - 02\n\n\nData Assimilation\n\n\n\n\nWeek 05\n\n\nFeb 05 - 09\n\n\nStatistical Data Assimilation\n\n\n\n\nWeek 06\n\n\nFeb 12 - 16\n\n\nBayesian Data Assimilation\n\n\n\n\nWeek 07\n\n\nFebruary 19 - 23\n\n\nMachine Learning\n\n\n\n\nWeek 08\n\n\nMarch 04 - 08\n\n\nMachine Learning\n\n\n\n\nWeek 09\n\n\nMarch 11 - 14\n\n\nMachine Learning\n\n\n\n\n\nNo matching items\n\n\n\n\nLabs will be due on Fridays at 11:59pm.\n\n\n\nLabs can be rerun in the language of your choice (Julia/R) but if you chose to reuse the Python code then you can easily run on Google Collab. This service includes a free tier that only requires a Google account. For future projects that require intensive matrix multiplication operations such as convolutional networks a hardware accelerator will drastically improve execution time. These are accessible in Collab by restarting the notebook after: Runtime -&gt; Change runtime type -&gt; Hardware accelerator = T4 GPU (in free tier).\n\n\n\nTo submit assignments please export your notebook as PDF and send to TA’s Gatech email: Rafael Orozco with subject line that includes the phrase “CSE-8803 Lab [# of lab]”. To export notebook as PDF in Google collab: File -&gt; Print -&gt; Set Destination to “save as PDF” -&gt; Save.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#weekly-schedule",
    "href": "schedule.html#weekly-schedule",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nDates\n\n\nTopic\n\n\n\n\n\n\nWeek 01\n\n\nJan 08 - Jan 12\n\n\nWelcome to Digital Twins for Physical Systems\n\n\n\n\nWeek 02\n\n\nJanuary 15 - 19\n\n\nAdjoint methods for inverse problems\n\n\n\n\nWeek 03\n\n\nJanuary 22- 26\n\n\nAutomatic Differentiation & Variational Data Assimilation\n\n\n\n\nWeek 04\n\n\nJan/Feb 29 - 02\n\n\nData Assimilation\n\n\n\n\nWeek 05\n\n\nFeb 05 - 09\n\n\nStatistical Data Assimilation\n\n\n\n\nWeek 06\n\n\nFeb 12 - 16\n\n\nBayesian Data Assimilation\n\n\n\n\nWeek 07\n\n\nFebruary 19 - 23\n\n\nMachine Learning\n\n\n\n\nWeek 08\n\n\nMarch 04 - 08\n\n\nMachine Learning\n\n\n\n\nWeek 09\n\n\nMarch 11 - 14\n\n\nMachine Learning\n\n\n\n\n\nNo matching items\n\n\n\n\nLabs will be due on Fridays at 11:59pm.\n\n\n\nLabs can be rerun in the language of your choice (Julia/R) but if you chose to reuse the Python code then you can easily run on Google Collab. This service includes a free tier that only requires a Google account. For future projects that require intensive matrix multiplication operations such as convolutional networks a hardware accelerator will drastically improve execution time. These are accessible in Collab by restarting the notebook after: Runtime -&gt; Change runtime type -&gt; Hardware accelerator = T4 GPU (in free tier).\n\n\n\nTo submit assignments please export your notebook as PDF and send to TA’s Gatech email: Rafael Orozco with subject line that includes the phrase “CSE-8803 Lab [# of lab]”. To export notebook as PDF in Google collab: File -&gt; Print -&gt; Set Destination to “save as PDF” -&gt; Save.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "slides/week-04/05-variational.html#data-assimilation",
    "href": "slides/week-04/05-variational.html#data-assimilation",
    "title": "Variational Data Assimilation",
    "section": "Data Assimilation",
    "text": "Data Assimilation\n\nDefinition 1 Data assimilation (DA) is the approximation of the true state of some physical system at a given time, by combining time-distributed observations with a dynamic model in an optimal way.\n\nDA is an approach for solving a specific class of inverse, or parameter estimation problems, where the parameter we seek is the initial condition.\nDA can be classically approached in two ways:\n\nvariational DA\nstatistical DA"
  },
  {
    "objectID": "slides/week-04/05-variational.html#outline",
    "href": "slides/week-04/05-variational.html#outline",
    "title": "Variational Data Assimilation",
    "section": "Outline",
    "text": "Outline\nAdjoint methods and variational data assimilation (4h)\n\nIntroduction to data assimilation: setting, history, overview, definitions.\nAdjoint method.\nVariational data assimilation methods:\n\n3D-Var,\n4D-Var.\n\n\nStatistical estimation, Kalman filters and sequential data assimilation (4h)\n\n\nIntroduction to statistical DA.\nStatistical estimation.\nThe Kalman filter.\nNonlinear extensions and ensemble filters."
  },
  {
    "objectID": "slides/week-04/05-variational.html#reference-text-books",
    "href": "slides/week-04/05-variational.html#reference-text-books",
    "title": "Variational Data Assimilation",
    "section": "Reference Text Books",
    "text": "Reference Text Books"
  },
  {
    "objectID": "slides/week-04/05-variational.html#overview",
    "href": "slides/week-04/05-variational.html#overview",
    "title": "Variational Data Assimilation",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nFrom Asch (2022)"
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-daformulation",
    "href": "slides/week-04/05-variational.html#variational-daformulation",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—formulation",
    "text": "Variational DA—formulation\nIn variational data assimilation we describe the state of the system by\n\na state variable, \\(\\mathbf{x}(t)\\in\\mathcal{X},\\) a function of space and time that\nrepresents the physical variables of interest, such as current velocity (in oceanography), temperature, sea-surface height, salinity, biological species concentration, chemical concentration, etc.\n\nEvolution of the state is described by a system of (in general nonlinear) differential equations in a region \\(\\Omega,\\) \\[\\left\\{ \\begin{aligned} & \\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t}=\\mathcal{M}(\\mathbf{x})\\quad\\mathrm{in}\\;\\Omega\\times\\left[0,T\\right],\\\\\n     & \\mathbf{x}(t=0)=\\mathbf{x}_{0},\n    \\end{aligned}\n    \\right. \\tag{1}\\] where the initial condition is unknown (or poorly known)."
  },
  {
    "objectID": "slides/week-04/05-variational.html#section",
    "href": "slides/week-04/05-variational.html#section",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "Suppose that we are in possession of observations \\(\\mathbf{y}(t)\\in\\mathcal{O}\\) and an observation operator \\(\\mathcal{H}\\) that describes the available observations.\nThen, to characterize the difference between the observations and the state, we define the objective (or cost) function, \\[J(\\mathbf{x}_{0})=\\frac{1}{2}\\int_{0}^{T}\\left\\Vert \\mathbf{y}(t)-\\mathcal{H}\\left(\\mathbf{x}(\\mathbf{x}_{0},t)\\right)\\right\\Vert _{\\mathcal{O}}^{2}\\mathrm{d}t+\\frac{1}{2}\\left\\Vert \\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}\\right\\Vert _{\\mathcal{X}}^{2} \\tag{2}\\] where\n\n\\(\\mathbf{x}^{\\mathrm{b}}\\) is the background (or first guess)\nand the second term plays the role of a regularization (in the sense of Tikhonov see previous Lecture.\nThe two norms under the integral, in the finite-dimensional case, will be represented by the error covariance matrices \\(\\mathbf{R}\\) and \\(\\mathbf{B}\\) respectively, and will be described below.\nNote that for mathematical rigor we have indicated, as subscripts, the relevant functional spaces on which the norms are defined."
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-1",
    "href": "slides/week-04/05-variational.html#section-1",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "In the continuous context, the data assimilation problem is formulated as follows:\n\n\n\n\n\n\nImportant\n\n\nFind the analyzed state \\(\\mathbf{x}^a_0\\) that minimizes \\(J\\) and satisfies\n\\[\\mathbf{x}^a_0=\\mathop{\\mathrm{arg\\,min}}_{\\mathbf{x}_0} J(\\mathbf{x}_0)\\]\n\n\n\nThe necessary condition for the existence of a (local) minimum is (as usual...) \\[\\nabla J(\\mathbf{x}_{0}^{\\mathrm{a}})=0.\\]\nVariational DA is based on an adjoint approach that is explained in the previous Lecture.\nThe particularity here is that the adjoint is used to solve an inverse problem for the unknown initial condition."
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-da3d-var",
    "href": "slides/week-04/05-variational.html#variational-da3d-var",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—3D Var",
    "text": "Variational DA—3D Var\nUsually, 3D-Var and 4D-Var are introduced in a finite dimensional or discrete context this approach will be used in this section.\nFor the infinite dimensional or continuous case, we must use the calculus of variations and (partial) differential equations, as was done in the previous Lectures.\nWe start out with the finite-dimensional version of the cost function 2, \\[\\begin{aligned}\n    J(\\mathbf{x}) & =\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}^{\\mathrm{b}}\\right)^{\\mathrm{T}}\\mathbf{B}^{-1}\\left(\\mathbf{x}-\\mathbf{x}^{\\mathrm{b}}\\right)\\\\\n     & +\\frac{1}{2}\\left(\\mathbf{Hx}-\\mathbf{y}\\right)^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{Hx}-\\mathbf{y}\\right),\n    \\end{aligned} \\tag{3}\\] where\n\n\\(\\mathbf{x},\\) \\(\\mathbf{x}^{\\mathrm{b}},\\) and \\(\\mathbf{y}\\) are the state, the background state, and the measured state respectively;"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-2",
    "href": "slides/week-04/05-variational.html#section-2",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\(\\mathbf{H}\\) is the observation matrix (a linearization of the observation operator \\(\\mathcal{H}\\) );\n\\(\\mathbf{R}\\) and \\(\\mathbf{B}\\) are the observation and background error covariance matrices respectively.\n\nThis quadratic function attempts to strike a balance between some a priori knowledge about a background (or historical) state and the actual measured, or observed, state.\nIt also assumes that we know and that we can invert the matrices \\(\\mathbf{R}\\) and \\(\\mathbf{B}\\). This, as we will be pointed out below, is not always obvious.\nFurthermore, it represents the sum of the (weighted) background deviations and the (weighted) observation deviations. The basic methodology is presented in the Algorithm below, which is nothing more than a classical gradient descent algorithm."
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-3",
    "href": "slides/week-04/05-variational.html#section-3",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\(j=0\\), \\(x=x_{0}\\)\nwhile \\(\\left\\Vert \\nabla J\\right\\Vert &gt;\\epsilon\\) or \\(j\\le j_{\\mathrm{max}}\\)\n    compute \\(J\\)\n    compute \\(\\nabla J\\)\n    gradient descent and update of \\(x_{j+1}\\)\n    \\(j=j+1\\)\nend\n\n\n\n\n\nWe note that when\n\nthe background \\(\\mathbf{x}^{\\mathrm{b}}=\\mathbf{x}^{\\mathrm{b}}+\\epsilon^{\\mathrm{b}}\\) is available at some time \\(t_{k},\\) together with\nobservations of the form \\(\\mathbf{y}=\\mathbf{Hx}^{\\mathrm{t}}+\\epsilon^{\\mathrm{o}}\\) that have been acquired at the same time (or over a short enough interval of time when the dynamics can be considered stationary),\nthen the minimization of 3 will produce an estimate of the system state at time \\(t_{k}.\\)"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-4",
    "href": "slides/week-04/05-variational.html#section-4",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "In this case, the analysis is called “three-dimensional variational analysis” and is abbreviated by 3D-Var.\nBorrowing from control theory see Asch (2022) the optimal gain can be shown to take the form \\[\\mathbf{K}=\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R})^{-1},\\] where \\(\\mathbf{B}\\) and \\(\\mathbf{R}\\) are the covariance matrices.\nWe obtain the analyzed state, \\[\\mathbf{x}^{\\mathrm{a}}=\\mathbf{x}^{\\mathrm{b}}+\\mathbf{K}(\\mathbf{y}-\\mathbf{H}(\\mathbf{x}^{\\mathrm{b}})).\\]\nThis is the state that minimizes the 3D-Var cost function.\nWe can verify this by taking the gradient, term by term, of the cost function Equation 3 and equating to zero, \\[\\nabla J(\\mathbf{x}^{\\mathrm{a}})=\\mathbf{B}^{-1}\\left(\\mathbf{x}^{\\mathrm{a}}-\\mathbf{x}^{\\mathrm{b}}\\right)-\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{a}}\\right)=0,\\label{eq:gradJ3DVar}\\]"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-5",
    "href": "slides/week-04/05-variational.html#section-5",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "where \\[\\mathbf{x}^{\\mathrm{a}}=\\mathop{\\mathrm{arg\\,min}}J(\\mathbf{x}).\\]\nSolving the equation, we find \\[\\begin{aligned}\n    \\mathbf{B}^{-1}\\left(\\mathbf{x}^{\\mathrm{a}}-\\mathbf{x}^{\\mathrm{b}}\\right) & = & \\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{a}}\\right)\\nonumber \\\\\n    \\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)\\mathbf{x}^{\\mathrm{a}}& = & \\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{y}+\\mathbf{B}^{-1}\\mathbf{x}^{\\mathrm{b}}\\nonumber \\\\\n    \\mathbf{x}^{\\mathrm{a}}& = & \\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\left(\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{y}+\\mathbf{B}^{-1}\\mathbf{x}^{\\mathrm{b}}\\right)\\nonumber \\\\\n     & = & \\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\left(\\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)\\mathbf{x}^{\\mathrm{b}}\\right.\\nonumber \\\\\n     & ~ & \\left.-\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\mathbf{x}^{\\mathrm{b}}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{y}\\right)\\nonumber \\\\\n     & = & \\mathbf{x}^{\\mathrm{b}}+\\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{b}}\\right)\\nonumber \\\\\n     & = & \\mathbf{x}^{\\mathrm{b}}+\\mathbf{K}\\left(\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{b}}\\right),\\label{eq:lincontrol}\n    \\end{aligned} \\tag{4}\\]"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-6",
    "href": "slides/week-04/05-variational.html#section-6",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "where we have simply added and subtracted the term \\(\\left(\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)\\mathbf{x}^{\\mathrm{b}}\\) in the third-last line and in the last line we have brought out what are known as the innovation term, \\[\\mathbf{d}=\\mathbf{y}-\\mathbf{H}\\mathbf{x}^{\\mathrm{b}},\\] and the gain matrix, \\[\\mathbf{K}=\\left(\\mathbf{B}^{-1}+\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\mathrm{T}}\\mathbf{R}^{-1}.\\]\nThis matrix can be rewritten as \\[\\mathbf{K}=\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\left(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\right)^{-1} \\tag{5}\\]\nusing a well-known Sherman-Morrison-Woodbury formula of linear algebra that completely avoids the direct computation of the inverse of the matrix \\(\\mathbf{B}.\\)"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-7",
    "href": "slides/week-04/05-variational.html#section-7",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "The linear combination in 4 of a background term plus a multiple of the innovation is a classical result of linear-quadratic (LQ) control theory and shows how nicely DA fits in with and corresponds to (optimal) control theory\nThe form of the gain matrix 5 can be explained quite simply.\n\nThe term \\(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) is the background covariance transformed to the observation space.\nThe “denominator” term \\(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) expresses the sum of observation and background covariances.\nThe “numerator” term \\(\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) takes the ratio of \\(\\mathbf{B}\\) and \\(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathrm{T}}\\) back to the model space.\n\nThis recalls (and is completely analogous to) the variance ratio"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-8",
    "href": "slides/week-04/05-variational.html#section-8",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\frac{\\sigma_{\\mathrm{b}}^{2}}{\\sigma_{b}^{2}+\\sigma_{\\mathrm{o}}^{2}}\\] that appears in the optimal BLUE (Best Linear Unbiased Estimate) that will be derived later in the statistical DA Lecture.\nThis corresponds to the case of a single observation \\(y^{\\mathrm{o}}=x^{\\mathrm{o}}\\) of a quantity \\(x,\\) \\[\\begin{aligned}\n    x^{\\mathrm{a}} & = & x^{\\mathrm{b}}+\\frac{\\sigma_{\\mathrm{b}}^{2}}{\\sigma_{\\mathrm{b}}^{2}+\\sigma_{\\mathrm{o}}^{2}}(x^{\\mathrm{o}}-x^{\\mathrm{b}})\\\\\n     & = & x^{\\mathrm{b}}+\\frac{1}{1+\\alpha}(x^{\\mathrm{o}}-x^{\\mathrm{b}}),\n    \\end{aligned}\\]\nwhere \\[\\alpha=\\frac{\\sigma_{\\mathrm{o}}^{2}}{\\sigma_{\\mathrm{b}}^{2}}.\\]"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-9",
    "href": "slides/week-04/05-variational.html#section-9",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "In other words, the best way to estimate the state is to take a weighted average of the background (or prior) and the observations of the state. And the best weight is the ratio of the mean squared errors (variances).\nThe statistical viewpoint is thus perfectly reproduced in the 3D-Var framework."
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-da-4d-var",
    "href": "slides/week-04/05-variational.html#variational-da-4d-var",
    "title": "Variational Data Assimilation",
    "section": "Variational DA — 4D Var",
    "text": "Variational DA — 4D Var\nA more realistic, but complicated situation arises when one wants to assimilate observations that are acquired over a time interval, during which the system dynamics (flow, for example) cannot be neglected.\nSuppose that the measurements are available at a succession of instants, \\(t_{k},\\) \\(k=0,1,\\ldots,K\\) and are of the form \\[\\mathbf{y}_{k}=\\mathbf{H}_{k}\\mathbf{x}_{k}+\\boldsymbol{\\epsilon}^{\\mathrm{o}}_{k}, \\tag{6}\\] where\n\n\\(\\mathbf{H}_{k}\\) is a linear observation operator and\n\\(\\boldsymbol{\\epsilon}^{\\mathrm{o}}_{k}\\) is the observation error with covariance matrix \\(\\mathbf{R}_{k},\\)\nand suppose that these observation errors are uncorrelated in time."
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-11",
    "href": "slides/week-04/05-variational.html#section-11",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    J(\\mathbf{x}_{0}) & =\\frac{1}{2}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}^{\\mathrm{b}}_{0}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)\\\\\n     & +\\frac{1}{2}\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right).\n    \\end{aligned} \\tag{8}\\]\nThe minimization of \\(J(\\mathbf{x}_{0})\\) will provide the initial condition of the model that fits the data most closely.\nThis analysis is called “strong constraint four-dimensional variational assimilation,” abbreviated as strong constraint 4D-Var. The term “strong constraint” implies that the model found by the state equation 7 must be exactly satisfied by the sequence of estimated state vectors.\nIn the presence of model uncertainty, the state equation becomes \\[\\mathbf{x}_{k+1}^{\\mathrm{t}}=\\mathbf{M}_{k+1}\\mathbf{x}_{k}^{\\mathrm{t}}+\\boldsymbol{\\eta}_{k+1},\\label{eq:mod_uncert}\\] where"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-12",
    "href": "slides/week-04/05-variational.html#section-12",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "the model noise \\(\\boldsymbol{\\eta}_{k}\\) has covariance matrix \\(\\mathbf{Q}_{k},\\)\nwhich we suppose to be uncorrelated in time and uncorrelated with the background and observation errors.\n\nThe objective function for the best, linear unbiased estimator (BLUE) for the sequence of states \\[\\left\\{ \\mathbf{x}_{k},\\,k=0,1,\\ldots,K\\right\\}\\] is of the form \\[\\begin{aligned}\n    J(\\mathbf{x}_{0},\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{K})=\\frac{1}{2}\\left(\\mathbf{x}_{0}-\\mathbf{x}_{0}^{\\mathrm{b}}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}_{0}^{\\mathrm{b}}\\right)\\nonumber \\\\\n    +\\frac{1}{2}\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}\\mathbf{x}_{k}-\\mathbf{y}_{k}\\right)\\nonumber \\\\\n    +\\frac{1}{2}\\sum_{k=0}^{K-1}\\left(\\mathbf{x}_{k+1}-\\mathbf{M}_{\\mathit{k}+1}\\mathbf{x}_{k}\\right)^{\\mathrm{T}}\\mathbf{Q}_{k+1}^{-1}\\left(\\mathbf{x}_{k+1}-\\mathbf{M}_{\\mathit{k}+1}\\mathbf{x}_{k}\\right).\n    \\end{aligned} \\tag{9}\\]"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-13",
    "href": "slides/week-04/05-variational.html#section-13",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "This objective function has become a function of the complete sequence of states \\[\\left\\{ \\mathbf{x}_{k},\\,k=0,1,\\ldots,K\\right\\} ,\\] and its minimization is known as “weak constraint four-dimensional variational assimilation,” abbreviated as weak constraint 4D-Var.\nEquations 8 and 9, with an appropriate reformulation of the state and observation spaces, are special cases of the BLUE objective function.\nAll the above forms of variational assimilation, as defined by Equations 3, 8 and 9, have been used for real-world data assimilation, in particular in meteorology and oceanography.\nHowever, these methods are directly applicable to a vast array of other domains, among which we can cite\n\ngeophysics and environmental sciences,\nseismology,"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-14",
    "href": "slides/week-04/05-variational.html#section-14",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "atmospheric chemistry, and terrestrial magnetism.\nMany other examples exist.\n\nWe remark that in real-world practice, variational assimilation is performed on nonlinear models. If the extent of the nonlinearity is sufficiently small (in some sense), then variational assimilation, even if it does not solve the correct estimation problem, will still produce useful results."
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-da4d-var-implementation",
    "href": "slides/week-04/05-variational.html#variational-da4d-var-implementation",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—4D Var — implementation",
    "text": "Variational DA—4D Var — implementation\nNow, our problem reduces to\n\nquantifying the covariance matrices and then, of course,\ncomputing the analyzed state.\n\nThe quantification of the covariance matrices must result from extensive data studies, or the use of a Kalman filter approach see below.\nThe computation of the analyzed state will be described next this will not be done directly, but rather by an adjoint approach for minimizing the cost functions.\nThere is of course the inverse of \\(\\mathbf{B}\\) or \\(\\mathbf{P}^{\\mathrm{b}}\\) to compute, but we remark that there appear only matrix-vector products of \\(\\mathbf{B}^{-1}\\) and \\(\\left(\\mathbf{P}^{\\mathrm{b}}\\right)^{-1}\\) and we can thus define operators (or routines) that compute these efficiently without the need for large storage capacities."
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-da4d-var-adjoint",
    "href": "slides/week-04/05-variational.html#variational-da4d-var-adjoint",
    "title": "Variational Data Assimilation",
    "section": "Variational DA—4D Var — adjoint",
    "text": "Variational DA—4D Var — adjoint\nWe explain the adjoint approach in the case of strong constraint 4D-Var, taking into account a completely general nonlinear setting for the model and for the observation operators.\nLet \\(\\mathbf{M}_{k}\\) and \\(\\mathbf{H}_{k}\\) be the nonlinear model and observation operators respectively.\nWe reformulate 7 and 8 in terms of the nonlinear operators as \\[\\begin{aligned}\n    J(\\mathbf{x}_{0}) & = & \\frac{1}{2}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)\\nonumber \\\\\n     &  & +\\frac{1}{2}\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right),\n    \\end{aligned} \\tag{10}\\]\nwith the dynamics \\[\\mathbf{x}_{k+1}=\\mathbf{M}_{k+1}\\left(\\mathbf{x}_{k}\\right),\\quad k=0,1,\\ldots,K-1. \\tag{11}\\]"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-15",
    "href": "slides/week-04/05-variational.html#section-15",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "The minimization problem requires that we now compute the gradient of \\(J\\) with respect to \\(\\mathbf{x}_{0}.\\)\nThe gradient is determined from the property that, for a given perturbation \\(\\delta\\mathbf{x}_{0}\\) of \\(\\mathbf{x}_{0},\\) the corresponding first-order variation of \\(J\\) is \\[\\delta J=\\left(\\nabla_{\\mathbf{x}_{0}}J\\right)^{\\mathrm{T}}\\delta\\mathbf{x}_{0}. \\tag{12}\\]\nThe perturbation is propagated by the tangent linear equation, \\[\\delta\\mathbf{x}_{k+1}=\\mathbf{M}_{k+1}\\delta\\mathbf{x}_{k},\\quad k=0,1,\\ldots,K-1, \\tag{13}\\] obtained by differentiation of the state equation 11, where \\(\\mathbf{M}_{k+1}\\) is the Jacobian matrix (of first-order partial derivatives) of \\(\\mathbf{x}_{k+1}\\) with respect to \\(\\mathbf{x}_{k}.\\)\nThe first-order variation of the cost function is obtained similarly by differentiation of 10,"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-16",
    "href": "slides/week-04/05-variational.html#section-16",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\delta J & =\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}^{\\mathrm{b}}_{0}\\right)^{-1}\\delta\\mathbf{x}_{0}\\label{eq:4DvarVarObj}\\\\\n     & +\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\mathrm{H}_{k}\\delta\\mathbf{x}_{k},\n    \\end{aligned} \\tag{14}\\] where \\(\\mathrm{H}_{k}\\) is the Jacobian of \\(\\mathbf{H}_{k}\\) and \\(\\delta\\mathbf{x}_{k}\\) is defined by 13.\n\nThis variation is a compound function of \\(\\delta\\mathbf{x}_{0}\\) that depends on all the \\(\\delta\\mathbf{x}_{k}\\)’s.\nBut if we can obtain a direct dependence on \\(\\delta\\mathbf{x}_{0}\\) in the form of 12, eliminating the explicit dependence on \\(\\delta\\mathbf{x}_{k},\\) then we will (as in the previously seen examples) arrive at an explicit expression for the gradient \\(\\nabla_{\\mathbf{x}_{0}}J\\) of our cost function \\(J.\\)\nThis will be done, as we have done before, by introducing an adjoint state and requiring that it satisfy certain conditions namely, the adjoint equation. Let us now proceed with this program.\n\nWe begin by defining, for \\(k=0,1,\\ldots,K,\\) the adjoint state vectors \\(\\mathbf{p}_{k}\\) that belong to the dual of the state space."
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-17",
    "href": "slides/week-04/05-variational.html#section-17",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "Now we take the null products (according to the tangent state equation 13), \\[\\mathbf{p}_{k}^{\\mathrm{T}}\\left(\\delta\\ \\mathbf{x}_{k}-\\mathbf{M}_{k}\\delta\\mathbf{x}_{k-1}\\right),\\] and subtract them from the right-hand side of the cost function variation 14, \\[\\begin{aligned}\n    \\delta J & =\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)^{\\mathrm{T}}\\left(\\mathbf{P}^{\\mathrm{b}}_{0}\\right)^{-1}\\delta\\\\\n     & \\mathbf{x}_{0}+\\sum_{k=0}^{K}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\mathrm{H}_{k}\\delta\\mathbf{x}_{k}\\\\\n     & -\\sum_{k=0}^{K}\\mathbf{p}_{k}^{\\mathrm{T}}\\left(\\delta\\mathbf{x}_{k}-\\mathbf{M}_{k}\\delta\\mathbf{x}_{k-1}\\right).\n    \\end{aligned}\\]\nRearranging the matrix products, using the symmetry of \\(\\mathbf{R}_{k}\\) and regrouping terms in \\(\\delta\\mathbf{x}_{\\cdot},\\) we obtain,"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-18",
    "href": "slides/week-04/05-variational.html#section-18",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\delta J & = & \\left[\\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}^{\\mathrm{b}}_{0}\\right)+\\mathrm{H}_{0}^{\\mathrm{T}}\\mathbf{R}_{0}^{-1}\\left(\\mathbf{H}_{0}(\\mathbf{x}_{0})-\\mathbf{y}_{0}\\right)+\\mathbf{M}_{0}^{\\mathrm{T}}\\mathbf{p}_{1}\\right]\\delta\\mathbf{x}_{0}\\\\\n     &  & +\\left[\\sum_{k=1}^{K-1}\\mathrm{H}_{k}^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)-\\mathbf{p}_{k}+\\mathbf{M}_{k}^{\\mathrm{T}}\\mathbf{p}_{k+1}\\right]\\delta\\mathbf{x}_{k}\\\\\n     &  & +\\left[\\mathrm{H}_{K}^{\\mathrm{T}}\\mathbf{R}_{K}^{-1}\\left(\\mathbf{H}_{K}(\\mathbf{x}_{K})-\\mathbf{y}_{K}\\right)-\\mathbf{p}_{K}\\right]\\delta\\mathbf{x}_{k}.\n    \\end{aligned}\\]\nNotice that this expression is valid for any choice of the adjoint states \\(\\mathbf{p}_{k}\\) and, in order to “kill” all \\(\\delta\\mathbf{x}_{k}\\) terms, except \\(\\delta\\mathbf{x}_{0},\\) we must simply impose that, \\[\\begin{aligned}\n    \\mathbf{p}_{K} & = & \\mathrm{H}_{K}^{\\mathrm{T}}\\mathbf{R}_{K}^{-1}\\left(\\mathbf{H}_{K}(\\mathbf{x}_{K})-\\mathbf{y}_{K}\\right),\\label{eq:4DvarAdj1}\\\\\n    \\mathbf{p}_{k} & = & \\mathrm{H}_{k}^{\\mathrm{T}}\\mathbf{R}_{k}^{-1}\\left(\\mathbf{H}_{k}(\\mathbf{x}_{k})-\\mathbf{y}_{k}\\right)+\\mathbf{M}_{k}^{\\mathrm{T}}\\mathbf{p}_{k+1},\\quad k=K-1,\\ldots,1,\\label{eq:4DvarAdj2}\\\\\n    \\mathbf{p}_{0} & = & \\left(\\mathbf{P}_{0}^{\\mathrm{b}}\\right)^{-1}\\left(\\mathbf{x}_{0}-\\mathbf{x}_{0}^{\\mathrm{b}}\\right)+\\mathrm{H}_{0}^{\\mathrm{T}}\\mathbf{R}_{0}^{-1}\\left(\\mathbf{H}_{0}(\\mathbf{x}_{0})-\\mathbf{y}_{0}\\right)+\\mathbf{M}_{0}^{\\mathrm{T}}\\mathbf{p}_{1}.\n    \\end{aligned} \\tag{15}\\]\nWe recognize the backward, adjoint equation for \\(\\mathbf{p}_{k}\\) and the only term remaining in the variation of \\(J\\) is then"
  },
  {
    "objectID": "slides/week-04/05-variational.html#section-19",
    "href": "slides/week-04/05-variational.html#section-19",
    "title": "Variational Data Assimilation",
    "section": "",
    "text": "\\[\\delta J=\\mathbf{p}_{0}^{\\mathrm{T}}\\delta\\mathbf{x}_{0},\\] so that \\(\\mathbf{p}_{0}\\) is the sought for gradient, \\(\\nabla_{\\mathbf{x}_{0}}J\\), of the objective function with respect to the initial condition \\(\\mathbf{x}_{0}\\) according to 12.\nThe system of equations 15 is the adjoint of the tangent linear equation 13.\nThe term adjoint here corresponds to the transposes of the matrices \\(\\mathrm{H}_{k}^{\\mathrm{T}}\\) and \\(\\mathbf{M}_{k}^{\\mathrm{T}}\\) that, as we have seen before, are the finite-dimensional analogues of an adjoint operator."
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-da---4d-var-algorithm",
    "href": "slides/week-04/05-variational.html#variational-da---4d-var-algorithm",
    "title": "Variational Data Assimilation",
    "section": "Variational DA - 4D Var Algorithm",
    "text": "Variational DA - 4D Var Algorithm\nWe can now propose the “usual” algorithm for solving the optimization problem by the adjoint approach:\n\nFor a given initial condition \\(\\mathbf{x}_{0},\\) integrate forwards the (nonlinear) state equation 11 and store the solutions \\(\\mathbf{x}_{k}\\) (or use some sort of checkpointing).\nFrom the final condition, \\(\\mathbf{p}_K\\) (cf. 15), integrate backwards in time the adjoint equations for \\(\\mathbf{p}_k\\) (cf. 15)\nCompute directly the required gradient \\(\\mathbf{p}_0\\) (cf. 15).\nUse this gradient in an iterative optimization algorithm to find a (local) minimum.\n\nThe above description for the solution of the 4D-Var data assimilation problem clearly covers the case of 3D-Var, where we seek to minimize 3. In this case, we only need the transpose Jacobian \\(\\mathrm{H}^{\\mathrm{T}}\\) of the observation operator."
  },
  {
    "objectID": "slides/week-04/05-variational.html#variational-da---roles-of-r-and-b",
    "href": "slides/week-04/05-variational.html#variational-da---roles-of-r-and-b",
    "title": "Variational Data Assimilation",
    "section": "Variational DA - roles of R and B",
    "text": "Variational DA - roles of R and B\nThe relative magnitudes of the errors due to measurement and background provide us with important information as to how much “weight” to give to the different information sources when solving the assimilation problem.\nFor example, if background errors are larger than observation errors, then the analyzed state, solution to the DA problem, should be closer to the observations than to the background and vice-versa.\nThe background error covariance matrix, \\(\\mathbf{B},\\) plays an important role in DA. This is illustrated in the following examples."
  },
  {
    "objectID": "slides/week-03/04-AD.html#outline",
    "href": "slides/week-03/04-AD.html#outline",
    "title": "Differential Programming",
    "section": "Outline",
    "text": "Outline\n\nAutomatic differentiation for scientific machine learning:\n\nDifferentiable programming with autograd and PyTorch and Zygote.jl in Flux.jl.\nGradients, adjoints, backpropagation and inverse problems.\nNeural networks for scientific machine learning.\nPhysics-informed neural networks.\nThe use of automatic differentiation in scientific machine learning.\nThe challenges of applying automatic differentiation to scientific applications.\n\n\nDifferential programming is a technique for automatically computing the derivatives of functions (code).\nThis can be done using a variety of techniques, including:"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section",
    "href": "slides/week-03/04-AD.html#section",
    "title": "Differential Programming",
    "section": "",
    "text": "Symbolic differentiation: This involves using symbolic mathematics to represent the function and its derivatives. This can be a powerful technique, but it can be difficult to use for complex functions.\nNumerical differentiation: This involves using numerical methods to approximate the derivatives of the function. This is a simpler technique than symbolic differentiation, but it is less accurate.\nAutomatic differentiation: This is a technique that combines symbolic and numerical differentiation to automatically compute the derivatives of functions. This is the most powerful technique for differential programming, and it is the most commonly used technique in scientific machine learning.\n\nThe mathematical theory of differential programming is based on the concept of gradients.\n\nThe gradient of a function is a vector that tells you how the function changes as its input changes. In other words, the gradient of a function tells you the direction of steepest ascent or descent."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-1",
    "href": "slides/week-03/04-AD.html#section-1",
    "title": "Differential Programming",
    "section": "",
    "text": "The gradient of a function can be calculated using the gradient descent algorithm. The gradient descent algorithm works by starting at a point and then moving in the direction of the gradient until it reaches a minimum or maximum.\nIn ML, we use stochastic gradient optimization methods\n\nDifferential programming can be used to solve a variety of problems in scientific machine learning, including:\n\nCalculating the gradients of loss functions for machine learning models—this is important for training machine learning models.\nSolving differential equations—this can be used to model the behavior of physical systems. (Caution: this does not scale!!!)\nPerforming optimization—this can be used to find the optimal solution to a problem.\nSolving inverse and data assimilation problems—this is none other than a special case of optimization."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-2",
    "href": "slides/week-03/04-AD.html#section-2",
    "title": "Differential Programming",
    "section": "",
    "text": "OPTIMIZATION"
  },
  {
    "objectID": "slides/week-03/04-AD.html#optimization",
    "href": "slides/week-03/04-AD.html#optimization",
    "title": "Differential Programming",
    "section": "Optimization",
    "text": "Optimization\n\n \n\nOptimization routines typically use local information about a function to iteratively approach a local minimum.\nIn this (rare) case, where we have a convex function, we easily find a global minimum.\nBut in general, global optimization can be very difficult\nWe usually get stuck in local minima!\nThings get MUCH harder in higher spatial dimensions…"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-3",
    "href": "slides/week-03/04-AD.html#section-3",
    "title": "Differential Programming",
    "section": "",
    "text": "DIFFERENTIAL PROGRAMMING"
  },
  {
    "objectID": "slides/week-03/04-AD.html#differential-programming",
    "href": "slides/week-03/04-AD.html#differential-programming",
    "title": "Differential Programming",
    "section": "Differential Programming",
    "text": "Differential Programming\nThere are 3 ways to compute derivatives of functions:\n\nSymbolic differentiation.\nNumerical differentiation.\nAutomatic differentiation.\n\nSee Notebooks for Intro Pytorch and Differential Programming."
  },
  {
    "objectID": "slides/week-03/04-AD.html#symbolic-differentiation",
    "href": "slides/week-03/04-AD.html#symbolic-differentiation",
    "title": "Differential Programming",
    "section": "Symbolic Differentiation",
    "text": "Symbolic Differentiation\nComputes exact, analytical derivatives, in the form of a mathematical expression.\n\nThere is no approximation error.\nOperates recursively by applying simple rules to symbols.\nThere may be no analytical expression for gradients of some functions.\nCan lead to redundant and overly complex expressions.\n\nBased on the sympy package of Python.\n\nOther software: Mathematica, Maple, Sage, etc."
  },
  {
    "objectID": "slides/week-03/04-AD.html#numerical-differentiation",
    "href": "slides/week-03/04-AD.html#numerical-differentiation",
    "title": "Differential Programming",
    "section": "Numerical Differentiation",
    "text": "Numerical Differentiation\n\nDefinition 1 If \\(f\\) is a differentiable function, then \\[f'(x)=\\lim_{h\\rightarrow0}\\frac{f(x+h)-f(x)}{h}\\]\n\nUsing Taylor expansions, and the definition of the derivative, we can obtain finite-difference, numerical approximationsto the derivatives of \\(f,\\) such as \\[f'(x)=\\frac{f(x+h)-f(x)}{h}+\\mathcal{O}(h),\\] \\[f'(x)=\\frac{f(x+h)-f(x-h)}{2h}+\\mathcal{O}(h^{2})\\]\n\nconceptually simple and very easy to code\ncompute gradients of \\(f\\colon\\mathbb{R}^{m}\\rightarrow\\mathbb{R},\\) requires at least \\(\\mathcal{O}(m)\\) function evaluations\nbig numerical errors due to truncation and roundoff."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-4",
    "href": "slides/week-03/04-AD.html#section-4",
    "title": "Differential Programming",
    "section": "",
    "text": "AUTOMATIC DIFFERENTIATION"
  },
  {
    "objectID": "slides/week-03/04-AD.html#automatic-differentiation",
    "href": "slides/week-03/04-AD.html#automatic-differentiation",
    "title": "Differential Programming",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nAutomatic differentiation is an umbrella term for a variety of techniques for efficiently computing accurate derivatives of more or less general programs.\n\nIt is employed by all major neural network frameworks, where a single reverse-mode AD backpass (also known as “backpropagation”) can compute a full gradient.\nNumerical differentiation would either require many forward passes or symbolic differentiation that is simply untenable due to expression explosion.\nThe survey paper (Baydin et al. 2018) provides an excellent review of all the methods and tools available.\n\nMany algorithms in machine learning, computer vision, physical simulation, and other fields require the calculation of gradients and other derivatives.\n\nManual derivation of gradients can be both time-consuming and error-prone.\nAutomatic differentiation comprises a set of techniques to calculate the derivative of a numerical computation expressed as a computer code."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-5",
    "href": "slides/week-03/04-AD.html#section-5",
    "title": "Differential Programming",
    "section": "",
    "text": "These techniques of AD, commonly used for data assimilation in atmospheric sciences and optimal design in computational fluid dynamics, have more recently also been adopted by machine learning researchers.\nThe backpropagation algorithm, used for optimally computing the weights of a neural network, is just a special case of general AD.\nAD can be found in all the major software libraries for ML/DL, such as TensorFlow, PyTorch, JaX, and Julia’s Flux.jl/Zygote.jl.\n\n\nPractitioners across many fields have built a wide set of automatic differentiation tools, using different programming languages, computational primitives, and intermediate compiler representations.\n\nEach of these choices comes with positive and negative trade-offs, in terms of their usability, flexibility, and performance in specific domains."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-6",
    "href": "slides/week-03/04-AD.html#section-6",
    "title": "Differential Programming",
    "section": "",
    "text": "Nevertheless, the availability of such tools should not be neglected, since the potential gain from their use is very large.\nMoreover, the fact that they are already built-in to a large number of ML methods, makes their use quite straightforward.\n\nAD can be readily and extensively used and is thus applicable to many industrial and practical Digital Twin contexts (Asch 2022).\nHowever Digital Twins that require large-scale ML remain challenging.\nWhile substantial efforts are made within the ML communities of PyTorch/Tensorflow, these approaches struggle for large-scale problems that need to\n\nbe frugal with memory use\nexploit parallelism across multiple nodes/GPUs\nintegrate with existing (parallel) CSE applications\n\nWorthwhile to explore Julia’s more integrated approach to HPC Differential Programming (Innes et al. 2019) and SciML (Rackauckas and Nie 2017)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#ad-for-sciml",
    "href": "slides/week-03/04-AD.html#ad-for-sciml",
    "title": "Differential Programming",
    "section": "AD for SciML",
    "text": "AD for SciML\nRecent progress in machine learning (ML) technology has been spectacular.\nAt the heart of these advances is the ability to obtain high-quality solutions to non-convex optimization problems for functions with billions—or even hundreds of billions—of parameters.\nIncredible opportunity for progress in classical applied mathematics problems.\n\nIn particular, the increased proficiency for systematically handling large, non-convex optimization scenarios may help solve some classical problems that have long been a challenge.\nWe now have the chance to make substantial headway on questions that have not yet been formulated or studied because we lacked the tools to solve them."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-7",
    "href": "slides/week-03/04-AD.html#section-7",
    "title": "Differential Programming",
    "section": "",
    "text": "To be clear, we do not wish to oversell the state of the art, however:\n\nAlgorithms that identify the global optimum for non-convex optimization problems do not yet exist.\nThe ML community has instead developed efficient, open source software tools that find candidate solutions.\nThey have created benchmarks to measure solution quality.\nThey have cultivated a culture of competition against these benchmarks."
  },
  {
    "objectID": "slides/week-03/04-AD.html#automatic-differentiationbackprop-autograd-zygote.jl-etc.",
    "href": "slides/week-03/04-AD.html#automatic-differentiationbackprop-autograd-zygote.jl-etc.",
    "title": "Differential Programming",
    "section": "Automatic Differentiation—backprop, autograd, Zygote.jl, etc.",
    "text": "Automatic Differentiation—backprop, autograd, Zygote.jl, etc.\n\nBackprop is a special case of Algorithmic Differentiation (AD).\nAutograd is a particular AD package that us supported w/i Python (as part of Pytorch).\nMost exercises of this course use PyTorch’s AD.\nHaving said that we strongly encourage students to do the exercises in Julia using its extensive AD capabilities (see JuliaDiff), integration in the Julia language, and use of abstractions that allow for\nmixing of hand-derived (adjoint-state) gradients and AD via ChainRules.jl\na single AD interface irrespective of the AD backend through the use of AbstractDifferentiation.jl."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-8",
    "href": "slides/week-03/04-AD.html#section-8",
    "title": "Differential Programming",
    "section": "",
    "text": "Important\n\n\nAD is NOT finite differences, nor symbolic differentiation. Finite differences are too expensive (one forward pass for each discrete point). They induce huge numerical errors (truncation/approximation and roundoff) and are very unstable in the presence of noise.\n\n\n\n\n\n\n\n\n\nNote\n\n\nAD is both efficient—linear in the cost of computing the value—and numerically stable.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe goal of AD is not a formula, but a procedure for computing derivatives."
  },
  {
    "objectID": "slides/week-03/04-AD.html#tools-for-ad",
    "href": "slides/week-03/04-AD.html#tools-for-ad",
    "title": "Differential Programming",
    "section": "Tools for AD",
    "text": "Tools for AD\nNew opportunities that exist because of the widespread, open-source deployment of effective software tools for automatic differentiation.\nWhile the mathematical framework for automatic differentiation was established long ago—dating back at least to the evolution of adjoint-based optimization in optimal control (Asch, Bocquet, and Nodet 2016; Asch 2022)—ML researchers have recently designed efficient software frameworks that natively run on hardware accelerators (GPUs).\n\nThese frameworks have served as a core technology for the ML revolution over the last decade and inspired high-quality software libraries such as\n\nJAX,\nPyTorch and TensorFlow\nJulia’s ML with Flux.jl and AD with Zygote.jl and abstractions with ChainRules.jl and AbstractDifferentiation.jl"
  },
  {
    "objectID": "slides/week-03/04-AD.html#statements",
    "href": "slides/week-03/04-AD.html#statements",
    "title": "Differential Programming",
    "section": "Statements",
    "text": "Statements\nThe technology’s key feature is: the computational cost of computing derivatives of a target loss function is independent of the number of parameters;\n\nthis trait makes it possible for users to implement gradient-based optimization algorithms for functions with staggering numbers of parameters.\n\n\n“Gradient descent can write code better than you, I’m sorry.”\n“Yes, you should understand backprop.”\n“I’ve been using PyTorch a few months now and I’ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.”\n\n\nAndrej Karpathy [~2017] (Tesla AI, OpenAI)\n\n\n\n\n\n\n\nNote\n\n\nTools such as PyTorch and TensorFlow may not scale to 3D problems and are challenging to integrate with physics-based simulations and gradients (via adjoint state)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-9",
    "href": "slides/week-03/04-AD.html#section-9",
    "title": "Differential Programming",
    "section": "",
    "text": "BACKPROPAGATION"
  },
  {
    "objectID": "slides/week-03/04-AD.html#backpropagationoptimization-problem",
    "href": "slides/week-03/04-AD.html#backpropagationoptimization-problem",
    "title": "Differential Programming",
    "section": "Backpropagation—optimization problem",
    "text": "Backpropagation—optimization problem\nWe want to solve a (nonlinear, non-convex) optimization problem, either\n\nfor a dynamic system, \\[\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t}=f(\\mathbf{x};\\mathbf{\\theta}),\\] where \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{\\theta}\\in\\mathbb{R}^{p}\\) with \\(n,p\\gg1.\\)\nor for a machine learning model \\[\\mathbf{y}=f(\\mathbf{x};\\mathbf{w}),\\] where \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{w}\\in\\mathbb{R}^{p}\\) with \\(n,p\\gg1.\\)\n\nTo find the minimum/optimum, we want to minimize an appropriate cost/loss function \\[J(\\mathbf{x},\\mathbf{\\theta}),\\quad\\mathcal{L}(\\mathbf{w},\\mathbf{\\theta})\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-10",
    "href": "slides/week-03/04-AD.html#section-10",
    "title": "Differential Programming",
    "section": "",
    "text": "usually some error norm, and then (usually) compute its average\nThe best/fastest way to solve this optimization problem, is to use gradients and gradient-based methods.\n\nDefinition 2 Backpropagation is an algorithm for computing gradients.\nBackpropagation is an instance of reverse-mode automatic differentiation\n\nvery broadly applicable to machine learning, data assimilation and inverse problems in general\nit is “just” a clever and efficient use of the Chain Rule for derivatives"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-11",
    "href": "slides/week-03/04-AD.html#section-11",
    "title": "Differential Programming",
    "section": "",
    "text": "We can prove mathematically the following equivalences:\n\nflowchart TD\n  A[Backpropagation]&lt;--&gt;B[Reverse-mode automatic differentiation]&lt;--&gt;C[Discrete adjoint-state method]\n\n\n\n\nflowchart TD\n  A[Backpropagation]&lt;--&gt;B[Reverse-mode automatic differentiation]&lt;--&gt;C[Discrete adjoint-state method]\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nRecall: the adjoint-state method is the theoretical basis for Data Assimilation, as well as many other inverse problems — Lecture on Adjoint Methods."
  },
  {
    "objectID": "slides/week-03/04-AD.html#chain-rule",
    "href": "slides/week-03/04-AD.html#chain-rule",
    "title": "Differential Programming",
    "section": "Chain Rule",
    "text": "Chain Rule\nWe want to compute the cost/loss function gradient, which is usually the average over the training samples of the loss gradient, \\[\\nabla_{w}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial w},\\quad\\nabla_{\\theta}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial\\theta},\\] or, in general \\[\\nabla_{z}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial z},\\] where \\(z=w\\) or \\(z=\\theta,\\) etc.\nRecall: if \\(f(x)\\) and \\(x(t)\\) are univariate (differentiable) functions, then \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x(t))=\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-12",
    "href": "slides/week-03/04-AD.html#section-12",
    "title": "Differential Programming",
    "section": "",
    "text": "and this can be easily generalized to the multivariate case, such as \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x(t),y(t))=\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\frac{\\mathrm{d}x}{\\mathrm{d}t}+\\frac{\\mathrm{d}f}{\\mathrm{d}y}\\frac{\\mathrm{d}y}{\\mathrm{d}t}\\]\n\nExample 1 Consider \\[f(x,y,z)=(x+y)z\\]\nDecompose \\(f\\) into simple differentiable elements \\[q(x,y)=x+y,\\] then \\[f=qz\\]\n\n\n\n\n\n\n\nNote\n\n\nEach element has an analytical (exact/known) derivative—eg. sums, products, sines, cosines, min, max, exp, log, etc."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-13",
    "href": "slides/week-03/04-AD.html#section-13",
    "title": "Differential Programming",
    "section": "",
    "text": "Compute the gradient of \\(f\\) with respect to its three variables, using the chain rule\n\nwe begin with \\[\\frac{\\partial f}{\\partial q}=z,\\quad\\frac{\\partial f}{\\partial z}=q\\] and \\[\\frac{\\partial q}{\\partial x}=1,\\quad\\frac{\\partial q}{\\partial y}=1\\]\nthen the chain rule gives the terms of the gradient, \\[\\begin{aligned}\n    \\frac{\\partial f}{\\partial x} & =\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}=z\\cdot1\\\\\n    \\frac{\\partial f}{\\partial y} & =\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}=z\\cdot1\\\\\n    \\frac{\\partial f}{\\partial z} & =q\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-14",
    "href": "slides/week-03/04-AD.html#section-14",
    "title": "Differential Programming",
    "section": "",
    "text": "# set some inputs\nx = -2; y = 5; z = -4\n# perform the forward pass\nq = x + y # q becomes 3\nf = q * z # f becomes -12\n# perform the backward pass (backpropagation)\n# in reverse order:\n# first backprop through f = q * z \ndfdz = q  # df/dz = q, so gradient on z becomes 3 \ndfdq = z  # df/dq = z, so gradient on q becomes -4 \ndqdx = 1.0\ndqdy = 1.0\n\n# now backprop through q = x + y\ndfdx = dfdq * dqdx  # The * here is the chain rule!\ndfdy = dfdq * dqdy\n\nWe obtain the gradient in the variables [``dfdx, dfdy, dfdz``] that give us the sensitivity of the function f to the variables x, y and z.\nIt’s all done with graphs... DAGs, in fact"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-15",
    "href": "slides/week-03/04-AD.html#section-15",
    "title": "Differential Programming",
    "section": "",
    "text": "The above computation can be visualized with a circuit diagram:\n\n\n\nthe forward pass, computes values from inputs to outputs\nthe backward pass then performs backpropagation, starting at the end and recursively applying the chain rule to compute the gradients all the way to the inputs of the circuit.\n\nThe gradients can be thought of as flowing backwards through the circuit."
  },
  {
    "objectID": "slides/week-03/04-AD.html#forward-vs-reverse-mode",
    "href": "slides/week-03/04-AD.html#forward-vs-reverse-mode",
    "title": "Differential Programming",
    "section": "Forward vs Reverse Mode",
    "text": "Forward vs Reverse Mode\nForward mode is used for\n\nsolving nonlinear equations\nsensitivity analysis\nuncertainty propagation/quantification \\[f(x+\\Delta x)\\approx f(x)+f'(x)\\Delta x\\]\n\nReverse mode is used for\n\nmachine/deep learning\noptimization"
  },
  {
    "objectID": "slides/week-03/04-AD.html#backprop---ml-example",
    "href": "slides/week-03/04-AD.html#backprop---ml-example",
    "title": "Differential Programming",
    "section": "Backprop - ML example",
    "text": "Backprop - ML example\nFor a univariate, logistic least-squares problem, we have:\n\nlinear model/function of \\(x\\): \\(z=wx+b\\)\nnonlinear activation: \\(y=\\sigma(x)\\)\nquadratic loss: \\(\\mathcal{L}=(1/2)(y-t)^{2},\\) where \\(t\\) is the target/observed value\n\nObjective: find the values of the parameters/weights, \\(w\\) and \\(b,\\) that minimize the loss \\(\\mathcal{L}\\)\n\nto do this, we will use the gradient of \\(\\mathcal{L}\\) with respect to the parameters/weights, \\(w\\) and \\(b,\\) \\[\\nabla_{w}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial w},\\quad\\nabla_{b}\\mathcal{L}=\\frac{\\partial\\mathcal{L}}{\\partial b}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#brute-force",
    "href": "slides/week-03/04-AD.html#brute-force",
    "title": "Differential Programming",
    "section": "Brute force",
    "text": "Brute force\nCalculus approach:\n\n\n\nIt’s a mess... too many computations, too complex to program!"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-16",
    "href": "slides/week-03/04-AD.html#section-16",
    "title": "Differential Programming",
    "section": "",
    "text": "Structured approach: \\[\\begin{aligned}\n     & \\mathrm{compute\\ loss} &  & \\mathrm{compute\\ derivatives}\\\\\n     & \\mathrm{{\\color{blue}forwards}} &  & \\mathrm{{\\color{red}backwards}}\\\\\n    z & =wx+b & \\frac{\\partial\\mathcal{L}}{\\partial y} & =y-t\\\\\n    y & =\\sigma(z) & \\frac{\\partial\\mathcal{L}}{\\partial z} & =\\frac{\\partial\\mathcal{L}}{\\partial y}\\frac{\\partial y}{\\partial z}=\\frac{\\partial\\mathcal{L}}{\\partial y}\\sigma'(z)\\\\\n    \\mathcal{L} & =\\frac{1}{2}(y-t)^{2} & \\frac{\\partial\\mathcal{L}}{\\partial w} & =\\frac{\\partial\\mathcal{L}}{\\partial z}\\frac{\\partial z}{\\partial w}=\\frac{\\partial\\mathcal{L}}{\\partial z}x\\\\\n     &  & \\frac{\\partial\\mathcal{L}}{\\partial b} & =\\frac{\\partial\\mathcal{L}}{\\partial z}\\frac{\\partial z}{\\partial b}=\\frac{\\partial\\mathcal{L}}{\\partial z}\\cdot1\n    \\end{aligned}\\]\n\ncan easily be written as a computational graph with\nnodes = inputs and computed quantities\nedges = nodes computed directly as functions of other nodes"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-17",
    "href": "slides/week-03/04-AD.html#section-17",
    "title": "Differential Programming",
    "section": "",
    "text": "Loss is computed in the forward pass\nGradient is computed in the backward pass\n\nthe derivatives of \\(y\\) and \\(z\\) are exact/known\nthe derivatives of \\(\\mathcal{L}\\) are computed, starting from the end\nthe gradients wrt to the parameters are readily obtained by backpropagation using the chain rule!"
  },
  {
    "objectID": "slides/week-03/04-AD.html#full-backprop-algorithm",
    "href": "slides/week-03/04-AD.html#full-backprop-algorithm",
    "title": "Differential Programming",
    "section": "Full Backprop Algorithm",
    "text": "Full Backprop Algorithm\n\n\n\nwhere \\(\\bar{v}_{i}\\) denotes the derivatives of the loss function with respect to \\(v_{i},\\) \\[\\frac{\\partial\\mathcal{L}}{\\partial v_{i}}\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-18",
    "href": "slides/week-03/04-AD.html#section-18",
    "title": "Differential Programming",
    "section": "",
    "text": "Computational cost of backprop: approximately two forward passes, and hence linear in the number of unknowns\n\nBackprop is used to train the overwhelming majority of neural nets today.\nOptimization algorithms, in addition to gradient descent (e.g. second-order methods) use backprop to compute the gradients.\nBackprop can thus be used in SciML, and in particular for Digital Twins (direct and inverse problems), wherever derivatives and/or gradients need to be computed."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-19",
    "href": "slides/week-03/04-AD.html#section-19",
    "title": "Differential Programming",
    "section": "",
    "text": "AUTOGRAD"
  },
  {
    "objectID": "slides/week-03/04-AD.html#autograd",
    "href": "slides/week-03/04-AD.html#autograd",
    "title": "Differential Programming",
    "section": "Autograd",
    "text": "Autograd\nAutograd can automatically differentiate native Python and Numpy code.\n\nIt can handle a large subset of Python’s features, including loops, ifs, recursion and closures.\nIt can even take derivatives of derivatives of derivatives, etc.\nIt supports reverse-mode differentiation (a.k.a. backpropagation), which means it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments, as well as forward-mode differentiation (to compute sensitivities), and the two can be composed arbitrarily.\nThe main intended application of Autograd is gradient-based optimization.\n\nAfter a function is evaluated, Autograd has a graph specifying all operations that were performed on the inputs with respect to which we want to differentiate.\n\nThis is the computational graph of the function evaluation."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-20",
    "href": "slides/week-03/04-AD.html#section-20",
    "title": "Differential Programming",
    "section": "",
    "text": "To compute the derivative, we simply apply the basic rules of (analytical) differentiation to each node in the graph.\n\nReverse mode differentiation\n\nGiven a function made up of several nested function calls, there are several ways to compute its derivative.\nFor example, given \\[L(x)=F(G(H(x))),\\] the chain rule says that its gradient is \\[\\mathrm{d}L/\\mathrm{d}x=\\mathrm{d}F/\\mathrm{d}G*\\mathrm{d}G/\\mathrm{d}H*\\mathrm{d}H/\\mathrm{d}x.\\]"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-21",
    "href": "slides/week-03/04-AD.html#section-21",
    "title": "Differential Programming",
    "section": "",
    "text": "If we evaluate this product from right-to-left: \\[(\\mathrm{d}F/\\mathrm{d}G*(\\mathrm{d}G/\\mathrm{d}H*\\mathrm{d}H/\\mathrm{d}x)),\\] the same order as the computations themselves were performed, this is called forward-mode differentiation.\nIf we evaluate this product from left-to-right: \\[((\\mathrm{d}F/\\mathrm{d}G*\\mathrm{d}G/\\mathrm{d}H)*\\mathrm{d}H/\\mathrm{d}x),\\] the reverse order as the computations themselves were performed, this is called reverse-mode differentiation.\n\nCompared to finite differences or forward-mode, reverse-mode differentiation is by far the more practical method for differentiating functions that take in a (very) large vector and output a single number.\nIn the machine learning community, reverse-mode differentiation is known as ‘backpropagation’, since the gradients propagate backwards through the function (as seen above)."
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-22",
    "href": "slides/week-03/04-AD.html#section-22",
    "title": "Differential Programming",
    "section": "",
    "text": "It’s particularly nice since you don’t need to instantiate the intermediate Jacobian matrices explicitly, and instead only rely on applying a sequence of matrix-free vector-Jacobian productfunctions (VJPs).\nBecause Autograd supports higher derivatives as well, Hessian-vector products (a form of second-derivative) are also available and efficient to compute.\n\n\n\n\n\n\nImportant\n\n\nAutograd is now being superseded by JAX."
  },
  {
    "objectID": "slides/week-03/04-AD.html#pytorch-versus-julia",
    "href": "slides/week-03/04-AD.html#pytorch-versus-julia",
    "title": "Differential Programming",
    "section": "PyTorch versus Julia",
    "text": "PyTorch versus Julia\nWhile extremely easy to use and featured, PyTorch & Jax are walled gardens\n\nmaking it difficult integrate w/ CSE software\ngo off the beaten path\n\nIn response to the prompt “Can you list in Markdown table form pros and cons of PyTorch and Julia AD systems” ChatGTP4.0 generated the following adapted table\n\n\n\n\n\n\n\n\n\nFeature\nPyTorch\nJulia AD\n\n\n\n\nLanguage\nPython-based, widely used in ML community\nJulia, known for high performance and mathematical syntax\n\n\nPerformance\nFast, but can be limited by Python’s speed\nGenerally faster, benefits from Julia’s performance\n\n\nEase of Use\nUser-friendly, extensive documentation and community support\nSteeper learning curve, but elegant for mathematical operations"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-23",
    "href": "slides/week-03/04-AD.html#section-23",
    "title": "Differential Programming",
    "section": "",
    "text": "Feature\nPyTorch\nJulia AD\n\n\n\n\nDynamic Computation Graph\nYes, allows flexibility\nYes, with support for advanced features\n\n\nEcosystem\nExtensive, with many libraries and tools\nGrowing, with packages for scientific computing\n\n\nCommunity Support\nLarge community, well-established in industry and academia\nSmaller but growing community, strong in scientific computing\n\n\nIntegration\nEasy integration with Python libraries and tools\nGood integration w/i Julia ecosystem\n\n\nDebugging\nGood debugging tools, but can be tricky due to dynamic nature\nGood, with benefits from Julia’s compiler & type system\n\n\nParallel & GPU \nExcellent support\nExcellent, potentially faster due to Julia’s design\n\n\nMaturity\nMature, widely adopted\nLess but rapidly evolving"
  },
  {
    "objectID": "slides/week-03/04-AD.html#section-24",
    "href": "slides/week-03/04-AD.html#section-24",
    "title": "Differential Programming",
    "section": "",
    "text": "Important\n\n\nThis table highlights key aspects but may not cover all nuances. Both systems are continuously evolving, so it’s always good to check the latest developments and community feedback when making a choice.\n\n\n\n\nFor those of you interested in Julia checkout the lecture Forward- & Reverse-Mode AD by Adrian Hill"
  },
  {
    "objectID": "slides/week-02/02-basics.html#text-book",
    "href": "slides/week-02/02-basics.html#text-book",
    "title": "Basics",
    "section": "Text book",
    "text": "Text book"
  },
  {
    "objectID": "slides/week-02/02-basics.html#section",
    "href": "slides/week-02/02-basics.html#section",
    "title": "Basics",
    "section": "",
    "text": "INTRODUCTION"
  },
  {
    "objectID": "slides/week-02/02-basics.html#what-is-data-assimilation",
    "href": "slides/week-02/02-basics.html#what-is-data-assimilation",
    "title": "Basics",
    "section": "What is data assimilation",
    "text": "What is data assimilation\nSimplest view: a method of combining observations with model output.\nWhy do we need data assimilation? Why not just use the observations? (cf. Regression)\nWe want to predict the future!\n\nFor that we need models.\nBut when models are not constrained periodically by reality, they are of little value.\nTherefore, it is necessary to fit the model state as closely as possible to the observations, before a prediction is made."
  },
  {
    "objectID": "slides/week-02/02-basics.html#data-assimilation-methods",
    "href": "slides/week-02/02-basics.html#data-assimilation-methods",
    "title": "Basics",
    "section": "Data assimilation methods",
    "text": "Data assimilation methods\nThere are two major classes of methods:\n\nVariational methods where we explicitly minimize a cost function using optimization methods.\nStatistical methods where we compute the best linear unbiased estimate (BLUE) by algebraic computations using the Kalman filter.\n\nThey provide the same result in the linear case, which is the only context where their optimality can be rigorously proved.\nThey both have difficulties in dealing with non-linearities and large problems.\nThe error statistics that are required by both, are in general poorly known."
  },
  {
    "objectID": "slides/week-02/02-basics.html#introduction-approaches",
    "href": "slides/week-02/02-basics.html#introduction-approaches",
    "title": "Basics",
    "section": "Introduction: approaches",
    "text": "Introduction: approaches\nDA is an approach for solving a specific class of inverse, or parameter estimation problems, where the parameter we seek is the initial condition.\nAssimilation problems can be approached from many directions (depending on your background/preferences):\n\ncontrol theory;\nvariational calculus;\nstatistical estimation theory;\nprobability theory,\nstochastic differential equations.\n\nNewer approaches (discussed later): nudging methods, reduced methods, ensemble methods and hybrid methods that combine variational and statistical approaches, Machine/Deep Learning based approaches."
  },
  {
    "objectID": "slides/week-02/02-basics.html#introduction-approaches-1",
    "href": "slides/week-02/02-basics.html#introduction-approaches-1",
    "title": "Basics",
    "section": "Introduction: approaches",
    "text": "Introduction: approaches\n\nNavigation: important application of the Kalman filter.\nRemote sensing: satellite data.\nGeophysics: seismic exploration, geophysical prospecting, earthquake prediction.\nAir and noise pollution, source estimation\nWeather forecasting.\nClimatology. Global warming.\nEpidemiology.\nForest fire evolution.\nFinance."
  },
  {
    "objectID": "slides/week-02/02-basics.html#introduction-nonlinearity",
    "href": "slides/week-02/02-basics.html#introduction-nonlinearity",
    "title": "Basics",
    "section": "Introduction: nonlinearity",
    "text": "Introduction: nonlinearity\nThe problems of data assimilation (in particular) and inverse problems in general arise from:\n\nThe nonlinear dynamics of the physical model equations.\nThe nonlinearity of the inverse problem."
  },
  {
    "objectID": "slides/week-02/02-basics.html#introduction-iterative-process",
    "href": "slides/week-02/02-basics.html#introduction-iterative-process",
    "title": "Basics",
    "section": "Introduction: iterative process",
    "text": "Introduction: iterative process\n\nClosely related to\n\nthe inference cycle\nmachine learning…"
  },
  {
    "objectID": "slides/week-02/02-basics.html#motivational-example-digital-twin-for-geological-co2-storage",
    "href": "slides/week-02/02-basics.html#motivational-example-digital-twin-for-geological-co2-storage",
    "title": "Basics",
    "section": "Motivational Example — Digital Twin for Geological CO2 storage",
    "text": "Motivational Example — Digital Twin for Geological CO2 storage\n\n\nPlume development & associated time-lapse seismic images\n\n\n\n\nRecovery of the CO2 plume\n\n\nSee for SLIM website for our research on Geological CO2 Storage."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-1",
    "href": "slides/week-02/02-basics.html#section-1",
    "title": "Basics",
    "section": "",
    "text": "FORWARD AND INVERSE PROBLEMS"
  },
  {
    "objectID": "slides/week-02/02-basics.html#inverse-problems",
    "href": "slides/week-02/02-basics.html#inverse-problems",
    "title": "Basics",
    "section": "Inverse problems",
    "text": "Inverse problems\n\n\n\n\nfrom Asch (2022)\n\n\nIngredients of an inverse problem: the physical reality (top) and the direct mathematical model (bottom). The inverse problem uses the difference between the model- predicted observations, u (calculated at the receiver array points \\(x_r\\)), and the real observations measured on the array, in order to find the unknown model parameters, m, or the source s (or both)."
  },
  {
    "objectID": "slides/week-02/02-basics.html#forward-and-inverse-problems",
    "href": "slides/week-02/02-basics.html#forward-and-inverse-problems",
    "title": "Basics",
    "section": "Forward and inverse problems",
    "text": "Forward and inverse problems\n\n\n\n\n\n\n\nConsider a parameter-dependent dynamical system, \\[\\frac{d\\mathbf{z}}{dt}=g(t,\\mathbf{z};\\boldsymbol{\\theta}),\\qquad \\mathbf{z}(t_{0})=\\mathbf{z}_{0},\\] with \\(g\\) known, \\(\\mathbf{z}_0\\) the initial condition, \\(\\boldsymbol{\\theta}\\in\\Theta\\) parameters of the system, \\(\\mathbf{z}(t)\\in\\mathbb{R}^{k}\\) the system’s state."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-2",
    "href": "slides/week-02/02-basics.html#section-2",
    "title": "Basics",
    "section": "",
    "text": "Forward problem: Given \\(\\boldsymbol{\\theta},\\) \\(\\mathbf{z}_{0},\\)find \\(\\mathbf{z}(t)\\) for \\(t\\ge t_{0}.\\)\nInverse problem: Given \\(\\mathbf{z}(t)\\) for \\(t\\ge t_{0},\\) find \\(\\boldsymbol{\\theta}\\in\\Theta.\\)"
  },
  {
    "objectID": "slides/week-02/02-basics.html#observations",
    "href": "slides/week-02/02-basics.html#observations",
    "title": "Basics",
    "section": "Observations",
    "text": "Observations\nObservation equation: \\[f(t,\\boldsymbol{\\theta})=\\mathcal{H}\\mathbf{z}(t,\\boldsymbol{\\theta}),\\] where \\(\\mathcal{H}\\) is the observation operator — to account for the fact that observations are never completely known (in space-time).\nUsually we have a finite number of discrete (space-time) observations \\[\\left\\{\\widetilde{y}_{j}\\right\\} _{j=1}^{n},\\] where \\[\\widetilde{y}_{j}\\approx f(t_{j},\\boldsymbol{\\theta}).\\]"
  },
  {
    "objectID": "slides/week-02/02-basics.html#noise-free-and-noise-data",
    "href": "slides/week-02/02-basics.html#noise-free-and-noise-data",
    "title": "Basics",
    "section": "Noise-free and noise data",
    "text": "Noise-free and noise data\nNoise-free: \\[\\widetilde{y}_{j}=f(t_{j},\\boldsymbol{\\theta})\\]\nNoisy Data: \\[\\widetilde{y}_{j}=f(t_{j},\\boldsymbol{\\theta})+\\varepsilon_{j},\\] where \\(\\varepsilon_{j}\\) is error and requires that we introduce variability/uncertainty into the modeling and analysis."
  },
  {
    "objectID": "slides/week-02/02-basics.html#well-posedness",
    "href": "slides/week-02/02-basics.html#well-posedness",
    "title": "Basics",
    "section": "Well-posedness",
    "text": "Well-posedness\n\nExistence\nUniqueness\nContinuous dependence of solutions on observations.\n\n✓ The existence and uniqueness together are also known as “identifiability”.\n✓ The continuous dependence is related to the “stability” of the inverse problem."
  },
  {
    "objectID": "slides/week-02/02-basics.html#well-posednessmath",
    "href": "slides/week-02/02-basics.html#well-posednessmath",
    "title": "Basics",
    "section": "Well-posedness—math",
    "text": "Well-posedness—math\n\nDefinition 2 Let \\(X\\) and \\(Y\\) be two normed spaces and let \\(K\\::\\:X\\rightarrow Y\\) be a linear or nonlinear map between the two. The problem of finding \\(x\\) given \\(y\\) such that \\[Kx=y\\] is well-posed if the following three properties hold:\n\n\nWP1\n\nExistence—for every \\(y\\in Y\\) there is (at least) one solution \\(x\\in X\\) such that \\(Kx=y.\\)\n\nWP2\n\nUniqueness—for every \\(y\\in Y\\) there is at most one \\(x\\in X\\) such that \\(Kx=y.\\)\n\nWP3\n\nStability— the solution \\(x\\) depends continuously on the data \\(y\\) in that for every sequence \\(\\left\\{ x_{n}\\right\\} \\subset X\\) with \\(Kx_{n}\\rightarrow Kx\\) as \\(n\\rightarrow\\infty,\\) we have that \\(x_{n}\\rightarrow x\\) as \\(n\\rightarrow\\infty.\\)"
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-3",
    "href": "slides/week-02/02-basics.html#section-3",
    "title": "Basics",
    "section": "",
    "text": "This concept of ill-posedness will help us to understand and distinguish between direct and inverse models.\nIt will provide us with basic comprehension of the methods and algorithms that will be used to solve inverse problems.\nFinally, it will assist us in the analysis of “what went wrong?” when we attempt to solve the inverse problems."
  },
  {
    "objectID": "slides/week-02/02-basics.html#ill-posedness-of-inverse-problems",
    "href": "slides/week-02/02-basics.html#ill-posedness-of-inverse-problems",
    "title": "Basics",
    "section": "Ill-posedness of inverse problems",
    "text": "Ill-posedness of inverse problems\nMany inverse problems are ill-posed!\nSimplest case: one observation \\(\\widetilde{y}\\) for \\(f(\\theta)\\) and we need to find the pre-image \\[\\theta^{*}=f^{-1}(\\widetilde{y})\\] for a given \\(\\widetilde{y}.\\)"
  },
  {
    "objectID": "slides/week-02/02-basics.html#simplest-case",
    "href": "slides/week-02/02-basics.html#simplest-case",
    "title": "Basics",
    "section": "Simplest case",
    "text": "Simplest case\n\nNon-existence: there is no \\(\\theta_{3}\\) such that \\(f(\\theta_{3})=y_{3}\\)\nNon-uniqueness: \\(y_{j}=f(\\theta_{j})=f(\\widetilde{\\theta}_{j})\\) for \\(j=1,2.\\)\nLack of continuity of inverse map: \\(\\left|y_{1}-y_{2}\\right|\\) small \\(\\nRightarrow\\left|f^{-1}(y_{1})-f^{-1}(y_{2})\\right|=\\left|\\theta_{1}-\\widetilde{\\theta}_{2}\\right|\\) small."
  },
  {
    "objectID": "slides/week-02/02-basics.html#why-is-this-so-important",
    "href": "slides/week-02/02-basics.html#why-is-this-so-important",
    "title": "Basics",
    "section": "Why is this so important?",
    "text": "Why is this so important?\nCouldn’t we just apply a good least squares algorithm (for example) to find the best possible solution?\n\nDefine \\(J(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}\\) for a given \\(y_{1}\\)\nApply a standard iterative scheme, such as direct search or gradient-based minimization, to obtain a solution\nNewton’s method: \\[\\theta^{k+1}=\\theta^{k}-\\left[J'(\\theta^{k})\\right]^{-1}J(\\theta^{k})\\]\nLeads to highly unstable behavior because of ill-posedness"
  },
  {
    "objectID": "slides/week-02/02-basics.html#what-went-wrong",
    "href": "slides/week-02/02-basics.html#what-went-wrong",
    "title": "Basics",
    "section": "What went wrong",
    "text": "What went wrong\n✗ This behavior is not the fault of steepest descent algorithms.\n✗ It is a manifestation of the inherent ill-posedness of the problem.\n✗ How to fix this problem is the subject of much research over the past 50 years!!!\nMany remedies (fortunately) exist…\n\nexplicit and implicit constrained optimizations\nregularization and penalization\nmachine learning…"
  },
  {
    "objectID": "slides/week-02/02-basics.html#tikhonov-regularization",
    "href": "slides/week-02/02-basics.html#tikhonov-regularization",
    "title": "Basics",
    "section": "Tikhonov regularization",
    "text": "Tikhonov regularization\nIdea: is to replace the ill-posed problem for \\(J(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}\\) by a “nearby” problem for \\[J_{\\beta}(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}+\\beta\\left|\\theta-\\theta_{0}\\right|^{2}\\] where \\(\\beta\\) is “suitably chosen” regularization/penalization parametersee below for details.\n\nWhen it is done correctly, TR provides convexity and compactness.\nEven when done correctly, it modifies the problem and new solutions may be far from the original ones.\nIt is not trivial to regularize correctly or even to know if you have succeeded…"
  },
  {
    "objectID": "slides/week-02/02-basics.html#non-uniqueness-seismic-travel-time-tomography",
    "href": "slides/week-02/02-basics.html#non-uniqueness-seismic-travel-time-tomography",
    "title": "Basics",
    "section": "Non uniqueness: seismic travel-time tomography",
    "text": "Non uniqueness: seismic travel-time tomography\n\n\n\n\n\nA signal seismic ray passes through a 2-parameter block model.\n\nunknowns are the 2 block slownesses (inverse of seismic velocity) \\(\\left(\\Delta s_{1},\\Delta s_{2}\\right)\\)\ndata is the observed travel time of the ray, \\(\\Delta t_{1}\\)\nmodel is the linearized travel time equation, \\(\\Delta t_{1}=l_{1}\\Delta s_{1}+l_{2}\\Delta s_{2}\\) where \\(l_{j}\\) is the length of the ray in the \\(j\\)-th block.\n\nClearly we have one equation for two unknowns and hence there is no unique solution."
  },
  {
    "objectID": "slides/week-02/02-basics.html#inverse-problems-general-formulation",
    "href": "slides/week-02/02-basics.html#inverse-problems-general-formulation",
    "title": "Basics",
    "section": "Inverse Problems: General Formulation",
    "text": "Inverse Problems: General Formulation\nAll inverse problems share a common formulation.\nLet the model parameters1 be a vector (in general, a multivariate random variable), \\(\\mathbf{m},\\) and the data be \\(\\mathbf{d},\\) \\[\\begin{aligned}\n    \\mathbf{m} & =\\left(m_{1},\\ldots,m_{p}\\right)\\in\\mathcal{M},\\\\\n    \\mathbf{d} & =\\left(d_{1},\\ldots,d_{n}\\right)\\in\\mathcal{D},\n    \\end{aligned}\\]\nwhere \\(\\mathcal{M}\\) and \\(\\mathcal{D}\\) are the corresponding model parameter space and data space.\nThe mapping \\(G\\colon\\mathcal{M}\\rightarrow\\mathcal{D}\\) is defined by the direct (or forward) model\n\\[\\mathbf{d}=g(\\mathbf{m}), \\qquad(1)\\] where\nApplied mathematicians often call the equation \\(G(m)=d\\) a mathematical model and \\(m\\) the parameters. Other scientists call \\(G\\) the forward operator and \\(m\\) the model. We will adopt the more mathematical convention, where \\(m\\) will be referred to as the model parameters, \\(G\\) the model and \\(d\\) the data."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-4",
    "href": "slides/week-02/02-basics.html#section-4",
    "title": "Basics",
    "section": "",
    "text": "\\(g\\in G\\) is an operator that describes the “physical” model and can take numerous forms, such as algebraic equations, differential equations, integral equations, or linear systems.\n\nThen we can add the observations or predictions, \\(\\mathbf{y}=(y_{1},\\ldots,y_{r}),\\) corresponding to the mapping from data space into observation space, \\(H\\colon\\mathcal{D}\\rightarrow\\mathcal{Y},\\) and described by \\[\\mathbf{y}=h(\\mathbf{d})=h\\left(g(\\mathbf{m})\\right),\\] where\n\n\\(h\\in H\\) is the observation operator, usually some projection into an observable subset of \\(\\mathcal{D}.\\)"
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-5",
    "href": "slides/week-02/02-basics.html#section-5",
    "title": "Basics",
    "section": "",
    "text": "Note\n\n\nIn addition, there will be some random noise in the system, usually modeled as additive noise, giving the more realistic, stochastic direct model \\[\\mathbf{d}=g(\\mathbf{m})+\\mathbf{\\epsilon},\\label{eq:stat-inv-pb} \\qquad(2)\\] where \\(\\mathbf{\\epsilon}\\) is a random vector."
  },
  {
    "objectID": "slides/week-02/02-basics.html#inverse-problemsclassification",
    "href": "slides/week-02/02-basics.html#inverse-problemsclassification",
    "title": "Basics",
    "section": "Inverse Problems—Classification",
    "text": "Inverse Problems—Classification\nWe can now classify inverse problems as:\n\ndeterministic inverse problems that solve 1 for \\(\\mathbf{m},\\)\nstatistical inverse problems that solve 2 for \\(\\mathbf{m}.\\)\n\nThe first class will be treated by linear algebra and optimization methods.\nThe latter can be treated by a Bayesian (filtering) approach, and by weighted least-squares, maximum likelihood and DA techniques\nBoth classes can be further broken down into:\n\nLinear inverse problems, where 1 or 2 are linear equations. These include linear systems that are often the result of discretizing (partial) differential equations and integral equations.\nNonlinear inverse problems where the algebraic or differential operators are nonlinear."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-6",
    "href": "slides/week-02/02-basics.html#section-6",
    "title": "Basics",
    "section": "",
    "text": "Finally, since most inverse problems cannot be solved explicitly, computational methods are indispensable for their solution, see sections 8.4 and 8.5 of Asch (2022)\nAlso note that we will be inverting here between the model and data spaces, that are usually both of high dimension and thus this model-based inversion will invariably be computationally expensive.\nThis will motivate us to employ\n\ninversion between the data and observation spaces in a purely data-driven approach, using machine learning methods\nthis aspect will be treated later during this course"
  },
  {
    "objectID": "slides/week-02/02-basics.html#tikhonov-regularizationintroduction",
    "href": "slides/week-02/02-basics.html#tikhonov-regularizationintroduction",
    "title": "Basics",
    "section": "Tikhonov Regularization—Introduction",
    "text": "Tikhonov Regularization—Introduction\nTikhonov regularization (TR) is probably the most widely used method for regularizing ill-posed, discrete and continuous inverse problems.\n\n\n\n\n\n\nNote\n\n\nNote that the LASSO and ridge regression methods—are special cases of TR.\n\n\n\nThe theory is the subject of entire books...\nRecall:\n\nthe objective of TR is to reduce, or remove, ill-posedness in optimization problems by modifying the objective function.\nthe three sources of ill-posedness: non-existence, non-uniqueness and sensitivity to perturbations.\nTR, in principle, addresses and alleviates all three sources of ill-posedness and is thus a vital tool for the solution of inverse problems."
  },
  {
    "objectID": "slides/week-02/02-basics.html#tikhonov-regularizationformulation",
    "href": "slides/week-02/02-basics.html#tikhonov-regularizationformulation",
    "title": "Basics",
    "section": "Tikhonov Regularization—Formulation",
    "text": "Tikhonov Regularization—Formulation\nThe most general TR objective function is \\[\\mathcal{T}_{\\alpha}(\\mathbf{m};\\mathbf{d})=\\rho\\left(G(\\mathbf{m}),\\mathbf{d}\\right)+\\alpha J(\\mathbf{m}),\\] where\n\n\\(\\rho\\) is the data discrepancy functional that quantifies the difference between the model output and the measured data;\n\\(J\\) is the regularization functional that represents some desired quality of the sought for model parameters, usually smoothness;\n\\(\\alpha\\) is the regularization parameter that needs to be tuned, and determines the relative importance of the regularization term.\n\nEach domain, each application and each context will require specific choices of these three items, and often we will have to rely either on previous experience, or on some sort of numerical experimentation (trial-and-error) to make a good choice.\nIn some cases there exist empirical algorithms, in particular for the choice of \\(\\alpha.\\)"
  },
  {
    "objectID": "slides/week-02/02-basics.html#tikhonov-regularizationdiscrepancy",
    "href": "slides/week-02/02-basics.html#tikhonov-regularizationdiscrepancy",
    "title": "Basics",
    "section": "Tikhonov Regularization—Discrepancy",
    "text": "Tikhonov Regularization—Discrepancy\nThe most common discrepancy functions are:\n\nleast-squares, \\[\\rho_{\\mathrm{LS}}(\\mathbf{d}_{1},\\mathbf{d}_{2})=\\frac{1}{2}\\Vert\\mathbf{d}_{1}-\\mathbf{d}_{2}\\Vert_{2}^{2},\\]\n1-norm, \\[\\rho_{1}(\\mathbf{d}_{1},\\mathbf{d}_{2})=\\vert\\mathbf{d}_{1}-\\mathbf{d}_{2}\\vert,\\]\nKullback-Leibler distance, \\[\\rho_{\\mathrm{KL}}(d_{1},d_{2})=\\left\\langle d_{1},\\log(d_{1}/d_{2})\\right\\rangle ,\\] where \\(d_{1}\\) and \\(d_{2}\\) are considered here as probability density functions. This discrepancy is valid in the Bayesian context."
  },
  {
    "objectID": "slides/week-02/02-basics.html#tikhonov-regularization-1",
    "href": "slides/week-02/02-basics.html#tikhonov-regularization-1",
    "title": "Basics",
    "section": "Tikhonov Regularization",
    "text": "Tikhonov Regularization\nThe most common regularization functionals are derivatives of order one or two.\n\nGradient smoothing: \\[J_{1}(\\mathbf{m})=\\frac{1}{2}\\Vert\\nabla\\mathbf{m}\\Vert_{2}^{2},\\] where \\(\\nabla\\) is the gradient operator of first-order derivatives of the elements of \\(\\mathbf{m}\\) with respect to each of the independent variables.\nLaplacian smoothing: \\[J_{2}(\\mathbf{m})=\\frac{1}{2}\\Vert\\nabla^{2}\\mathbf{m}\\Vert_{2}^{2},\\] where \\(\\nabla^{2}=\\nabla\\cdot\\nabla\\) is the Laplacian operator defined as the sum of all second-order derivatives of \\(\\mathbf{m}\\) with respect to each of the independent variables."
  },
  {
    "objectID": "slides/week-02/02-basics.html#trcomputing-the-regularization-parameter",
    "href": "slides/week-02/02-basics.html#trcomputing-the-regularization-parameter",
    "title": "Basics",
    "section": "TR—Computing the Regularization Parameter",
    "text": "TR—Computing the Regularization Parameter\nOnce the data discrepancy and regularization functionals have been chosen, we need to tune the regularization parameter, \\(\\alpha.\\)\nWe have here, similarly to the bias-variance trade-off of ML Lectures, a competition between the discrepancy error and the magnitude of the regularization term.\nWe need to choose, the best compromise between the two.\nWe will briefly present three frequently used approaches:\n\nL-curve method.\nDiscrepancy principle.\nCross-validation and LOOCV."
  },
  {
    "objectID": "slides/week-02/02-basics.html#trcomputing-the-regularization-parameter-1",
    "href": "slides/week-02/02-basics.html#trcomputing-the-regularization-parameter-1",
    "title": "Basics",
    "section": "TR—Computing the Regularization Parameter",
    "text": "TR—Computing the Regularization Parameter\n\n\n\n\n\n\nFigure 1\n\n\n\nThe L-curve criterion is an empirical method for picking a value of \\(\\alpha.\\)\n\nsince \\(e_{m}(\\alpha)=\\Vert\\mathbf{m}\\Vert_{2}\\) is a strictly decreasing function of \\(\\alpha\\) and \\(e_{d}(\\alpha)=\\Vert G\\mathbf{m}-\\mathbf{d}\\Vert_{2}\\) is a strictly increasing one,\nwe plot \\(\\log e_{m}\\) against \\(\\log e_{d}\\) we will always obtain an L-shaped curve that has an “elbow” at the optimal value of \\(\\alpha=\\alpha_{L},\\) or at least at a good approximation of this optimal value see Figure 1."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-7",
    "href": "slides/week-02/02-basics.html#section-7",
    "title": "Basics",
    "section": "",
    "text": "This trade-off curve gives us a visual recipe for choosing the regularization parameter, reminiscent of the bias-variance trade off\nThe range of values of \\(\\alpha\\) for which one should plot the curve has to be determined by either trial-and-error, previous experience, or a balancing of the two terms in the TR functional.\n\nThe discrepancy principle\n\nchoose the value of \\(\\alpha=\\alpha_{D}\\) such that the residual error (first term) is equal to an a priori bound, \\(\\delta,\\) that we would like to attain.\non the L-curve, this corresponds to the intersection with the vertical line at this bound, as shown in Figure 1.\na good approximation for the bound is to put \\(\\delta=\\sigma\\sqrt{n},\\) where \\(\\sigma^{2}\\) is the variance and \\(n\\) the number of observations.1 This can be thought of as the noise level of the data.\n\nThis is strictly valid under the hypothesis of i.i.d. Gaussian noise."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-8",
    "href": "slides/week-02/02-basics.html#section-8",
    "title": "Basics",
    "section": "",
    "text": "the discrepancy principle is also related to regularization by the truncated singular value decomposition (TSVD), in which case the truncation level implicitly defines the regularization parameter.\n\nCross-validation, as we explained in ML Lectures, is a way of using the observations themselves to estimate a parameter.\n\nWe then employ the classical approach of either LOOCV or \\(k\\)-fold cross validation, and choose the value of \\(\\alpha\\) that minimizes the RSS (Residual Sum of Squares) of the test sets.\nIn order to reduce the computational cost, a generalized cross validation (GCV) method can be used."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-9",
    "href": "slides/week-02/02-basics.html#section-9",
    "title": "Basics",
    "section": "",
    "text": "DATA ASSIMILATION"
  },
  {
    "objectID": "slides/week-02/02-basics.html#definitions-and-notation",
    "href": "slides/week-02/02-basics.html#definitions-and-notation",
    "title": "Basics",
    "section": "Definitions and notation",
    "text": "Definitions and notation\nAnalysis is the process of approximating the true state of a physical system at a given time\nAnalysis is based on:\n\nobservational data,\na model of the physical system,\nbackground information on initial and boundary conditions.\n\nAn analysis that combines time-distributed observations and a dynamic model is called data assimilation."
  },
  {
    "objectID": "slides/week-02/02-basics.html#standard-notation",
    "href": "slides/week-02/02-basics.html#standard-notation",
    "title": "Basics",
    "section": "Standard notation",
    "text": "Standard notation\nA discrete model for the evolution of a physical (atmospheric, oceanic, etc.) system from time \\(t_{k}\\) to time \\(t_{k+1}\\) is described by a dynamic, state equation\n\\[\\mathbf{x}^{f}(t_{k+1})=M_{k+1}\\left[\\mathbf{x}^{f}(t_{k})\\right], \\qquad(3)\\]\n\n\\(\\mathbf{x}\\) is the model’s state vector of dimension \\(n,\\)\n\\(M\\) is the corresponding dynamics operator (finite difference or finite element discretization), which can be time dependent.\n\nThe error covariance matrix associated with \\(\\mathbf{x}\\) is given by \\(\\mathbf{P}\\) since the true state will differ from the simulated state (Equation 3) by random or systematic errors.\nObservations, or measurements, at time \\(t_{k}\\) are defined by"
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-10",
    "href": "slides/week-02/02-basics.html#section-10",
    "title": "Basics",
    "section": "",
    "text": "\\[\\mathbf{y}_{k}^{\\mathrm{o}}=H_{k}\\left[\\mathbf{x}^{t}(t_{k})\\right]+\\varepsilon_{k}^{\\mathrm{o}},\\]\n\n\\(H\\) is an observation operator that can be time dependent\n\\(\\varepsilon_k^{\\mathrm{o}}\\) is a white noise process zero mean and covariance matrix \\(\\mathbf{R}\\) (instrument errors and representation errors due to the discretization)\nobservation vector \\(\\mathbf{y}_{k}^{\\mathrm{o}}=\\mathbf{y}^{\\mathrm{o}}(t_{k})\\) has dimension \\(p_{k}\\) (usually \\(p_{k}\\ll n.\\) )\n\nSubscripts are used to denote the discrete time index, the corresponding spatial indices or the vector with respect to which an error covariance matrix is defined.\nSuperscripts refer to the nature of the vectors/matrices\n\n“a” for analysis, “b” for background (or ‘initial/first guess’),\n“f” for forecast, “o” for observation, and\n“t” for the (unknown) true state.\n\nAn analysis that combines time-distributed observations and a dynamic model is called data assimilation."
  },
  {
    "objectID": "slides/week-02/02-basics.html#standard-notationcontinuous-system",
    "href": "slides/week-02/02-basics.html#standard-notationcontinuous-system",
    "title": "Basics",
    "section": "Standard notation—continuous system",
    "text": "Standard notation—continuous system\nNow let us introduce the continuous system. In fact, continuous time simplifies both the notation and the theoretical analysis of the problem. For a finite-dimensional system of ordinary differential equations, the sate and observation equations become \\[\\dot{\\mathbf{x}}^{\\mathrm{f}}=\\mathcal{M}(\\mathbf{x}^{\\mathrm{f}},t)%\\mathbf{\\dot{\\xf}}=\\mathcal{M}(\\xf,t)\\] and \\[\\mathbf{y}^{\\mathrm{o}}(t)=\\mathcal{H}(\\mathbf{x}^{\\mathrm{t}},t)+\\boldsymbol{\\epsilon},\\] where \\(\\dot{\\left(\\,\\right)}=\\mathrm{d}/\\mathrm{d}t,\\) \\(\\mathcal{M}\\) and \\(\\mathcal{H}\\) are nonlinear operators in continuous time for the model and the observation respectively.\nThis implies that \\(\\mathbf{x},\\) \\(\\mathbf{y},\\) and \\(\\boldsymbol{\\epsilon}\\) are also continuous-in-time functions.\n\nFor PDEs, where there is in addition a dependence on space, attention must be paid to the function spaces, especially when performing variational analysis."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-11",
    "href": "slides/week-02/02-basics.html#section-11",
    "title": "Basics",
    "section": "",
    "text": "With a PDE model, the field (state) variable is commonly denoted by \\(\\boldsymbol{u}(\\mathbf{x},t),\\) where \\(\\mathbf{x}\\) represents the space variables (no longer the state variable as above!), and the model dynamics is now a nonlinear partial differential operator, \\[\\mathcal{M}=\\mathcal{M}\\left[\\partial_{\\mathbf{x}}^{\\alpha},\\boldsymbol{u}(\\mathbf{x},t),\\mathbf{x},t\\right]\\] with \\(\\partial_{\\mathbf{x}}^{\\alpha}\\) denoting the partial derivatives with respect to the space variables of order up to \\(\\left|\\alpha\\right|\\le m\\) where \\(m\\) is usually equal to two and in general varies between one and four."
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-12",
    "href": "slides/week-02/02-basics.html#section-12",
    "title": "Basics",
    "section": "",
    "text": "CONCLUSIONS"
  },
  {
    "objectID": "slides/week-02/02-basics.html#section-13",
    "href": "slides/week-02/02-basics.html#section-13",
    "title": "Basics",
    "section": "",
    "text": "Data assimilation requires not only the observations and a background, but also knowledge of:\n\nerror statistics(background, observation, model, etc.)\nphysics (forecast model, model relating observed to retrieved variables, etc.).\n\nThe challenge of data assimilation is in combining our stochastic knowledge with our physical knowledge."
  },
  {
    "objectID": "slides/week-02/02-basics.html#open-source-software",
    "href": "slides/week-02/02-basics.html#open-source-software",
    "title": "Basics",
    "section": "Open-source software",
    "text": "Open-source software\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen.\nDataAssim.jl\nEnKF.jl"
  },
  {
    "objectID": "slides/week-02/02-basics.html#references",
    "href": "slides/week-02/02-basics.html#references",
    "title": "Basics",
    "section": "References",
    "text": "References\n\nK. Law, A. Stuart, K. Zygalakis. Data Assimilation. A Mathematical Introduction. Springer, 2015.\nG. Evensen. Data assimilation, The Ensemble Kalman Filter, 2nd ed., Springer, 2009.\nA. Tarantola. Inverse problem theory and methods for model parameter estimation. SIAM. 2005.\nO. Talagrand. Assimilation of observations, an introduction. J. Meteorological Soc. Japan, 75, 191, 1997.\nF.X. Le Dimet, O. Talagrand. Variational algorithms for analysis and assimilation of meteorological observations: theoretical aspects. Tellus, 38(2), 97, 1986.\nJ.-L. Lions. Exact controllability, stabilization and perturbations for distributed systems. SIAM Rev., 30(1):1, 1988.\nJ. Nocedal, S.J. Wright. Numerical Optimization. Springer, 2006.\nF. Tröltzsch. Optimal Control of Partial Differential Equations. AMS, 2010.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. SIAM."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#statistical-da-introduction",
    "href": "slides/week-05/07-DA-stat.html#statistical-da-introduction",
    "title": "Statistical Data Assimilation",
    "section": "Statistical DA: introduction",
    "text": "Statistical DA: introduction\nNow we will generalize the variational approach to deal with errors and noise in\n\nthe models,\nthe observations and\nthe initial conditions.\n\nThe variational results could of course be derived as a special case of statistical DA, in the limit where the noise disappears.\nEven the statistical results can be derived in a very general way, using SDEs and/or Bayesian analysis, and then specialized to the various Kalman-type filters that we will study here.\nPractical inverse problems and data assimilation problems involve measured data.\n\nThese data are inexact and are mixed with random noise."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section",
    "href": "slides/week-05/07-DA-stat.html#section",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Only statistical models can provide rigorous, effective means for dealing with this measurement error.\n\nWe want to estimate a scalar quantity, say the temperature or the ozone level, at a fixed point in space.\n\n\n\nSuppose we have:\n\na model forecast, \\(x^{\\mathrm{b}}\\) (background, or a priori value)\nand a measured value, \\(x^{\\mathrm{obs}}\\) (observation).\n\nThe simplest possible approach is to try a linear combination of the two, \\[x^{\\mathrm{a}}=x^{\\mathrm{b}}+w(x^{\\mathrm{obs}}-x^{\\mathrm{b}}),\\] where \\(x^{\\mathrm{a}}\\) denotes the analysis that we seek and \\(0\\le w\\le1\\) is a weight factor. We subtract the (always unknown) true state \\(x^{\\mathrm{t}}\\) from both sides, \\[x^{\\mathrm{a}}-x^{\\mathrm{t}}=x^{\\mathrm{b}}-x^{\\mathrm{t}}+w(x^{\\mathrm{obs}}-x^{\\mathrm{t}}-x^{\\mathrm{b}}+x^{\\mathrm{t}})\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-1",
    "href": "slides/week-05/07-DA-stat.html#section-1",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "and defining the three errors (analysis, background, observation) as \\[e^{\\mathrm{a}}=x^{\\mathrm{a}}-x^{\\mathrm{t}},\\quad\\mathrm{e^{b}}=x^{\\mathrm{b}}-x^{\\mathrm{t}},\\quad e^{\\mathrm{obs}}=x^{\\mathrm{obs}}-x^{\\mathrm{t}},\\] we obtain \\[e^{\\mathrm{a}}=e^{\\mathrm{b}}+w(e^{\\mathrm{obs}}-e^{\\mathrm{b}})=we^{\\mathrm{obs}}+(1-w)e^{\\mathrm{b}}.\\] If we have many realizations, we can take an ensemble average, or expectation, denoted by \\(\\left\\langle \\cdot\\right\\rangle ,\\) \\[\\left\\langle e^{\\mathrm{a}}\\right\\rangle =\\left\\langle e^{\\mathrm{b}}\\right\\rangle +w(\\left\\langle e^{\\mathrm{obs}}\\right\\rangle -\\left\\langle e^{\\mathrm{b}}\\right\\rangle ).\\] Now if these errors are centred (have zero mean, or the estimates of the true state are unbiased), then \\[\\left\\langle e^{\\mathrm{a}}\\right\\rangle =0\\] also. So we must look at the variance and demand that it be as small as possible. The variance is defined, using the above notation, as"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-2",
    "href": "slides/week-05/07-DA-stat.html#section-2",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "\\[\\sigma^{2}=\\left\\langle \\left(e-\\left\\langle e\\right\\rangle \\right)^{2}\\right\\rangle .\\] Now, taking variances of the error equation, and using the zero-mean property, we obtain \\[\\sigma_{\\mathrm{a}}^{2}=\\sigma_{\\mathrm{b}}^{2}+w^{2}\\left\\langle \\left(e^{\\mathrm{obs}}-e^{\\mathrm{b}}\\right)^{2}\\right\\rangle +2w\\left\\langle e^{\\mathrm{b}}\\left(e^{\\mathrm{obs}}-e^{\\mathrm{b}}\\right)\\right\\rangle .\\] This reduces to \\[\\sigma_{\\mathrm{a}}^{2}=\\sigma_{\\mathrm{b}}^{2}+w^{2}\\left(\\sigma_{\\mathrm{o}}^{2}+\\sigma_{\\mathrm{b}}^{2}\\right)-2w\\sigma_{\\mathrm{b}}^{2}\\] if \\(e^{\\mathrm{o}}\\) and \\(e^{\\mathrm{b}}\\) are uncorrelated.\nNow, to compute a minimum, take the derivative with respect to \\(w\\) and equate to zero, to obtain \\[0=2w\\left(\\sigma_{\\mathrm{obs}}^{2}+\\sigma_{\\mathrm{b}}^{2}\\right)-2\\sigma_{\\mathrm{b}}^{2},\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-3",
    "href": "slides/week-05/07-DA-stat.html#section-3",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "where we have ignored all cross terms (errors are assumed independent). Finally, solving this last equation, we can write the optimal weight, \\[w_{*}=\\frac{\\sigma_{\\mathrm{b}}^{2}}{\\sigma_{\\mathrm{obs}}^{2}+\\sigma_{\\mathrm{b}}^{2}}=\\frac{1}{1+\\sigma_{\\mathrm{o}}^{2}/\\sigma_{\\mathrm{b}}^{2}}\\] which depends on the ratio of the background and the observation errors. Clearly \\(0\\le w_{*}\\le1\\) and\n\nif the observation is perfect, \\(\\sigma_{\\mathrm{obs}}^{2}=0\\) and thus \\(w_{*}=1,\\) the maximum weight;\nif the background is perfect, \\(\\sigma_{\\mathrm{b}}^{2}=0\\) and \\(w_{*}=0,\\) so the observation will not be taken into account.\n\nWe can now rewrite the analysis error variance as,"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-4",
    "href": "slides/week-05/07-DA-stat.html#section-4",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\sigma_{\\mathrm{a}}^{2} & =w_{*}^{2}\\sigma_{\\mathrm{obs}}^{2}+(1-w_{*})^{2}\\sigma_{\\mathrm{b}}^{2}\\\\\n& =\\frac{\\sigma_{\\mathrm{b}}^{2}\\sigma_{\\mathrm{obs}}^{2}}{\\sigma_{\\mathrm{obs}}^{2}+\\sigma_{\\mathrm{b}}^{2}}\\\\\n& =(1-w_{*})\\sigma_{\\mathrm{b}}^{2}\\\\\n& =\\frac{1}{\\sigma_{\\mathrm{obs}}^{-2}+\\sigma_{\\mathrm{b}}^{-2}},\n\\end{aligned}\\] where we suppose that \\(\\sigma_{\\mathrm{b}}^{2},\\;\\sigma_{\\mathrm{o}}^{2}&gt;0.\\) In other words, \\[\\frac{1}{\\sigma_{\\mathrm{a}}^{2}}=\\frac{1}{\\sigma_{\\mathrm{o}}^{2}}+\\frac{1}{\\sigma_{\\mathrm{b}}^{2}}.\\] This is a very fundamental result, implying that the overall precision, \\(\\tau=1/\\sigma^{2},\\) (reciprocal of the variance) is the sum of the background and measurement precisions. Finally, the analysis equation becomes\n\\[x^{\\mathrm{a}}=x^{\\mathrm{b}}+\\frac{1}{1+\\alpha}(x^{\\mathrm{obs}}-x^{\\mathrm{b}}),\\] where \\(\\alpha=\\sigma_{\\mathrm{obs}}^{2}/\\sigma_{\\mathrm{b}}^{2}.\\) This is called the BLUE- Best Linear Unbiased Estimator - because it gives an unbiased, optimal weighting for a linear combination of two independent measurements."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#statistical-da-3-special-cases-and-conclusions",
    "href": "slides/week-05/07-DA-stat.html#statistical-da-3-special-cases-and-conclusions",
    "title": "Statistical Data Assimilation",
    "section": "Statistical DA: 3 special cases and conclusions",
    "text": "Statistical DA: 3 special cases and conclusions\nWe can isolate three special cases:\n\nif the observation is very accurate, \\(\\sigma_{\\mathrm{obs}}^{2}\\ll\\sigma_{\\mathrm{b}}^{2},\\) \\(\\alpha\\ll1\\) and thus \\(x^{\\mathrm{a}}\\approx x^{\\mathrm{obs}}\\)\nif the background is accurate, \\(\\alpha\\gg1\\) and \\(x^{\\mathrm{a}}\\approx x^{\\mathrm{b}}\\)\nand finally, if observation and background varaiances are approximately equal, \\(\\alpha\\approx1\\) and \\(x^{\\mathrm{a}}\\) is the arithmetic average of \\(x^{\\mathrm{b}}\\) and \\(x^{\\mathrm{obs}}.\\)\n\nConclusion: this simple, linear model does indeed capture the full range of possible solutions in a statistically rigorous manner, thus providing us with an “enriched” solution when compared with a non-probabilistic, scalar response such as the arithmetic average of observation and background, which would correspond to only the last of the above three special cases."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-5",
    "href": "slides/week-05/07-DA-stat.html#section-5",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters---background-and-history",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters---background-and-history",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters - background and history",
    "text": "Kalman Filters - background and history\nDA is concerned with dynamic systems, where (noisy) observations are acquired over time.\nQuestion: Is there some statistically optimal way to combine the dynamic model and the observations?\n\nOne answer is provided by Kalman filters\nThey are linear models for state estimation of noisy dynamic systems.\nThey have been the de facto standard in many robotics and tracking/prediction applications because they are well-suited for systems where there is uncertainty about an observable dynamic process.\nThey are also the basis of many data assimilation systems.\nThey use a paradigm of “observe, predict, correct” to extract information from a noisy signal.\n\nThe Kalman filter was invented1 in 1960 by R. E. Kálmán to solve this sort of problem in a mathematically optimal way.\n1 Apparently, following a prior invention by Stratonovich, one year earlier."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-6",
    "href": "slides/week-05/07-DA-stat.html#section-6",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains.\n\nThere are Kalman filters in aircraft and autonomous vehicles, on submarines, and, in cruise missiles.\nWall Street uses them to track the market.\nThey are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments.\nChemical plants use them to control and monitor reactions.\nThey are used to perform medical imaging and to remove noise from cardiac signals.\nWeather forecasting is based on Kalman filters.\nThey can effectively be used for modeling in epidemiology.\n\nIn summary, if it involves a sensor and/or time-series data, a Kalman filter or a close relative of the Kalman filter is usually involved."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters-formulation",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters-formulation",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters — formulation",
    "text": "Kalman Filters — formulation\nConsider a dynamical system that evolves in time and we would like to estimate a series of true states, \\(\\mathbf{x}^{\\mathrm{t}}_{k}\\) (a sequence of random vectors) where discrete time is indexed by the letter \\(k.\\)\nThese times are those when the observations or measurements are taken, as shown in the Figure.\n\n\nFigure 1: Sequential assimilation: a computed model trajectory, observations, and their error bars."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-7",
    "href": "slides/week-05/07-DA-stat.html#section-7",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "The assimilation starts with an unconstrained model trajectory from \\(t_{0},t_{1},\\ldots,t_{k-1},t_{k},\\ldots,t_{n}\\) and aims to provide an optimal fit to the available observations/measurements given their uncertainties (error bars).\n\nFor example, in current, synoptic scale weather forecasts, \\(t_{k}-t_{k-1}=6\\) hours and is less for the convective scale.\nIn robotics, or autonomous vehicles, the time intervals are of the order of the instrumental frequency, which can be a few milliseconds."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters---stochastic-model",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters---stochastic-model",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters - stochastic model",
    "text": "Kalman Filters - stochastic model\nWe seek to estimate the state \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) of a discrete-time dynamic process that is governed by the linear stochastic difference equation \\[\\mathbf{x}_{k+1}=\\mathbf{M}_{k+1}\\mathbf{x}_{k}+\\mathbf{w}_{k} \\tag{1}\\]\nwith a measurement/observation \\(\\mathbf{y}\\in\\mathbb{R}^{m},\\) \\[\\mathbf{y}_{k}=\\mathbf{H}_{k}\\mathbf{x}_{k}+\\mathbf{v}_{k}. \\tag{2}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\mathbf{M}_{k+1}\\) and \\(\\mathbf{H}_{k}\\) are considered linear, here.\nThe random vectors, \\(\\mathbf{w}_{k}\\) and \\(\\mathbf{v}_{k},\\) represent the process/modeling and measurement/observation errors respectively.\nThey are assumed to be independent, white noise processes with Gaussian/normal probability distributions, \\[\\begin{aligned}\n    \\mathbf{w}_{k} & \\sim & \\mathcal{N}(0,\\mathbf{Q}_{k}),\\\\\n    \\mathbf{v}_{k} & \\sim & \\mathcal{N}(0,\\mathbf{R}_{k}),\n    \\end{aligned}\\] where \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) are the covariance matrices (supposed known) of the modeling and observation errors respectively."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-8",
    "href": "slides/week-05/07-DA-stat.html#section-8",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "All these assumptions about unbiased and uncorrelated errors (in time and between each other) are not limiting, since extensions of the standard Kalman filter can be developed should any of these not be valid see next lecture.\nWe note that, for a broader mathematical view on the above system, we could formulate all of statistical DA in terms of stochastic differential equations (SDEs).\n\nThen the theory of Itô provides a detailed solution of the problem of optimal filtering as well as rigorous existence and uniqueness results… see (Law, Stuart, and Zygalakis 2015; Särkkä and Svensson 2023)."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters-sequential-assimilation-scheme",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters-sequential-assimilation-scheme",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters — sequential assimilation scheme",
    "text": "Kalman Filters — sequential assimilation scheme\nThe typical assimilation scheme is made up of two major steps:\n\na prediction/forecast step, and\na correction/analysis step.\n\n\n\nFigure 2: Sequential assimilation scheme for the Kalman filter. The x-axis denotes time, the y-axis denotes the values of the state and observations vectors."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-9",
    "href": "slides/week-05/07-DA-stat.html#section-9",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "At time \\(t_{k}\\) we have the result of a previous forecast, \\(\\mathbf{x}^{\\mathrm{f}}_{k},\\) (the analogue of the background state \\(\\mathbf{x}^{\\mathrm{b}}_{k}\\)) and the result of an ensemble of observations in \\(\\mathbf{y}_{k}.\\)\nBased on these two vectors, we perform an analysis that produces \\(\\mathbf{x}^{\\mathrm{a}}_{k}.\\)\nWe then use the evolution model to obtain a prediction of the state at time \\(t_{k+1}.\\)\nThe result of the forecast is denoted \\(\\mathbf{x}^{\\mathrm{f}}_{k+1},\\) and becomes the background, or initial guess, for the next time-step see Figure 2).\nThe Kalman filter problem can be resumed as follows:\n\ngiven a prior/background estimate \\(\\mathbf{x}^{\\mathrm{f}}\\) of the system state at time \\(t_{k},\\)\nwhat is the best update/analysis \\(\\mathbf{x}^{\\mathrm{a}}_{k}\\) based on the currently available measurements \\(\\mathbf{y}_{k}?\\)"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters-the-filter",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters-the-filter",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters — the filter",
    "text": "Kalman Filters — the filter\nThe goal of the Kalman filter is:\n\nto compute an optimal a posteriori estimate \\(\\mathbf{x}_{k}^{\\mathrm{a}}\\)\nthat is a linear combination of an a priori estimate \\(\\mathbf{x}_{k}^{\\mathrm{f}}\\) and a weighted difference between the actual measurement \\(\\mathbf{y}_{k}\\) and the measurement prediction \\(\\mathbf{H}_{k}\\mathbf{x}^{\\mathrm{f}}_{k}.\\)\n\nThis is none other than the BLUE that we have seen above.\nThe filter is thus of the linear, recursive form \\[\\mathbf{x}_{k}^{\\mathrm{a}}=\\mathbf{x}_{k}^{\\mathrm{f}}+\\mathbf{K}_{k}\\left(\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\right). \\tag{3}\\]\nThe difference \\(\\mathbf{d}_{k}=\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\) is called the innovation and reflects the discrepancy between the actual and the predicted measurements at time \\(t_{k}.\\)"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-10",
    "href": "slides/week-05/07-DA-stat.html#section-10",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Note\n\n\nNote that, for generality, the matrices are shown with a time-dependence. When this is not the case, the subscripts \\(k\\) can be dropped.\n\n\n\nThe Kalman gain matrix, \\(\\mathbf{K},\\) minimizes the a posteriori error covariance Equation 4.\n\nWe define forecast (a priori) and analysis (a posteriori) estimate errors as \\[\\begin{aligned}\n    \\mathbf{e}_{k}^{\\mathrm{f}} & = & \\mathbf{x}_{k}^{\\mathrm{f}}-\\mathbf{x}_{k}^{\\mathrm{t}},\\\\\n    \\mathbf{e}_{k}^{\\mathrm{a}} & = & \\mathbf{x}_{k}^{\\mathrm{a}}-\\mathbf{x}_{k}^{\\mathrm{t}},\n    \\end{aligned}\\] where \\(\\mathbf{x}_{k}^{\\mathrm{t}}\\) is the (unknown) true state. Respective error covariance matrices are \\[\\begin{aligned}\n    \\mathbf{P}_{k}^{\\mathrm{f}} & = & \\mathop{\\mathrm{Cov}}(\\mathbf{e}_{k}^{\\mathrm{f}})=\\mathrm{E}\\left[\\mathbf{e}_{k}^{\\mathrm{f}}(\\mathbf{e}_{k}^{\\mathrm{f}})^{\\mathrm{T}}\\right],\\nonumber \\\\\n    \\mathbf{P}_{k}^{\\mathrm{a}} & = & \\mathop{\\mathrm{Cov}}(\\mathbf{e}_{k}^{\\mathrm{a}})=\\mathrm{E}\\left[\\mathbf{e}_{k}^{\\mathrm{a}}(\\mathbf{e}_{k}^{\\mathrm{a}})^{\\mathrm{T}}\\right].\n    \\end{aligned} \\tag{4}\\]\n\nOptimal gain requires a careful derivation, that is beyond our scope here (see (Asch 2022; Asch, Bocquet, and Nodet 2016))."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters-optimal-gain",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters-optimal-gain",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters — optimal gain",
    "text": "Kalman Filters — optimal gain\nThe Kalman gainmatrix, \\(\\mathbf{K},\\) is chosen to minimize the a posteriori error covariance Equation 4.\nThe resulting \\(\\mathbf{K}\\) that minimizes Equation 4 is given by \\[\\mathbf{K}_{k}=\\mathbf{P}_{k}^{\\mathrm{f}}\\mathbf{H}_{k}^{\\mathrm{T}}\\left(\\mathbf{H}_{k}\\mathbf{P}_{k}^{\\mathrm{f}}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k}\\right)^{-1} \\tag{5}\\] where we remark that \\(\\mathbf{H}_k\\mathbf{P}_{k}^{\\mathrm{f}}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k}=\\mathrm{E}\\left[\\mathbf{d}_{k}\\mathbf{d}_{k}^{\\mathrm{T}}\\right]\\) is the covariance of the innovation.\nLooking at this expression for \\(\\mathbf{K}_{k},\\) we see:\n\nwhen the measurement error covariance\\(\\mathbf{R}_{k}\\) approaches zero, the gain \\(\\mathbf{K}_{k}\\) weights the innovation more heavily, since \\[\\lim_{\\mathbf{R}\\rightarrow0}\\mathbf{K}_{k}=\\mathbf{H}_{k}^{-1}.\\]\nOn the other hand, as the a priori error estimate covariance \\(\\mathbf{P}_{k}^{\\mathrm{f}}\\) approaches zero,"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-11",
    "href": "slides/week-05/07-DA-stat.html#section-11",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "the gain \\(\\mathbf{K}_{k}\\) weights the innovation less heavily, and \\[\\lim_{\\mathbf{P}_{k}^{\\mathrm{f}}\\rightarrow0}\\mathbf{K}_{k}=0.\\]\nAnother way of thinking about the weighting of \\(\\mathbf{K}\\) is that as the measurement error covariance \\(\\mathbf{R}\\) approaches zero, the actual measurement \\(\\mathbf{y}_{k}\\) is “trusted” more and more, while the predicted measurement \\(\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\) is trusted less and less.\nOn the other hand, as the a priori error estimate covariance \\(\\mathbf{P}_{k}^{\\mathrm{f}}\\) approaches zero, the actual measurement \\(\\mathbf{y}_{k}\\) is trusted less and less, while the predicted measurement \\(\\mathbf{H}_{k}\\mathbf{x}_{k}^{\\mathrm{f}}\\) is “trusted” more and more this will be illustrated in the computational example below."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filters-2-step-procedure",
    "href": "slides/week-05/07-DA-stat.html#kalman-filters-2-step-procedure",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filters — 2-step procedure",
    "text": "Kalman Filters — 2-step procedure\n\n\nFigure 3: Kalman filter loop, showing the two phases, predict and correct, preceded by an initialization step.\nThe predictor-corrector loop is illustrated in Figure 3 and can be transposed, as is, into an operational algorithm."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kf-predictorforecast-step",
    "href": "slides/week-05/07-DA-stat.html#kf-predictorforecast-step",
    "title": "Statistical Data Assimilation",
    "section": "KF — predictor/forecast step",
    "text": "KF — predictor/forecast step\n\n\nStart from a previous analyzed state1, \\(\\mathbf{x}_{k}^{\\mathrm{a}},\\) or from the initial state if \\(k=0,\\) characterized by the Gaussian pdf \\(p(\\mathbf{x}_{k}^{\\mathrm{a}}\\mid\\mathbf{y}_{1:k}^{\\mathrm{o}})\\) of mean \\(\\mathbf{x}_{k}^{\\mathrm{a}}\\) and covariance matrix \\(\\mathbf{P}_{k}^{a}.\\)\nAn estimate of \\(\\mathbf{x}_{k+1}^{\\mathrm{t}}\\) is given by the dynamical model which defines the forecast as \\[\\begin{gather}\n    \\mathbf{x}_{k+1}^{\\mathrm{f}} & = & \\mathbf{M}_{k+1}\\mathbf{x}_{k}^{\\mathrm{a}},\\label{eq:fstate}\\\\\n    \\mathbf{P}_{k+1}^{\\mathrm{f}} & = & \\mathbf{M}_{k+1}\\mathbf{P}_{k}^{\\mathrm{a}}\\mathbf{M}_{k+1}^{\\mathrm{T}}+\\mathbf{Q}_{k+1},\n    \\end{gather} \\tag{6}\\] where the expression for \\(\\mathbf{P}^{\\mathrm{f}}_{k+1}\\) is obtained from the dynamics equation and the definition of the model noise covariance, \\(\\mathbf{Q}.\\)\n\n\n\n\nWe use here the classical notation \\(\\mathbf{y}_{i:j}=(\\mathbf{y}_{i},\\mathbf{y}_{i+1},\\ldots,\\mathbf{y}_{j})\\) for \\(i\\le j\\) that denotes conditioning on all the observations in the interval."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kf---correctoranalysis-step",
    "href": "slides/week-05/07-DA-stat.html#kf---correctoranalysis-step",
    "title": "Statistical Data Assimilation",
    "section": "KF - corrector/analysis step",
    "text": "KF - corrector/analysis step\n\n\nAt time \\(t_{k+1},\\) the pdf \\(p(\\mathbf{x}_{k+1}^{\\mathrm{f}}\\mid\\mathbf{y}_{1:k}^{\\mathrm{o}})\\) is known, thanks to the mean \\(\\mathbf{x}_{k+1}^{\\mathrm{f}}\\) and covariance matrix \\(\\mathbf{P}_{k+1}^{\\mathrm{f}}\\) just calculated, as well as the assumption of a Gaussian distribution.\nThe analysis step then consists of correcting this pdf using the observation available at time \\(t_{k+1}\\) in order to compute \\(p(\\mathbf{x}_{k+1}^{\\mathrm{a}}\\mid\\mathbf{y}_{1:k+1}^{\\mathrm{o}}).\\) This comes from the BLUE in the dynamical context and gives\n\n\n\n\n\\[\\begin{gather}\n    \\mathbf{K}_{k+1} & = & \\mathbf{P}_{k+1}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}\\left(\\mathbf{H}\\mathbf{P}_{k+1}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R}_{k+1}\\right)^{-1},\\label{eq:aK}\\\\\n    \\mathbf{x}_{k+1}^{\\mathrm{a}} & = & \\mathbf{x}_{k+1}^{\\mathrm{f}}+\\mathbf{K}_{k+1}\\left(\\mathbf{y}_{k+1}-\\mathbf{H}\\mathbf{x}_{k+1}^{\\mathrm{f}}\\right),\\label{eq:astate}\\\\\n    \\mathbf{P}_{k+1}^{\\mathrm{a}} & = & \\left(\\mathbf{I}-\\mathbf{K}_{k+1}\\mathbf{H}\\right)\\mathbf{P}_{k+1}^{\\mathrm{f}}.\\label{eq:acov}\n    \\end{gather} \\tag{7}\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#overall-picture",
    "href": "slides/week-05/07-DA-stat.html#overall-picture",
    "title": "Statistical Data Assimilation",
    "section": "Overall Picture",
    "text": "Overall Picture\n\n\n\nPrinciple: as we move forward in time, the uncertainty of the analysis is reduced, and the forecast is improved."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kf-relation-between-bayes-and-blue",
    "href": "slides/week-05/07-DA-stat.html#kf-relation-between-bayes-and-blue",
    "title": "Statistical Data Assimilation",
    "section": "KF — Relation Between Bayes and BLUE",
    "text": "KF — Relation Between Bayes and BLUE\nIf we know that the a priori and the observation data are both Gaussian, Bayes’ rule can be readily applied to compute the a posteriori pdf.\n\nThe a posteriori pdf is then Gaussian, and its parameters are given by the BLUE equations.\n\nHence with Gaussian pdfs and a linear observation operator, there is no need to use Bayes’ rule.\n\nThe BLUE equations can be used instead to compute the parameters of the resulting pdf.\nSince the BLUE provides the same result as Bayes’ rule, it is the best estimator of all.\n\nIn addition one can recognize the 3D-Var cost function.\n\nBy optimizing this cost function, 3D-Var finds the MAP (maximum a posteriori) estimate of the Gaussian pdf, which is equivalent to the MV (minimum variance) estimate found by the BLUE."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#kalman-filter-extensions",
    "href": "slides/week-05/07-DA-stat.html#kalman-filter-extensions",
    "title": "Statistical Data Assimilation",
    "section": "Kalman Filter — extensions",
    "text": "Kalman Filter — extensions\nThere are many variants, extensions and generalizations of the Kalman Filter.\nLater, we will study in more detail:\n\nensemble Kalman Filters\nBayesian and nonlinear Kalman Filters: extended, unscented\nParticle filters\n\nOne usually has to choose between\n\nlinear Kalman filters\nensemble Kalman filters\nnonlinear filters\nhybrid variational-filter methods.\n\nThese questions will be addressed later."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-12",
    "href": "slides/week-05/07-DA-stat.html#section-12",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "ENSEMBLE KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#ensemble-kalman-filter-enkf",
    "href": "slides/week-05/07-DA-stat.html#ensemble-kalman-filter-enkf",
    "title": "Statistical Data Assimilation",
    "section": "Ensemble Kalman Filter — EnKF",
    "text": "Ensemble Kalman Filter — EnKF\nThe ensemble Kalman filter (EnKF) is an elegant approach that avoids\n\nthe steps of linearization in the classical Kalman Filter,\nand the need for adjoints in the variational approach.\n\nIt is still based on a Kalman filter, but an ensemble of realizations is used to compute an estimate of the population mean and variance, thus avoiding the need to compute inverses of potentially large matrices to obtain the posterior covariance, as was the case above in equations for \\(\\mathbf{K}_{k+1}\\) and \\(\\mathbf{P}_{k+1}^a\\) (Equation 7).\nThe EnKF and its variants have been successfully developed and implemented in meteorology and oceanography, including in operational weather forecasting systems. Because the method is simple to implement, it has been widely used in these fields."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-13",
    "href": "slides/week-05/07-DA-stat.html#section-13",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "But it has spread out to other geoscience disciplines and beyond. For instance, to name a few domains, it has been applied\n\nin greenhouse gas inverse modeling,\nair quality forecasting,\nextra-terrestrial atmosphere forecasting ,\ndetection and attribution in climate sciences,\ngeomagnetism re-analysis , and\nice-sheet parameter estimation and forecasting. It has also been used in petroleum reservoir estimation,\nin adaptive optics for extra large telescopes, and highway traffic estimation.\n\nMore recently, the idea was proposed to exploit the EnKF as a universal approach for all inverse problems. The term EKI, Ensemble Kalman Inversion, is used to describe this approach."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#principle-of-the-enkf",
    "href": "slides/week-05/07-DA-stat.html#principle-of-the-enkf",
    "title": "Statistical Data Assimilation",
    "section": "Principle of the EnKF",
    "text": "Principle of the EnKF\nThe EnKF was originally proposed by G. Evensen in 1994 and amended in Evensen et al. (2009).\n\nDefinition 1 The ensemble Kalman filter (EnKF) is a Kalman filter that uses an ensemble of realizations to compute estimates of the population mean and covariance.\n\nSince it is based on Gaussian statistics (mean and covariance) it does not solve the Bayesian filtering problem in the limit of a large number of particles, as opposed to the more general particle filter, which will be discussed later. Nonetheless, it turns out to be an excellent approximate algorithm for the filtering problem.\nAs in the particle filter, the EnKF is based on the concept of particles, a collection of state vectors, which are called the members of the ensemble.\n\nRather than propagating huge covariance matrices, the errors are emulated by scattered particles, a collection of state vectors whose variability is meant to be representative of the uncertainty of the system’s state resulting from the forecaster’s ignorance."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-14",
    "href": "slides/week-05/07-DA-stat.html#section-14",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Just like the particle filter, the members are propagated by the nonlinear model, without any linearization. Not only does this avoid the derivation of the tangent linear model, but it also circumvents the approximate linearization.\nFinally, as opposed to the particle filter, the EnKF does not irremediably suffer from the curse of dimensionality.\n\nTo sum up, here are the important remarks:\n\nthe EnKF avoids the linearization step of the KF;\nthe EnKF avoids the inversion of potentially large matrices;\nthe EnKF does not require any adjoint, as in variational assimilation;\nthe EnKF has been applied to a vast number of real-world problems."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#enkf-the-three-steps",
    "href": "slides/week-05/07-DA-stat.html#enkf-the-three-steps",
    "title": "Statistical Data Assimilation",
    "section": "EnKF — the Three Steps",
    "text": "EnKF — the Three Steps\n\nInitialization: generate an ensemble of \\(m\\) random states \\(\\left\\{ \\mathbf{x}_{i,0}^{\\mathrm{f}}\\right\\} _{i=1,\\ldots,m}\\) at time \\(t=0.\\)\nForecast: compute the prediction for each member of the ensemble.\nAnalysis: correct the prediction in light of the observations.\n\nPlease see the Algorithm 0.44 below for details of each step.\n\n\n\n\n\n\nNote\n\n\n\nPropagation can equivalently be performed either at the end of the analysis step or at the beginning of the forecast step.\nThe Kalman gain is not computed directly, but estimated from the ensemble statistics.\nWith the important exception of the Kalman gain computation, all operations on the ensemble members are independent. As a result, parallelization is straightforward.\nThis is one of the main reasons for the success/popularity of the EnKF."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#enkf-analysis-step",
    "href": "slides/week-05/07-DA-stat.html#enkf-analysis-step",
    "title": "Statistical Data Assimilation",
    "section": "EnKF — Analysis Step",
    "text": "EnKF — Analysis Step\nThe EnKF seeks to mimic the analysis step of the Kalman filter but with an ensemble of limited size in place of the unwieldy covariance matrices.\nThe goal is to perform for each member of the ensemble an analysis of the form, \\[\\mathbf{x}_{i}^{{\\rm a}}=\\mathbf{x}_{i}^{{\\rm f}}+\\mathbf{K}\\left[\\mathbf{y}_{i}-\\mathcal{H}(\\mathbf{x}^{\\mathrm{f}}_{i})\\right], \\tag{8}\\] where\n\n\\(i=1,\\ldots,m\\) is the member index in the ensemble,\n\\(\\mathbf{x}^{\\mathrm{f}}_{i}\\) is the forecast state vector \\(i\\), which represents a background state or prior at the analysis time.\n\nTo mimic the Kalman filter, \\(\\mathbf{K}\\) must be identified with the Kalman gain \\[\\mathbf{K}=\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}{\\mathbf{H}\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R}}^{-1},\\label{eq:kalman-gain}\\] that we wish to estimate from the ensemble statistics."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-15",
    "href": "slides/week-05/07-DA-stat.html#section-15",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "First, we compute the forecast error covariance matrix as a sum over the ensemble, \\[\\mathbf{P}^{\\mathrm{f}}=\\frac{1}{m-1}\\sum_{i=1}^{m}\\left({\\mathbf{x}_{i}^{{\\rm f}}-\\overline{\\mathbf{x}}^{{\\rm f}}}\\right)\\left({\\mathbf{x}_{i}^{{\\rm f}}-\\overline{\\mathbf{x}}^{{\\rm f}}}\\right)^{\\mathrm{T}},\\] with \\(\\overline{\\mathbf{x}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}_{i}^{{\\rm f}}.\\)\nThe forecast error covariance matrix can be factorized into \\[\\mathbf{P}^{\\mathrm{f}}=\\mathbf{X}_{\\mathrm{f}}\\mathbf{X}_{\\mathrm{f}}^{\\mathrm{T}},\\] where \\(\\mathbf{X}_{\\mathrm{f}}\\) is a \\(n\\times m\\) matrix whose columns are the normalized anomalies or normalized perturbations , i.e. for \\(i=1,\\ldots,m\\) \\[\\left[\\mathbf{X}_{\\mathrm{f}}\\right]_{i}=\\frac{\\mathbf{x}_{i}^{{\\rm f}}-\\overline{\\mathbf{x}}^{{\\rm f}}}{\\sqrt{m-1}}.\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-16",
    "href": "slides/week-05/07-DA-stat.html#section-16",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "We can now obtain from Equation 8 a posterior ensemble \\(\\left\\{ \\mathbf{x}_{i}^{{\\rm a}}\\right\\} _{i=1,\\ldots,m}\\) from which we can compute the posterior statistics.\nHence, the posterior state and an ensemble of posterior perturbations can be estimated from \\[\\overline{\\mathbf{x}}^{{\\rm a}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}^{\\mathrm{a}}_{i}\\,,\\quad\\left[\\mathbf{X}_{\\mathrm{a}}\\right]_{i}=\\frac{\\mathbf{x}_{i}^{{\\rm a}}-\\overline{\\mathbf{x}}^{{\\rm a}}}{\\sqrt{m-1}}.\\]\nSince \\(\\mathbf{y}_{i}\\equiv\\mathbf{y}\\) was assumed, the normalized anomalies, \\(\\mathbf{X}_{i}^{{\\rm a}}\\equiv\\left[\\mathbf{X}_{\\mathrm{a}}\\right]_{i}\\), i.e. the normalized deviations of the ensemble members from the mean are obtained from Equation 8 minus the mean update, \\[\\mathbf{X}_{i}^{{\\rm a}}=\\mathbf{X}_{i}^{{\\rm f}}+\\mathbf{K}\\left(\\mathbf{0}-\\mathbf{H}\\mathbf{X}_{i}^{{\\rm f}}\\right)=\\left(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H}\\right)\\mathbf{X}_{i}^{{\\rm f}},\\label{eq:anomaly-update}\\]\nwhere \\(\\mathbf{X}_{i}^{{\\rm f}}\\equiv\\left[\\mathbf{X}_{\\mathrm{f}}\\right]_{i}\\), which yields the analysis error covariance matrix,"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-17",
    "href": "slides/week-05/07-DA-stat.html#section-17",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n        \\mathbf{P}^{{\\rm a}} & =\\mathbf{X}_{\\mathrm{a}}\\mathbf{X}_{\\mathrm{a}}^{\\mathrm{T}}\\\\\n         & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{X}_{\\mathrm{f}}\\mathbf{X}_{\\mathrm{f}}^{\\mathrm{T}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}\\\\\n         & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}.\n        \\end{aligned}\\]\nNote that such a computation is never carried out in practice. However, theoretically, in order to mimic the best linear unbiased estimator (BLUE) analysis of the Kalman filter, we should have obtained \\[\\begin{aligned}\n    \\mathbf{P}^{{\\rm a}} & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}+\\mathbf{K}\\mathbf{R}\\mathbf{K}^{\\mathrm{T}}\\\\\n     & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}.\n    \\end{aligned}\\]\n\nTherefore, the error covariances are underestimated since the second positive term, related to the observation errors, is ignored, which is likely to lead to the divergence of the EnKF when the scheme is cycled.\n\nAn elegant solution around this problem is to perturb the observation vector for each member: \\(\\mathbf{y}_{i}=\\mathbf{y}+\\mathbf{u}_{i}\\), where \\(\\mathbf{u}_{i}\\) is drawn from the Gaussian distribution \\(\\mathbf{u}_{i}\\sim N(\\mathbf{0},\\mathbf{R}).\\)"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-18",
    "href": "slides/week-05/07-DA-stat.html#section-18",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Let us define \\(\\overline{\\mathbf{u}}\\) the mean of the sampled \\(\\mathbf{u}_{i}\\), and the innovation perturbations \\[\\left[\\mathbf{Y}_{\\mathrm{f}}\\right]_{i}=\\frac{\\mathbf{H}\\mathbf{x}_{i}^{{\\rm f}}-\\mathbf{u}_{i}-\\mathbf{H}\\overline{\\mathbf{x}}^{{\\rm f}}+\\overline{\\mathbf{u}}}{\\sqrt{m-1}}.\\label{eq:innovation-pert}\\]\nThe posterior anomalies are modified accordingly, \\[\\mathbf{X}_{i}^{{\\rm a}}=\\mathbf{X}_{i}^{{\\rm f}}-\\mathbf{K}\\mathbf{Y}_{i}^{{\\rm f}}=(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{X}_{i}^{{\\rm f}}+\\frac{\\mathbf{K}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})}{\\sqrt{m-1}}.\\label{eq:anomaly-update-correction}\\]\n\nThese anomalies yield the analysis error covariance matrix,"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-19",
    "href": "slides/week-05/07-DA-stat.html#section-19",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\mathbf{P}^{{\\rm a}}= & (\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}\n      +\\mathbf{K}\\left[\\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})^{\\mathrm{T}}\\right]\\mathbf{K}^{\\mathrm{T}}\\\\\n     & +\\frac{1}{\\sqrt{m-1}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})^{\\mathrm{T}}\\mathbf{K}^{\\mathrm{T}}\\\\\n     & +\\frac{1}{\\sqrt{m-1}}\\mathbf{K}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}},\n    \\end{aligned}\\] whose expectation over the random noise gives the proper expected posterior covariances, \\[\\begin{aligned}\n    \\mathrm{E}\\left[\\mathbf{P}^{{\\rm a}}\\right] & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}\n      +\\mathbf{K}\\mathrm{E}\\left[\\frac{1}{m-1}\\sum_{i=1}^{m}(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})(\\mathbf{u}_{i}-\\overline{\\mathbf{u}})^{\\mathrm{T}}\\right]\\mathbf{K}^{\\mathrm{T}}\\\\\n     & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})^{\\mathrm{T}}+\\mathbf{K}\\mathbf{R}\\mathbf{K}\\\\\n     & =(\\mathbf{I}_{n}-\\mathbf{K}\\mathbf{H})\\mathbf{P}^{\\mathrm{f}}.\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-20",
    "href": "slides/week-05/07-DA-stat.html#section-20",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Note that the gain can be formulated in terms of the anomaly matrices only, \\[\\mathbf{K}=\\mathbf{X}_{\\mathrm{f}}\\mathbf{Y}_{\\mathrm{f}}^{\\mathrm{T}}\\left(\\mathbf{Y}_{\\mathrm{f}}\\mathbf{Y}_{\\mathrm{f}}^{\\mathrm{T}}\\right)^{-1},\\label{eq:kalman-gain-pert}\\] since\n\n\\(\\mathbf{X}_{\\mathrm{f}}\\mathbf{Y}_{\\mathrm{f}}^{\\mathrm{T}}\\) is a sample estimate for \\(\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}\\) and\n\\(\\mathbf{Y}_{\\mathrm{f}}\\mathbf{Y}_{{\\rm f}}^{\\mathrm{T}}\\) is a sample estimate for \\(\\mathbf{H}\\mathbf{P}^{\\mathrm{f}}\\mathbf{H}^{\\mathrm{T}}+\\mathbf{R}.\\)\n\nIn this form, it is striking that the updated perturbations are linear combinations of the forecast perturbations. The new perturbations are sought within the ensemble subspace of the initial perturbations.\nSimilarly, the state analysis is sought within the affine space \\(\\overline{\\mathbf{x}}^{{\\rm f}}+\\mathrm{vec}\\left(\\mathbf{X}_{1}^{{\\rm f}},\\mathbf{X}_{2}^{{\\rm f}},\\ldots,\\mathbf{X}_{m}^{{\\rm f}}\\right).\\)"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-21",
    "href": "slides/week-05/07-DA-stat.html#section-21",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Note\n\n\nIn practice \\(\\mathbf{YY}^T\\) is not invertible since it is a low-rank approximation of \\(\\mathbf{HBH}^T + \\mathbf{R}\\).\nTo make the inversion feasible, we define \\(\\mathbf{YY}^T\\) to not include the noise samples \\(\\mathbf{u}_i\\) and add a full-rank \\(\\mathbf{R}\\) to the covariance estimate \\(\\mathbf{YY}^T\\) in the inversion yielding\n\\[\\begin{equation}\n    \\mathbf{XY}^T (\\mathbf{YY}^T + \\mathbf{R})^{-1}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#enkf-forecast-step",
    "href": "slides/week-05/07-DA-stat.html#enkf-forecast-step",
    "title": "Statistical Data Assimilation",
    "section": "EnKF — Forecast Step",
    "text": "EnKF — Forecast Step\nIn the forecast step, the updated ensemble obtained at the analysis step is propagated by the model over a time step, \\[\\mbox{for}\\quad i=1,\\ldots,m\\quad\\mathbf{x}_{i,k+1}^{{\\rm f}}=\\mathcal{M}_{k+1}(\\mathbf{x}^{\\mathrm{a}}_{i,k}).\\]\nA forecast can be computed from the mean of the forecast ensemble, while the forecast error covariances can be estimated from the forecast perturbations.\n\n\n\n\n\n\nNote\n\n\n\nThese are only optional diagnostics in the scheme and they are not required in the cycling of the EnKF.\nIt is important to observe that using the tangent linear model (TLM) operator, or any linearization thereof, was avoided.\nThis difference should particularly matter in a significantly nonlinear regime.\nHowever, as we shall see later, in strongly nonlinear regimes, the EnKF is largely dominated by schemes known as the iterative EnKF and the iterative ensemble Kalman smoother"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#comparison-enkf-and-4d-var",
    "href": "slides/week-05/07-DA-stat.html#comparison-enkf-and-4d-var",
    "title": "Statistical Data Assimilation",
    "section": "Comparison: EnKf and 4D-Var",
    "text": "Comparison: EnKf and 4D-Var\n\n\n\nPrinciple of data assimilation: Having a physical model able to forecast the evolution of a system from time \\(t=t_{0}\\) to time\\(t=T_{f}\\) (cyan curve), the aim of DA is to use available observations (blue triangles) to correct the model projections and get closer to the (unknown) truth (dotted line).\nIn EnKFs, the initial system state and its uncertainty (green square and ellipsoid) are represented by \\(m\\) members."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-22",
    "href": "slides/week-05/07-DA-stat.html#section-22",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "The members are propagated forward in time during \\(n_{1}\\) model time steps \\(dt\\) to \\(t=T_{1}\\) where observations are available (forecast phase, orange dashed lines).\nAt \\(t=T_{1}\\) the analysis uses the observations and their uncertainty (blue triangle and ellipsoid) to produce a new system state that is closer to the observations and with a lower uncertainty (red square and ellipsoid).\nA new forecast is issued from the analyzed state and this procedure is repeated until the end of the assimilation window at \\(t=T_{f}.\\)\nThe model state should get closer to the truth and with lower uncertainty as more observations are assimilated.\n\nTime-dependent variational methods (4D-Var) iterate over the assimilation window to find the trajectory that minimizes the misfit (\\(J_{0}\\)) between the model and all observations available from \\(t_{0}\\) to \\(T_{f}\\) (violet curve).\nFor linear dynamics, Gaussian errors and infinite ensemble sizes, the states produced at the end of the assimilation window by the two methods should be equivalent (Li and Navon 2001)."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#sec-alg-EnKG",
    "href": "slides/week-05/07-DA-stat.html#sec-alg-EnKG",
    "title": "Statistical Data Assimilation",
    "section": "EnKF — the Algorithm",
    "text": "EnKF — the Algorithm\n\n\n\nGiven: For \\(k=0,\\ldots,K,\\) observation error cov. matrices\n            \\(\\mathbf{R}_{k}\\), observation models \\(\\mathcal{H}_{k}\\), forward models \\(\\mathcal{M}_{k}.\\)  \nCompute: the ensemble forecast \\(\\left\\{ \\mathbf{x}_{i,k}^{\\mathrm{f}}\\right\\} _{i=1,\\ldots,m,\\,k=1,\\ldots,K}\\) \n\\(\\left(\\mathbf{x}_{i,0}^{\\mathrm{f}}\\right)_{i=1,\\ldots,m}\\)            %Initialize the ensemble\nfor \\(k=0\\) to \\(K\\) do        %Loop over time\n  for  \\(i=1\\) to \\(m\\) do #Draw a stat. consistent obs. set\n    \\(\\mathbf{u}_{i}\\sim{\\cal N}(0,\\mathbf{R}_{k})\\)\n    \\(\\mathbf{y}_{i,k}=\\mathbf{y}_{k}+\\mathbf{u}_{i}\\)\n  end for\n\n\n\n% Compute the ensemble means\n  \\(\\overline{\\mathbf{x}}_{k}^{\\mathrm{f}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{x}^{\\mathrm{f}}_{i,k}\\,,\\overline{\\mathbf{u}}=\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{u}_{i}\\) \n  \\(\\left[\\mathbf{X}_{\\mathrm{f}}\\right]_{i,k}=\\frac{\\mathbf{x}_{i,k}^{\\mathrm{f}}-\\overline{\\mathbf{x}}_{k}^{\\mathrm{f}}}{\\sqrt{m-1}},\\)   %Compute the normalized anomalies\n  \\(\\left[\\mathbf{Y}_{\\mathrm{f}}\\right]_{i,k}=\\frac{\\mathbf{H}_{k}\\mathbf{x}_{i,k}^{\\mathrm{f}}-\\mathbf{u}_{i}-\\mathbf{H}_{k}\\overline{\\mathbf{x}}_{k}^{\\mathrm{f}}+\\overline{\\mathbf{u}}}{\\sqrt{m-1}}\\)\n   \\(K_{k}=\\mathbf{X}_{k}^{\\mathrm{f}}\\left({\\mathbf{Y}_{k}^{\\mathrm{f}}}\\right)^{\\mathrm{T}}\\left(\\mathbf{Y}_{k}^{\\mathrm{f}}\\left({\\mathbf{Y}_{k}^{\\mathrm{f}}}\\right)^{\\mathrm{T}}\\right)^{-1}\\) % gain\n  for \\(i=1\\) to \\(m\\) do              % Update the ensemble\n    \\(\\mathbf{x}_{i,k}^{{\\rm a}}=\\mathbf{x}_{i,k}^{\\mathrm{f}}+K_{k}\\left({\\mathbf{y}_{i,k}-\\mathcal{H}_{k}\\left(\\mathbf{x}^{\\mathrm{f}}_{i,k}\\right)}\\right)\\) \n    \\({\\displaystyle \\mathbf{x}_{i,k+1}^{\\mathrm{f}}=\\mathcal{M}_{k+1}\\left(\\mathbf{x}^{\\mathrm{a}}_{i,k}\\right)}\\)          % Ensemble forecast\n  end for\nend for"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#localization-and-inflation",
    "href": "slides/week-05/07-DA-stat.html#localization-and-inflation",
    "title": "Statistical Data Assimilation",
    "section": "Localization and Inflation",
    "text": "Localization and Inflation\nWe have traded the extended Kalman filter for a seemingly considerably cheaper filter meant to achieve similar performances.\nBut this comes with significant drawbacks.\n\nFundamentally, one cannot hope to represent the full error covariance matrix of a complex high-dimensional system with only a few modes \\(m\\ll n\\), usually from a few dozens to a few hundreds.\nThis implies large sampling errors, meaning that the error covariance matrix is only sampled by a limited number of modes.\nThis rank-deficiency is accompanied by spurious correlations at long distances that strongly affect the filter performance.\nEven though the unstable degrees of freedom of dynamical systems that we wish to control with a filter are usually far fewer than the dimension of the system, they often still represent a substantial fraction of the total degrees of freedom. Forecasting an ensemble of such size is usually not affordable."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-23",
    "href": "slides/week-05/07-DA-stat.html#section-23",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "The consequence of this issue always is the divergence of the filter.\nHence, the EnKF is useful on the condition that efficient fixes are applied.\n\nTo make it a viable algorithm, one first needs to cope with the rank-deficiency of the filter and with its manifestations, i.e. sampling errors.\nFortunately, there are clever tricks to overcome this major issue, known as localization and inflation, which explains, ultimately, the broad success of the EnKF in geosciences and engineering."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#localization",
    "href": "slides/week-05/07-DA-stat.html#localization",
    "title": "Statistical Data Assimilation",
    "section": "Localization",
    "text": "Localization\nFor many systems with geographic spread, distant observables are weakly correlated.\nIn other words, two distant parts of the system are almost independent at least for short time scales.\nIt is possible to exploit this relative independence and spatially localize the analysis. This has been naturally termed localization.\nThere are two types of localization:\n\nDomain localization, where instead of performing a global analysis valid at any location in the domain, we perform a local analysis to update the local state variables using local observations.\nCovariance localization focuses on the forecast error covariance matrix. It is based on the remark that the forecast error covariance matrix \\(\\mathbf{P}_{{\\rm f}}\\) is of low rank, at most \\(m-1,\\) and that this rank-deficiency could be cured by filtering these empirical covariances.\n\nFor implementation details, please consult (Asch, Bocquet, and Nodet 2016)."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#inflation",
    "href": "slides/week-05/07-DA-stat.html#inflation",
    "title": "Statistical Data Assimilation",
    "section": "Inflation",
    "text": "Inflation\nEven when the analysis is made local, the error covariance matrices are still evaluated with an ensemble of limited size.\n\nThis often leads to sampling errors and spurious correlations.\nWith a proper localization scheme, they might be significantly reduced.\nHowever small are the residual errors, they will accumulate and they will carry over to the next cycles of the sequential EnKF scheme. As a consequence, there is always a risk that the filter may ultimately diverge.\n\nOne way around is to inflate the error covariance matrix by a factor \\(\\lambda^{2}\\) slightly greater than \\(1\\) before or after the analysis.\n\nFor instance, after the analysis, \\[\\mathbf{P}^{\\rm a}\\longrightarrow\\lambda^{2}\\mathbf{P}^{\\mathrm{a}}.\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-24",
    "href": "slides/week-05/07-DA-stat.html#section-24",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Another way to achieve this is to inflate the ensemble, \\[\\mathbf{x}^{\\mathrm{a}}_{i}\\longrightarrow\\overline{\\mathbf{x}}^{{\\rm a}}+\\lambda{\\mathbf{x}^{\\mathrm{a}}_{i}-\\overline{\\mathbf{x}}^{{\\rm a}}},\\] which can alternatively be enforced on the prior (forecast) ensemble. This type of inflation is called multiplicative inflation.\n\nFor implementation details, please consult (Asch, Bocquet, and Nodet 2016)."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-25",
    "href": "slides/week-05/07-DA-stat.html#section-25",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "EXAMPLES"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-26",
    "href": "slides/week-05/07-DA-stat.html#section-26",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "As in the previous Lecture, we consider the same scalar 4D-Var example, but this time apply the Kalman filter to it.\nWe take the most simple linear forecast model, \\[\\frac{\\mathrm{d}x}{\\mathrm{d}t}=-\\alpha x,\\] with \\(\\alpha\\) a known positive constant.\nWe assume the same discrete dynamics considered in with a single observation at time step \\(3.\\)\nThe stochastic system 1-2 is \\[\\begin{aligned}\n    x_{k+1}^{\\mathrm{t}} & =M(x_{k}^{\\mathrm{t}})+w_{k},\\\\\n    y_{k+1} & =x_{k}^{\\mathrm{t}}+v_{k},\n    \\end{aligned}\\] where \\(w_{k}\\thicksim\\mathcal{N}(0,\\sigma_{Q}^{2}),\\) \\(v_{k}\\thicksim\\mathcal{N}(0,\\sigma_{R}^{2})\\) and \\(x_{0}^{\\mathrm{t}}-x_{0}^{\\mathrm{b}}\\thicksim\\mathcal{N}(0,\\sigma_{B}^{2}).\\)"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-27",
    "href": "slides/week-05/07-DA-stat.html#section-27",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "The Kalman filter steps are\nForecast:\n\\[\\begin{aligned}\nx_{k+1}^{\\mathrm{f}} & =M(x_{k}^{\\mathrm{a}})=\\gamma x_{k},\\\\\nP_{k+1}^{\\mathrm{f}} & =\\gamma^{2}P_{k}^{\\mathrm{a}}+\\sigma_{Q}^{2}.\n\\end{aligned}\\]\nAnalysis:\n\\[\\begin{aligned}\nK_{k+1} & =P_{k+1}^{\\mathrm{f}}H\\left(H^{2}P_{k+1}^{\\mathrm{f}}+\\sigma_{R}^{2}\\right)^{-1},\\\\\nx_{k+1}^{\\mathrm{a}} & =x_{k+1}^{\\mathrm{f}}+K_{k+1}(x_{k+1}^{\\mathrm{o}}-Hx_{k+1}^{\\mathrm{f}}),\\\\\nP_{k+1}^{\\mathrm{a}} & =(1-K_{k+1}H)P_{k+1}^{\\mathrm{f}}=\\left(\\frac{1}{P_{k+1}^{\\mathrm{f}}}+\\frac{1}{\\sigma_{R}^{2}}\\right)^{-1},\\quad H=1.\n\\end{aligned}\\]\nInitialization:\n\\[\\begin{equation}\nx_{0}^{\\mathrm{a}}  =x_{0}^{\\mathrm{b}},\\quad\nP_{0}^{\\mathrm{a}}  =\\sigma_{B}^{2}.\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-28",
    "href": "slides/week-05/07-DA-stat.html#section-28",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "We start with the initial state, at time step \\(k=0.\\) The initial conditions are as above. The forecast is \\[\\begin{aligned}\n    x_{1}^{\\mathrm{f}} & =M(x_{0}^{\\mathrm{a}})=\\gamma x_{0}^{\\mathrm{b}},\\\\\n    P_{1}^{\\mathrm{f}} & =\\gamma^{2}\\sigma_{B}^{2}+\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nSince there is no observation available, \\(H=0,\\) and the analysis gives,\n\\[\\begin{aligned}\n    K_{1} & =0,\\\\\n    x_{1}^{\\mathrm{a}} & =x_{1}^{\\mathrm{f}}=\\gamma x_{0}^{\\mathrm{b}},\\\\\n    P_{1}^{\\mathrm{a}} & =P_{1}^{\\mathrm{f}}=\\gamma^{2}\\sigma_{B}^{2}+\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nAt the next time step, \\(k=1,\\) and the forecast gives\n\\[\\begin{aligned}\n    x_{2}^{\\mathrm{f}} & =M(x_{1}^{\\mathrm{a}})=\\gamma^{2}x_{0}^{\\mathrm{b}},\\\\\n    P_{2}^{\\mathrm{f}} & =\\gamma^{2}P_{1}^{\\mathrm{a}}+\\sigma_{Q}^{2}=\\gamma^{4}\\sigma_{B}^{2}+(\\gamma^{2}+1)\\sigma_{Q}^{2}.\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-29",
    "href": "slides/week-05/07-DA-stat.html#section-29",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Once again there is no observation available, \\(H=0,\\) and the analysis yields \\[\\begin{aligned}\n    K_{2} & =0,\\\\\n    x_{2}^{\\mathrm{a}} & =x_{2}^{\\mathrm{f}}=\\gamma^{2}x_{0}^{\\mathrm{b}},\\\\\n    P_{2}^{\\mathrm{a}} & =P_{2}^{\\mathrm{f}}=\\gamma^{4}\\sigma_{B}^{2}+(\\gamma^{2}+1)\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nMoving on to \\(k=2,\\) we have the new forecast, \\[\\begin{aligned}\n    x_{3}^{\\mathrm{f}} & =M(x_{2}^{\\mathrm{a}})=\\gamma^{3}x_{0}^{\\mathrm{b}},\\\\\n    P_{3}^{\\mathrm{f}} & =\\gamma^{2}P_{2}^{\\mathrm{a}}+\\sigma_{Q}^{2}=\\gamma^{6}\\sigma_{B}^{2}+(\\gamma^{4}+\\gamma^{2}+1)\\sigma_{Q}^{2}.\n    \\end{aligned}\\]\nNow there is an observation, \\(x_{3}^{\\mathrm{o}},\\) available, so \\(H=1\\) and the analysis is \\[\\begin{aligned}\n    K_{3} & =P_{3}^{\\mathrm{f}}\\left(P_{3}^{\\mathrm{f}}+\\sigma_{R}^{2}\\right)^{-1},\\\\\n    x_{3}^{\\mathrm{a}} & =x_{3}^{\\mathrm{f}}+K_{3}(x_{3}^{\\mathrm{o}}-x_{3}^{\\mathrm{f}}),\\\\\n    P_{3}^{\\mathrm{a}} & =(1-K_{3})P_{3}^{\\mathrm{f}}.\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-30",
    "href": "slides/week-05/07-DA-stat.html#section-30",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Substituting and simplifying, we find \\[x_{3}^{\\mathrm{a}}=\\gamma^{3}x_{0}^{\\mathrm{b}}+\\frac{\\gamma^{6}\\sigma_{B}^{2}+(\\gamma^{4}+\\gamma^{2}+1)\\sigma_{Q}^{2}}{\\sigma_{R}^{2}+\\gamma^{6}\\sigma_{B}^{2}+(\\gamma^{4}+\\gamma^{2}+1)\\sigma_{Q}^{2}}\\left(x_{3}^{\\mathrm{o}}-\\gamma^{3}x_{0}^{\\mathrm{b}}\\right).\\label{eq:saclarKF_xa} \\tag{9}\\]\nCase 1: Assume we have a perfect model, then \\(\\sigma_{Q}^{2}=0\\) and the Kalman filter state 9 becomes \\[x_{3}^{\\mathrm{a}}=\\gamma^{3}x_{0}^{\\mathrm{b}}+\\frac{\\gamma^{6}\\sigma_{B}^{2}}{\\sigma_{R}^{2}+\\gamma^{6}\\sigma_{B}^{2}}\\left(x_{3}^{\\mathrm{o}}-\\gamma^{3}x_{0}^{\\mathrm{b}}\\right),\\] which is precisely the 4D-Var expression obtained before.\nCase 2: When the parameter \\(\\alpha\\) tends to zero, then \\(\\gamma\\) tends to one, the model is stationary and the Kalman filter state 9 becomes \\[x_{3}^{\\mathrm{a}}=x_{0}^{\\mathrm{b}}+\\frac{\\sigma_{B}^{2}+3\\sigma_{Q}^{2}}{\\sigma_{R}^{2}+\\sigma_{B}^{2}+3\\sigma_{Q}^{2}}\\left(x_{3}^{\\mathrm{o}}-x_{0}^{\\mathrm{b}}\\right),\\]"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-31",
    "href": "slides/week-05/07-DA-stat.html#section-31",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "which, when \\(\\sigma_{Q}^{2}=0,\\) reduces to the 3D-Var solution, \\[x_{3}^{\\mathrm{a}}=x_{0}^{\\mathrm{b}}+\\frac{\\sigma_{B}^{2}}{\\sigma_{R}^{2}+\\sigma_{B}^{2}}\\left(x_{3}^{\\mathrm{o}}-x_{0}^{\\mathrm{b}}\\right),\\] that was obtained before.\nCase 3: When \\(\\alpha\\) tends to infinity, then \\(\\gamma\\) goes to zero, and we are in the case where there is no longer any memory with \\[x_{3}^{\\mathrm{a}}=\\frac{\\sigma_{Q}^{2}}{\\sigma_{R}^{2}+\\sigma_{Q}^{2}}x_{3}^{\\mathrm{o}}.\\] Then, if the model is perfect, \\(\\sigma_{Q}^{2}=0\\) and \\(x_{3}^{\\mathrm{a}}=0.\\) If the observation is perfect, \\(\\sigma_{R}^{2}=0\\) and \\(x_{3}^{\\mathrm{a}}=x_{3}^{\\mathrm{o}}.\\)\nThis example shows the complete chain, from the Kalman filter solution, through the 4D-Var, and finally reaching the 3D-Var one. Hopefully this clarifies the relationship between the three and demonstrates why the Kalman filter provides the most general solution possible."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-32",
    "href": "slides/week-05/07-DA-stat.html#section-32",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "PRACTICAL GUIDELINES"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#general-guidelines",
    "href": "slides/week-05/07-DA-stat.html#general-guidelines",
    "title": "Statistical Data Assimilation",
    "section": "General Guidelines",
    "text": "General Guidelines\nWe briefly point out some important practical considerations. It should now be clear that there are four basic ingredients in any inverse or data assimilation problem:\n\nObservation or measured data.\nA forward or direct model of the real-world context.\nA backwards or adjoint model, in the variational case. A probabilistic framework, in the statistical case.\nAn optimization cycle.\n\nBut where does one start?\nThe traditional approach, often employed in mathematical and numerical modeling, is to begin with some simplified, or at least well-known, situation.\nOnce the above four items have been successfully implemented and tested on this instance, we then proceed to take into account more and more reality in the form of real data, more realistic models, more robust optimization procedures, etc."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-33",
    "href": "slides/week-05/07-DA-stat.html#section-33",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "In other words, we introduce uncertainty, but into a system where we at least control some of the aspects.\nTwin experiments, or synthetic runs, are a basic and indispensable tool for all inverse problems. In order to evaluate the performance of a data assimilation system we invariably begin with the following methodology.\n\nFix all parameters and unknowns and define a reference trajectory, obtained from a run of the direct model call this the “truth”.\nDerive a set of (synthetic) measurements, or background data, from this “true” run.\nOptionally, perturb these observations in order to generate a more realistic observed state.\nRun the data assimilation or inverse problem algorithm, starting from an initial guess (different from the “true” initial state used above), using the synthetic observations.\nEvaluate the performance, modify the model/algorithm/observations, and cycle back to step 1."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-34",
    "href": "slides/week-05/07-DA-stat.html#section-34",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Twin experiments thus provide a well-structured methodological framework.\nWithin this framework we can perform different “stress tests” of our system.\nWe can modify the observation network,\n\nincrease or decrease (even switch off) the uncertainty,\ntest the robustness of the optimization method,\neven modify the model.\n\nIn fact, these experiments can be performed on the full physical model, or on some simpler (or reduced-order) model.\nToy models are, by definition, simplified models that we can play with. Yes, but these are of course “serious games.” In certain complex physical contexts, of which meteorology is a famous example, we have well-established toy models, often of increasing complexity. These can be substituted for the real model, whose computational complexity is often too large, and provide a cheaper test-bed."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#section-35",
    "href": "slides/week-05/07-DA-stat.html#section-35",
    "title": "Statistical Data Assimilation",
    "section": "",
    "text": "Some well-known examples of toy models are:\n\nLorenz models that are used as an avatar for weather simulations.\nVarious harmonic oscillators that are used to simulate dynamic systems.\nOther well-known models are the Ising model in physics, the Lotka-Volterra model in life sciences, and the Schelling model in social sciences.\n\nMachine Learning (ML) is becoming more and more present in our daily lives, and in scientific research. The use of ML in DA and Inverse modeling will be dealt with later, where we will consider:\n\nML-based Surrogate Models with Fourier Neural Operators for the dynamics.\nML-based Surrogate Models with Conditional Normalizing flows for the posterior.\nScientific ML."
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#where-are-we-in-the-inference-cycle",
    "href": "slides/week-05/07-DA-stat.html#where-are-we-in-the-inference-cycle",
    "title": "Statistical Data Assimilation",
    "section": "Where are we in the Inference cycle",
    "text": "Where are we in the Inference cycle\n\n\nFigure 4: DA is the inductive phase of DTs"
  },
  {
    "objectID": "slides/week-05/07-DA-stat.html#open-source-software",
    "href": "slides/week-05/07-DA-stat.html#open-source-software",
    "title": "Statistical Data Assimilation",
    "section": "Open-source software",
    "text": "Open-source software\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen."
  },
  {
    "objectID": "slides/week-09/12.html",
    "href": "slides/week-09/12.html",
    "title": "Digital Twins for Physical Systems",
    "section": "",
    "text": "ReuseCC BY 4.0"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#bayesian-filters",
    "href": "slides/week-06/08-DA-Bayes.html#bayesian-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Bayesian filters",
    "text": "Bayesian filters\nWe begin by defining a probabilistic state-space, or nonlinear filtering model, of the form \\[\\begin{aligned}\n    \\mathbf{x}_{k} & \\sim p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\label{eq:filter1}\\\\\n    \\mathbf{y}_{k} & \\sim p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}),\\quad k=0,1,2,\\ldots,\\label{eq:filter2}\n    \\end{aligned} \\tag{1}\\] where\n\n\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) is the state vector at time \\(k,\\)\n\\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}\\) is the observation vector at time \\(k,\\)\nthe conditional probability, \\(p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\) represents the stochastic dynamics model, and can be a probability density or a discrete probability function, or a mixture of both,\nthe conditional probability, \\(p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}),\\) represents the measurement model and its inherent noise."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section",
    "href": "slides/week-06/08-DA-Bayes.html#section",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "In addition, we assume that the model is Markovian, such that \\[p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{1:k-1},\\mathbf{y}_{1:k-1})=p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\] and that the observations are conditionally independent of state and measurement histories, \\[p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{1:k},\\mathbf{y}_{1:k-1})=p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}).\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#example-of-gaussian-random-walk",
    "href": "slides/week-06/08-DA-Bayes.html#example-of-gaussian-random-walk",
    "title": "Bayesian Data Assimilation",
    "section": "Example of Gaussian Random Walk",
    "text": "Example of Gaussian Random Walk\nTo fix ideas and notation, we begin with a very simple, scalar case, the Gaussian random walk model. This model can then easily be generalized.\nConsider the scalar system \\[\\begin{aligned}\n    x_{k} & =x_{k-1}+w_{k-1},\\quad w_{k-1}\\sim\\mathcal{N}(0,Q),\\label{eq:GRW-x}\\\\\n    y_{k} & =x_{k}+v_{k},\\quad v_{k}\\sim\\mathcal{N}(0,R),\\label{eq:GRW-y}\n    \\end{aligned} \\tag{2}\\] where \\(x_{k}\\) is the (hidden) state and \\(y_{k}\\) is the (known) measurement.\nNoting that \\(x_{k}-x_{k-1}=w_{k-1}\\) and that \\(y_{k}-x_{k}=v_{k},\\) we can immediately rewrite this system in terms of the conditional probability densities, \\[\\begin{aligned}\n    p(x_{k}\\mid x_{k-1}) & =\\mathcal{N}\\left(x_{k}\\mid x_{k-1},Q\\right)\\\\\n     & =\\dfrac{1}{\\sqrt{2\\pi Q}}\\exp\\left[-\\frac{1}{2Q}\\left(x_{k}-x_{k-1}\\right)^{2}\\right]\n    \\end{aligned}\\] and"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-1",
    "href": "slides/week-06/08-DA-Bayes.html#section-1",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    p(y_{k}\\mid x_{k}) & =\\mathcal{N}\\left(y_{k}\\mid x_{k},R\\right)\\\\\n     & =\\dfrac{1}{\\sqrt{2\\pi R}}\\exp\\left[-\\frac{1}{2R}\\left(y_{k}-x_{k}\\right)^{2}\\right].\n    \\end{aligned}\\]\nA realization of the model is shown in Figure 1.\n\n\nFigure 1: Gaussian random walk state space model equations 2. State, \\(x_k\\) is solid blue curve, measurements, \\(y_k\\), are red circles. Fixed values of noise variance are \\(Q=1\\) and \\(R=1\\)"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#code",
    "href": "slides/week-06/08-DA-Bayes.html#code",
    "title": "Bayesian Data Assimilation",
    "section": "Code",
    "text": "Code\n% Simulate a Gaussian random walk.\n% initialize\nrandn('state',123)\nR=1; Q=1; K=100;\n% simulate\nX_init = sqrt(Q)\\*randn(K,1);\nX = cumsum(X_init);\nW = sqrt(R)\\*randn(K,1);\nY = X + W;\n% plot\nplot(1:K,X,1:K,Y(1:K,1),'ro')\nxlabel('k'), ylabel('x_k')"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#nonlinear-filter-model",
    "href": "slides/week-06/08-DA-Bayes.html#nonlinear-filter-model",
    "title": "Bayesian Data Assimilation",
    "section": "Nonlinear Filter Model",
    "text": "Nonlinear Filter Model\nUsing the nonlinear filtering model 1 and the Markov property, we can express the joint prior of the states, \\(\\mathbf{x}_{0:T}=\\{\\mathbf{x}_{0},\\ldots,\\mathbf{x}_{T}\\},\\) and the joint likelihood of the measurements, \\(\\mathbf{y}_{1:T}=\\{\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{T}\\},\\) as the products \\[p(\\mathbf{x}_{0:T})=p(\\mathbf{x}_{0})\\prod_{k=1}^{T}p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1})\\] and \\[p(\\mathbf{y}_{1:T}\\mid\\mathbf{x}_{0:T})=\\prod_{k=1}^{T}p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})\\] respectively."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-2",
    "href": "slides/week-06/08-DA-Bayes.html#section-2",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Then, applying Bayes’ law, we can compute the complete posterior distribution of the states as \\[p(\\mathbf{x}_{0:T}\\mid\\mathbf{y}_{1:T})=\\frac{p(\\mathbf{y}_{1:T}\\mid\\mathbf{x}_{0:T})p(\\mathbf{x}_{0:T})}{p(\\mathbf{y}_{1:T})}.\\label{eq:Bayes-post-state-eq}\\]\nBut this type of complete characterization is not feasible to compute in real-time, or near real-time, since the number of computations per time-step increases as measurements arrive.\nWhat we need is a fixed number of computations per time-step.\n\nThis can be achieved by a recursive estimation that, step by step, produces the filtering distribution defined above.\nIn this light, we can now define the general Bayesian filtering problem, of which Kalman filters will be a special case."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-3",
    "href": "slides/week-06/08-DA-Bayes.html#section-3",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Definition 1 Bayesian filtering is the recursive computation of the marginal posterior distribution, \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\] known as the filtering distribution, of the state \\(\\mathbf{x}_{k}\\) at each time step \\(k,\\) given the measurements up to time \\(k.\\)\n\nNow, based on Bayes’ rule, we can formulate the Bayesian filtering theorem (Särkkä and Svensson 2023) .\n\nTheorem 1 (Bayesian Filter). The recursive equations, known as the Bayesian filter, for computing the filtering distribution \\(p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\) and the predicted distribution \\(p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})\\) at the time step \\(k,\\) are given by the three-stage process:\nInitialization: Define the prior distribution \\(p(\\mathbf{x}_{0}).\\)\nPrediction: Compute the predictive distribution \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})=\\int p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1})p(\\mathbf{x}_{k-1}\\mid\\mathbf{y}_{1:k-1})\\,\\mathrm{d}\\mathbf{x}_{k-1}.\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-4",
    "href": "slides/week-06/08-DA-Bayes.html#section-4",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Correction: Compute the posterior distribution by Bayes’ rule,\n\n\\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})=\\frac{p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})}{\\int p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})}\\,\\mathrm{d}\\mathbf{x}_{k}.\\]\nNow, if we assume that the dynamic and measurement models are linear, with i.i.d. Gaussian noise, then we obtain the closed-form solution for the Kalman filter, already derived in the Basic Course. We recall the linear, Gaussian state-space model, \\[\\begin{aligned}\n\\mathbf{x}_{k} & =\\mathbf{M}_{k-1}\\mathbf{x}_{k-1}+\\mathbf{w}_{k-1},\\label{eq:kf-1}\\\\\n\\mathbf{y}_{k} & =\\mathbf{H}_{k}\\mathbf{x}_{k}+\\mathbf{v}_{k},\\label{eq:kf-2}\n\\end{aligned} \\tag{3}\\] for the Kalman filter, where\n\n\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) is the state,\n\\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}\\) is the measurement,\n\\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1})\\) is the process noise,\n\\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k})\\) is the measurement noise,\n\\(\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{m}_{0},\\mathbf{P}_{0})\\) is the Gaussian distributed initial state, with mean \\(\\mathbf{m}_{0}\\) and covariance \\(\\mathbf{P}_{0},\\)\n\\(\\mathbf{M}_{k-1}\\) is the time-dependent transition matrix of the dynamic model at time \\(k-1,\\) and\n\\(\\mathbf{H}_{k}\\) is the time-dependent measurement model matrix."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-5",
    "href": "slides/week-06/08-DA-Bayes.html#section-5",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "This model can be very elegantly rewritten in terms of conditional probabilities as \\[\\begin{aligned}\np(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}) & =\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{M}_{k-1}\\mathbf{x}_{k-1},\\mathbf{Q}_{k-1}\\right),\\\\\np(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}) & =\\mathcal{N}\\left(\\mathbf{y}_{k}\\mid\\mathbf{H}_{k}\\mathbf{x}_{k},\\mathbf{R}_{k}\\right).\n\\end{aligned}\\]\n\nTheorem 2 (Kalman Filter). The Bayesian filtering equations for the linear, Gaussian model Equation 3 can be explicitly computed and the resulting conditional probability distributions are Gaussian. The prediction distribution is \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})=\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\hat{\\mathbf{m}}_{k},\\hat{\\mathbf{P}}_{k}\\right),\\] the filtering distribution is \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})=\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right)\\] and the smoothing distribution is \\[p(\\mathbf{y}_{k}\\mid\\mathbf{y}_{1:k-1})=\\mathcal{N}\\left(\\mathbf{y}_{k}\\mid\\mathbf{H}_{k}\\mathbf{\\hat{m}}_{k},\\mathbf{S}_{k}\\right).\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-6",
    "href": "slides/week-06/08-DA-Bayes.html#section-6",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "The parameters of these distributions can be computed by the three-stage Kalman filter loop:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)\nPrediction: Compute the predictive distribution mean and covariance, \\[\\begin{aligned}\n\\mathbf{\\hat{m}}_{k} & =\\mathbf{M}_{k-1}\\mathbf{m}_{k-1},\\\\\n\\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{k-1}\\mathbf{P}_{k-1}\\mathbf{M}_{k-1}^{\\mathrm{T}}+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]\nCorrection: Compute the filtering distribution mean and covariance by first defining \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{\\hat{m}}_{k},\\quad\\textrm{the innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{k}\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k},\\quad\\textrm{the measurement covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{k}^{\\mathrm{T}}\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]\nthen finally updating the filter mean and covariance,"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-7",
    "href": "slides/week-06/08-DA-Bayes.html#section-7",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]\n\nProof. The proof see Särkkä and Svensson (2023) is a direct application of classical results for the joint, marginal, and conditional distributions of two Gaussian random variables, \\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}.\\)"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#kf-for-gaussian-random-walk",
    "href": "slides/week-06/08-DA-Bayes.html#kf-for-gaussian-random-walk",
    "title": "Bayesian Data Assimilation",
    "section": "KF for Gaussian Random Walk",
    "text": "KF for Gaussian Random Walk\nWe now return to the Gaussian random walk model seen above in the Example, and formulate a Kalman filter for estimating its state from noisy measurements.\n\nExample 1 Kalman Filter for Gaussian Random Walk. Suppose that we have measurements of the scalar \\(y_{k}\\) from the Gaussian random walk model \\[\\begin{aligned}\nx_{k} & =x_{k-1}+w_{k-1},\\quad w_{k-1}\\sim\\mathcal{N}(0,Q),\\label{eq:GRW-x2}\\\\\ny_{k} & =x_{k}+v_{k},\\quad v_{k}\\sim\\mathcal{N}(0,R).\\label{eq:GRW-y2}\n\\end{aligned} \\tag{4}\\] This very basic system is found in many applications where\n\n\\(x_{k}\\) represents a slowly varying quantity that we measure directly.\nprocess noise, \\(w_{k},\\) takes into account fluctuations in the state \\(x_{k}.\\)\nmeasurement noise, \\(v_{k},\\) accounts for measurement instrument errors.\n\n\nWe want to estimate the state \\(x_{k}\\) over time, taking into account the measurements \\(y_{k}.\\) That is, we would like to compute the filtering density,"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-8",
    "href": "slides/week-06/08-DA-Bayes.html#section-8",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[p({x}_{k}\\mid{y}_{1:k})=\\mathcal{N}\\left({x}_{k}\\mid{m}_{k},{P}_{k}\\right).\\] We proceed by simply writing down the three stages of the Kalman filter, noting that \\(M_{k}=1\\) and \\(H_{k}=1\\) for this model. We obtain:\nInitialization: Define the prior mean \\({m}_{0}\\) and prior covariance \\({P}_{0}.\\)\nPrediction: \\[\\begin{aligned}\n{\\hat{m}}_{k} & ={m}_{k-1},\\\\\n\\hat{{P}}_{k} & ={P}_{k-1}+{Q}.\n\\end{aligned}\\]\nCorrection: Define \\[\\begin{aligned}\n{d}_{k} & ={y}_{k}-{\\hat{m}}_{k},\\quad\\textrm{the innovation},\\\\\n{S}_{k} & =\\hat{{P}}_{k}+{R},\\quad\\textrm{the measurement covariance},\\\\\n{K}_{k} & =\\hat{{P}}_{k}{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-9",
    "href": "slides/week-06/08-DA-Bayes.html#section-9",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "then update, \\[\\begin{aligned}\n{m}_{k} & ={\\hat{m}}_{k}+K_{k}{d}_{k},\\\\\nP_{k} & =\\hat{P}_{k}-\\frac{\\hat{P}_{k}^{2}}{S_{k}}.\n\\end{aligned}\\]\nIn Figure 2, we show simulations with system noise standard deviation of \\(1\\) and measurement noise standard deviation of \\(0.5.\\) We observe that the KF tracks the random walk very efficiently.\n\n\nFigure 2: Kalman filter for tracking a Gaussian random walk state space model 4“. State, \\(x_k\\), is solid blue curve; measurements, \\(y_k\\), are red circles; Kalman filter estimate is green curve and 95% quantiles are shown. Fixed values of noise variances are \\(Q=1\\) and \\(R=0.5^2\\). Results computed by kf_gauss_state.m."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#code-1",
    "href": "slides/week-06/08-DA-Bayes.html#code-1",
    "title": "Bayesian Data Assimilation",
    "section": "Code",
    "text": "Code\n\n\n% Kalman Filter for scalar Gaussian random walk\n% Set parameters\nsig_w = 1; sig_v = 0.5;\nM = 1;\nQ = sig_w\\^2;\nH = 1;\nR = sig_v\\^2;\n% Initialize\nm0 = 0;\nP0 = 1;\n% Simulate data\nrandn('state',1234);\nsteps = 100; T = \\[1:steps\\];\nX = zeros(1,steps);\nY = zeros(1,steps);\nx = m0;\nfor k=1:steps\n  w = Q'\\*randn(1);\n  x = M\\*x + w;\n  y = H\\*x + sig_v\\*randn(1);\n  X(k) = x;\n  Y(k) = y;\nend\nplot(T,X,'-',T,Y,'.');\nlegend('Signal','Measurements');\nxlabel('{k}'); ylabel('{x}\\_k');\n% Kalman filter\nm = m0;\nP = P0;\nfor k=1:steps\n  m = M\\*m;\n  P = M\\*P\\*M' + Q;\n  d = Y(:,k) - H\\*m;\n  S = H\\*P\\*H' + R;\n  K = P\\*H'/S;\n  m = m + K\\*d;\n  P = P - K\\*S\\*K';\n  kf_m(k) = m;\n  kf_P(k) = P;\nend\n% Plot   \nclf; hold on\nfill(\\[T fliplr(T)\\],\\[kf_m+1.96\\*sqrt(kf_P) \\...\n  fliplr(kf_m-1.96\\*sqrt(kf_P))\\],1, \\...\n  'FaceColor',\\[.9 .9 .9\\],'EdgeColor',\\[.9 .9 .9\\])\nplot(T,X,'-b',T,Y,'or',T, kf_m(1,:),'-g')\nplot(T,kf_m+1.96\\*sqrt(kf_P),':r',T,kf_m-1.96\\*sqrt(kf_P),':r');\nhold off\n\n\n\n\n\n\n\nNote\n\n\n\nLine 3: by modifying these noise amplitudes, one can better understand how the KF operates.\nLines 31-32 and 34-38: the complete filter is coded in only 7 lines, exactly as prescribed by Theorem 5. This is the reason for the excellent performance of the KF, in particular in real-time systems. In higher dimensions, when the matrices become large, more attention must be paid to the numerical linear algebra routines used. The inversion of the measurement covariance matrix, \\(S,\\) in line 36, is particularly challenging and requires highly tuned decomposition methods."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-10",
    "href": "slides/week-06/08-DA-Bayes.html#section-10",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "EXTENDED KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#extended-kalman-filters",
    "href": "slides/week-06/08-DA-Bayes.html#extended-kalman-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Extended Kalman filters",
    "text": "Extended Kalman filters\nIn real applications, we are usually confronted with nonlinearity in the model and in the measurements.\n\nMoreover, the noise is not necessarily additive.\n\nTo deal with these nonlinearities, one possible approach is to linearize about the current mean and covariance, which produces the extended Kalman filter (EKF).\nThis filter is widely accepted as the standard for navigation and GPS systems, among others.\nRecall the nonlinear problem, \\[\\begin{aligned}\n\\mathbf{x}_{k} & = & \\mathcal{M}_{k-1}(\\mathbf{x}_{k-1})+\\mathbf{w}_{k-1},\\label{eq:state_nl}\\\\\n\\mathbf{y}_{k} & = & \\mathcal{H}_{k}(\\mathbf{x}_{k})+\\mathbf{v}_{k},\\label{eq:obs_nl}\n\\end{aligned} \\tag{5}\\] where"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-11",
    "href": "slides/week-06/08-DA-Bayes.html#section-11",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n},\\) \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m},\\) \\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1}),\\) \\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k}),\\)\nand now \\(\\mathcal{M}_{k-1}\\) and \\(\\mathcal{H}_{k}\\) are nonlinear functions of \\(\\mathbf{x}_{k-1}\\) and \\(\\mathbf{x}_{k}\\) respectively.\n\nThe EKF is then based on Gaussian approximations of the filtering densities, \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\approx\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right),\\] where these approximations are derived from the first-order truncation of the corresponding Taylor series in terms of the statistical moments of the underlying random variables.\nLinearization in the Taylor series expansions will require evaluation of the Jacobian matrices, defined as \\[\\mathbf{M}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{M}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}\\] and"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-12",
    "href": "slides/week-06/08-DA-Bayes.html#section-12",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\mathbf{H}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}.\\]\n\nTheorem 3 The first-order extended Kalman filter with additive noise for the nonlinear system 5 can be computed by the three-stage process:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)*\nPrediction: Compute the predictive distribution mean and covariance, \\[\\begin{aligned}\n\\mathbf{\\hat{m}}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{m}_{k-1}),\\\\\n\\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{\\mathbf{x}}(\\mathbf{m}_{k-1})\\mathbf{P}_{k-1}\\mathbf{M}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]\nCorrection: Compute the filtering distribution mean and covariance by first defining \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathcal{H}_{k}(\\mathbf{\\hat{m}}_{k}),\\quad\\textrm{the innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{\\mathbf{x}}(\\mathbf{\\hat{m}}_{k})\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})+\\mathbf{R}_{k},\\thinspace\\textrm{the measurement covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-13",
    "href": "slides/week-06/08-DA-Bayes.html#section-13",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "then finally updating the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]\n\nProof. The proof see Särkkä and Svensson (2023) is once again a direct application of classical results for the joint, marginal and conditional distributions of two Gaussian random variables, \\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}.\\) In addition, use is made of the Taylor series approximations to compute the Jacobian matrices \\(\\mathbf{M}_{\\mathbf{x}}\\) and \\(\\mathbf{H}_{\\mathbf{x}}\\) evaluated at \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k-1}\\) and \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k}\\) respectively."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#extended-kalman-filter-non-additive-noise",
    "href": "slides/week-06/08-DA-Bayes.html#extended-kalman-filter-non-additive-noise",
    "title": "Bayesian Data Assimilation",
    "section": "Extended Kalman filter — non-additive noise",
    "text": "Extended Kalman filter — non-additive noise\nFor non-additive noise, the model is now \\[\\begin{aligned}\n\\mathbf{x}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{x}_{k-1},\\mathbf{w}_{k-1}),\\label{eq:state_nl_na}\\\\\n\\mathbf{y}_{k} & =\\mathcal{H}_{k}(\\mathbf{x}_{k},\\mathbf{v}_{k}),\\label{eq:obs_nl_na}\n\\end{aligned} \\tag{6}\\] where \\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1}),\\) and \\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k})\\) are system and measurement Gaussian noises.\nIn this case the overall three-stage scheme is the same, with necessary modifications to take into account the additional functional dependence on \\(\\mathbf{w}\\) and \\(\\mathbf{v}.\\)\n\nInitialization:\n\nDefine the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)\n\nPrediction:\n\nCompute the predictive distribution mean and covariance,"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-14",
    "href": "slides/week-06/08-DA-Bayes.html#section-14",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\mathbf{\\hat{m}}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{m}_{k-1},\\mathbf{0}),\\\\\n    \\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{\\mathbf{x}}(\\mathbf{m}_{k-1})\\mathbf{P}_{k-1}\\mathbf{M}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})\\\\\n     & +\\mathbf{M}_{\\mathbf{w}}(\\mathbf{m}_{k-1})\\mathbf{Q}_{k-1}\\mathbf{M}_{\\mathbf{w}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})+\\mathbf{Q}_{k-1}.\n    \\end{aligned}\\]\n\nCorrection:\n\nCompute the filtering distribution mean and covariance by first defining \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathcal{H}_{k}(\\mathbf{\\hat{m}}_{k},\\mathbf{0}),\\,\\textrm{the}\\,\\textrm{innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{\\mathbf{x}}(\\mathbf{\\hat{m}}_{k})\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\\\\n& +\\mathbf{H}_{\\mathbf{v}}(\\mathbf{\\hat{m}}_{k})\\mathbf{R}_{k}\\mathbf{H}_{\\mathbf{v}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k}),\\,\\textrm{the}\\,\\textrm{measurement\\,covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\mathbf{S}_{k}^{-1},\\,\\textrm{the}\\,\\textrm{Kalman\\,gain,}\n\\end{aligned}\\] then finally updating the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#ekf-pros-and-cons",
    "href": "slides/week-06/08-DA-Bayes.html#ekf-pros-and-cons",
    "title": "Bayesian Data Assimilation",
    "section": "EKF — pros and cons",
    "text": "EKF — pros and cons\nPros:\n\nRelative simplicity, based on well-known linearization methods.\nMaintains the simple, elegant, and computationally efficient KF update equations.\nGood performance for such a simple method.\nAbility to treat nonlinear process and observation models.\nAbility to treat both additive and more general nonlinear Gaussian noise.\n\nCons:\n\nPerformance can suffer in presence of strong nonlinearity because of the local validity of the linear approximation (valid for small perturbations around the linear term).\nCannot deal with non-Gaussian noise, such as discrete-valued random variables."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-15",
    "href": "slides/week-06/08-DA-Bayes.html#section-15",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Requires differentiable process and measurement operators and evaluation of Jacobian matrices, which might be problematic in very high dimensions.\n\nIn spite of this, the EKF remains a solid filter and, as mentioned earlier, remains the basis of most GPS and navigation systems."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#ekf-example-nonlinear-oscillator",
    "href": "slides/week-06/08-DA-Bayes.html#ekf-example-nonlinear-oscillator",
    "title": "Bayesian Data Assimilation",
    "section": "EKF Example — nonlinear oscillator",
    "text": "EKF Example — nonlinear oscillator\nConsider the nonlinear ODE model for the oscillations of a noisy pendulum with unit mass and length \\(L,\\) \\[\\frac{\\mathrm{d}^{2}\\theta}{\\mathrm{d}t^{2}}+\\frac{g}{L}\\sin\\theta+w(t)=0\\] where\n\n\\(\\theta\\) is the angular displacement of the pendulum,\n\\(g\\) is the gravitational constant,\n\\(L\\) is the pendulum’s length, and\n\\(w(t)\\) is a white noise process.\n\nThis is rewritten in state space form, \\[\\dot{\\mathbf{x}}+\\mathcal{M}(\\mathbf{x})+\\mathbf{w}=0,\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-16",
    "href": "slides/week-06/08-DA-Bayes.html#section-16",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "where\n\\[\\begin{aligned}\n    \\mathbf{x} & =\\left[\\begin{array}{c}\n    x_{1}\\\\\n    x_{2}\n    \\end{array}\\right]=\\left[\\begin{array}{c}\n    \\theta\\\\\n    \\dot{\\theta}\n    \\end{array}\\right],\\quad\\mathcal{M}(\\mathbf{x})=\\left[\\begin{array}{c}\n    x_{2}\\\\\n    -\\dfrac{g}{L}\\sin x_{1}\n    \\end{array}\\right],\\\\\n    \\mathbf{w} & =\\left[\\begin{array}{c}\n    0\\\\\n    w(t)\n    \\end{array}\\right].\n    \\end{aligned}\\]\nSuppose that we have discrete, noisy measurements of the horizontal component of the position, \\(\\sin(\\theta).\\)\n\nThen the measurement equation is scalar, \\[y_{k}=\\sin\\theta_{k}+v_{k},\\] where \\(v_{k}\\) is a zero-mean Gaussian random variable with variance \\(R.\\)\n\nThe system is thus nonlinear in both state and measurement and the state-space system is of the general form 5."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-17",
    "href": "slides/week-06/08-DA-Bayes.html#section-17",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "A simple discretization, based on the simplest Euler’s method, produces \\[\\begin{aligned}\n    \\mathbf{x}_{k} & =\\mathcal{M}(\\mathbf{x}_{k-1})+\\mathbf{w}_{k-1}\\\\\n    {y}_{k} & =\\mathcal{H}_{k}(\\mathbf{x}_{k})+{v}_{k},\n    \\end{aligned}\\] where \\[\\begin{aligned}\n    \\mathbf{x}_{k} & =\\left[\\begin{array}{c}\n    x_{1}\\\\\n    x_{2}\n    \\end{array}\\right]_{k},\\\\\n    \\mathcal{M}(\\mathbf{x}_{k-1}) & =\\left[\\begin{array}{c}\n    x_{1}+\\Delta tx_{2}\\\\\n    x_{2}-\\Delta t\\dfrac{g}{L}\\sin x_{1}\n    \\end{array}\\right]_{k-1},\\\\\n    \\mathcal{H}(\\mathbf{x}_{k}) & =[\\sin x_{1}]_{k}.\n    \\end{aligned}\\]\nThe noise terms have distributions \\[\\mathbf{w}_{k-1}\\sim\\mathcal{N}(\\mathbf{0},Q),\\quad v_{k}\\sim\\mathcal{N}(0,R),\\] where the process covariance matrix is"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-18",
    "href": "slides/week-06/08-DA-Bayes.html#section-18",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[Q=\\left[\\begin{array}{cc}\n    q_{11} & q_{12}\\\\\n    q_{21} & q_{22}\n    \\end{array}\\right],\\] with components (see remark below the example), \\[q_{11}=q_{c}\\frac{\\Delta t^{3}}{3},\\quad q_{12}=q_{21}=q_{c}\\frac{\\Delta t^{2}}{2},\\quad q_{22}=q_{c}\\Delta t,\\] and \\(q_{c}\\) is the continuous process noise spectral density.\nFor the first-order EKF higher orders are possiblewe will need the Jacobian matrices of \\(\\mathcal{M}(\\mathbf{x})\\) and \\(\\mathcal{H}(\\mathbf{x})\\) evaluated at \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k-1}\\) and \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k}\\) . These are easily obtained here, in an explicit form, \\[\\mathbf{M}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{M}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}=\\left[\\begin{array}{cc}\n    1 & \\Delta t\\\\\n    -\\Delta t\\dfrac{g}{L}\\cos x_{1} & 1\n    \\end{array}\\right]_{k-1},\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-19",
    "href": "slides/week-06/08-DA-Bayes.html#section-19",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\mathbf{H}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}=\\left[\\begin{array}{cc}\n\\cos x_{1} & 0\\end{array}\\right]_{k}.\\]\nFor the simulations, we take:\n\n500 time steps with \\(\\Delta t=0.01.\\)\nNoise levels \\(q_{c}=0.01\\) and \\(R=0.1.\\)\nInitial angle \\(x_{1}=1.8\\) and initial angular velocity \\(x_{2}=0.\\)\nInitial diagonal state covariance of \\(0.1.\\)\n\nResults are plotted in Figure Figure 3.\n\nWe notice that despite the very noisy, nonlinear measurements, the EKF rapidly approaches the true state and then tracks it extremely well."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-20",
    "href": "slides/week-06/08-DA-Bayes.html#section-20",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Figure 3: Extended Kalman filter for tracking a noisy pendulum model, where horizontal position is measured. State, \\(x_k\\), $is solid blue curve; measurements, \\(y_k\\), are red circles; extended Kalman filter estimate is green curve. Results computed by EKfPendulum.m"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-21",
    "href": "slides/week-06/08-DA-Bayes.html#section-21",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Note\n\n\nIn the above example, we have used a rather special form for the process noise covariance, \\(Q.\\) It cannot be computed exactly for nonlinear systems and some kind of approximations are needed.1 One way is to use an Euler-Maruyama method from SDEs, but this leads to singular dynamics where the particle smoothers will not work. Another approach, which was used here, is to first construct an approximate model and then compute the covariance using that model. In this case the approximate model was taken as \\[\\ddot{x}=w(t),\\] which is maybe overly simple, but works. Then the matrix \\(Q\\) is propagated through this simplified dynamics using an integration factor (exponential) solution and the corresponding power series expression of the transition matrix. Details of this can be found in [Grewal; Andrews].\n\n\n\nThanks to Simo Särkkä (private communication) for suggesting this explanation."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#unscented-kalman-filters",
    "href": "slides/week-06/08-DA-Bayes.html#unscented-kalman-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Unscented Kalman filters",
    "text": "Unscented Kalman filters\nThe unscented Kalman filter (UKF) was developed to overcome two shortcomings of the EKF:\n\nits difficulty to treat strong nonlinearities and\nits reliance on the computation of Jacobians.\n\nThe UKF is based on the unscented transform (UT), a method for approximating the distribution of a transformed variable, \\[\\mathbf{y}=g(\\mathbf{x}),\\] where \\(\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{m},\\mathbf{P}),\\) without linearizing the function \\(g.\\)\nThe UT is computed as follows:"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-22",
    "href": "slides/week-06/08-DA-Bayes.html#section-22",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Choose a collection of so-called \\(\\sigma\\)-points that reproduce the mean and covariance of the distribution of \\(\\mathbf{x}\\).\nApply the nonlinear function to the \\(\\sigma\\)-points.\nEstimate the mean and variance of the transformed random variable.\n\nThis is a deterministic sampling approach, as opposed to Monte Carlo, particle filters, and ensemble filters that all use randomly sampled points. Note that the first two usually require orders of magnitude more points than the UKF.\nSuppose that the random variable \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) with mean \\(\\mathbf{m}\\) and covariance \\(\\mathbf{P}.\\) Compute \\(N=2n+1\\) \\(\\sigma\\)-points and their corresponding weights \\[\\{\\mathbf{x}^{(\\pm i),},w^{(\\pm i)}\\},\\quad i=0,1,\\ldots,N\\] by the formulas"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-23",
    "href": "slides/week-06/08-DA-Bayes.html#section-23",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\mathbf{x}^{(0)} & =\\mathbf{m},\\\\\n\\mathbf{x}^{(\\pm i)} & =\\mathbf{m}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}^{(i)},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda},\\\\\nw^{(\\pm i)} & =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n,\n\\end{aligned}\\] where\n\n\\(\\mathbf{p}_{i}\\) is the \\(i\\)-th column of the square root of \\(\\mathbf{P},\\) which is the matrix \\(S\\) such that \\(SS^{\\mathrm{T}}=\\mathbf{P},\\) sometimes denoted as \\(\\mathbf{P}^{1/2},\\)\n\\(\\lambda\\) is a scaling parameter, defined as \\[\\lambda=\\alpha^{2}(n+\\kappa)-n,\\quad0&lt;\\alpha&lt;1,\\]\n\\(\\alpha\\) and \\(\\kappa\\) describe the spread of the \\(\\sigma\\)-points around the mean, with \\(\\kappa=3-n\\) usually,\n\\(\\beta\\) is used to include prior information on non-Gaussian distributions of \\(\\mathbf{x}.\\)"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-24",
    "href": "slides/week-06/08-DA-Bayes.html#section-24",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "For the covariance matrix, the weight \\(w^{(0)}\\) is modified to \\[w^{(0)}=\\frac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right).\\] These points and weights ensure that the means and covariances are consistently captured by the UT."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#ukf-algorithm",
    "href": "slides/week-06/08-DA-Bayes.html#ukf-algorithm",
    "title": "Bayesian Data Assimilation",
    "section": "UKF — algorithm",
    "text": "UKF — algorithm\n\nTheorem 4 The UKF for the nonlinear system 5 computes a Gaussian approximation of the filtering distribution \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\approx\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right),\\] based on the UT, following the three-stage process:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0},\\) prior covariance \\(\\mathbf{P}_{0}\\) and the parameters \\(\\alpha,\\) \\(\\beta,\\) \\(\\kappa.\\)\nPrediction:\nCompute the \\(\\sigma\\)-points and weights, \\[\\begin{aligned}\n\\mathbf{x}_{k-1}^{(0)} & =\\mathbf{m}_{k-1},\\\\\n\\mathbf{x}_{k-1}^{(\\pm i)} & =\\mathbf{m}_{k-1}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}_{k-1}^{(i)},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda},\\quad w^{(\\pm i)}  =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-25",
    "href": "slides/week-06/08-DA-Bayes.html#section-25",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Propagate the \\(\\sigma\\)-points through the dynamic model \\[\\tilde{\\mathbf{x}}_{k}^{(i)}=\\mathcal{M}_{k-1}\\left(\\mathbf{x}_{k-1}^{(i)}\\right).\\]\nCompute the predictive distribution mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k}^{-} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\tilde{\\mathbf{x}}_{k}^{(i)}\\\\\n\\mathbf{P}_{k}^{-} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left(\\tilde{\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)\\left(\\tilde{\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)^{\\mathrm{T}}+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-26",
    "href": "slides/week-06/08-DA-Bayes.html#section-26",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Correction:\nCompute the updated \\(\\sigma\\)-points and weights, \\[\\begin{aligned}\n\\mathbf{x}_{k}^{(0)} & =\\mathbf{m}_{k}^{-},\\\\\n\\mathbf{x}_{k}^{(\\pm i)} & =\\mathbf{m}_{k}^{-}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}_{k}^{(i)-},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right),\\\\\nw^{(\\pm i)} & =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n.\n\\end{aligned}\\]\nPropagate the updated \\(\\sigma\\)-points through the measurement model \\[\\tilde{\\mathbf{y}}_{k}^{(i)}=\\mathcal{H}_{k}\\left(\\mathbf{x}_{k}^{(i)}\\right).\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-27",
    "href": "slides/week-06/08-DA-Bayes.html#section-27",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Compute the predicted mean and innovation, predicted measurement covariance, state-measurement cross-covariance, and filter gain, \\[\\begin{aligned}\n\\boldsymbol{\\mu}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\tilde{\\mathbf{y}}_{k}^{(i)},\\quad\\textrm{the mean,}\\\\\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\boldsymbol{\\mu}_{k},\\quad\\textrm{the innovation,}\\\\\n\\mathbf{S}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}}+\\mathbf{R}_{k},\\textrm{ measur cov}\\\\\n\\mathbf{C}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left({\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}},\\textrm{ s-m cross-cov},\\\\\n\\mathbf{K}_{k} & =\\mathbf{C}_{k}\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain.}\n\\end{aligned}\\]\nFinally, update the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-28",
    "href": "slides/week-06/08-DA-Bayes.html#section-28",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Just as was the case with the EKF, the UKF can also be applied to the non-additive noise model 6.\n\nThis is achieved by applying a non-additive version of the UT. Details can be found in (Särkkä and Svensson 2023)."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#particle-filters",
    "href": "slides/week-06/08-DA-Bayes.html#particle-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Particle filters",
    "text": "Particle filters\nWhat happens if both the models are nonlinear and the pdfs are non Gaussian?\nThe Kalman filter and its extensions are no longer optimal and, more importantly, can easily fail the estimation process. Another approach must be used.\nA promising candidate is the particle filter (PF)\nThe particle filter (Doucet, Johansen, et al. 2009) (and references therein) works sequentially in the spirit of the Kalman filter, but unlike the latter, it handles an ensemble of states (the particles) whose distribution approximates the pdf of the true state.\nBayes’ rule and the marginalization formula, \\[p(x)=\\int p(x\\mid y)p(y)\\,\\mathrm{d}y,\\] are explicitly used in the estimation process.\nThe linear and Gaussian hypotheses can then be ruled out, in theory."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-29",
    "href": "slides/week-06/08-DA-Bayes.html#section-29",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "In practice though, the particle filter cannot yet be applied to very high dimensional systems (this is often referred to as “the curse of dimensionality”). Though recent work by [Friedemann, Raffin2023] has improved this by sophisticated parallel computing.\nParticle filters are methods for obtaining Monte Carlo approximations of the solutions of the Bayesian filtering equations.\nRather than trying to compute the exact solution of the Bayesian filtering equations, the transformations of such filtering (Bayes’ rule for the analysis, model propagation for the forecast) are applied to the members of the sample.\n\nThe statistical moments are meant to be those of the targeted pdf.\nObviously this sampling strategy can only be exact in the asymptotic limit; that is, in the limit where the number of members (or particles) goes to infinity.\n\nThe most popular and simple algorithm of Monte Carlo type that solves the Bayesian filtering equations is called the bootstrap particle filter. It is computed by a three-stage process."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-30",
    "href": "slides/week-06/08-DA-Bayes.html#section-30",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Sampling\n\nWe consider a sample of particles \\(\\left\\{ \\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{M}\\right\\}\\). The related probability density function at time \\(t_{k}\\) is \\(p_{k}(\\mathbf{x}),\\) where \\[p_{k}(\\mathbf{x})\\simeq\\sum_{i=1}^{M}\\omega_{i}^{k}\\delta(\\mathbf{x}-\\mathbf{x}_{k}^{i})\\] and \\(\\delta\\) is the Dirac mass and the sum is meant to be an approximation of the exact density that the samples emulate. A positive scalar, \\(\\omega_{k}^{i},\\) weights the importance of particle \\(i\\) within the ensemble. At this stage, we assume that the weights \\(\\omega_{i}^{k}\\) are uniform and \\(\\omega_{i}^{k}=1/M\\)."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-31",
    "href": "slides/week-06/08-DA-Bayes.html#section-31",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Forecast\n\nAt the forecast step, the particles are propagated by the model without approximation, \\[p_{k+1}(\\mathbf{x})\\simeq\\sum_{i=1}^{M}\\omega_{k}^{i}\\delta(\\mathbf{x}-\\mathbf{x}_{k+1}^{i}),\\] with \\(\\mathbf{x}_{k+1}^{i}=\\mathcal{M}_{k+1}(\\mathbf{x}_{k}).\\) A stochastic noise can optionally be added to the dynamics of each particle."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-32",
    "href": "slides/week-06/08-DA-Bayes.html#section-32",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Analysis\n\nThe analysis step of the particle filter is extremely simple and elegant. The rigorous implementation of Bayes’ rule ascribes to each particle a statistical weight that corresponds to the likelihood of the particle given the data. The weight of each particle is updated according to \\[\\omega_{k+1}^{\\mathrm{a},i}\\propto\\omega_{k+1}^{\\mathrm{f},i}p(\\mathbf{y}_{k+1}|\\mathbf{x}_{k+1}^{i})\\,.\\] It is remarkable that the analysis is carried out with only a few multiplications. It does not involve inverting any system or matrix, as opposed for instance to the Kalman filter."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#choosing-a-filter",
    "href": "slides/week-06/08-DA-Bayes.html#choosing-a-filter",
    "title": "Bayesian Data Assimilation",
    "section": "Choosing a Filter",
    "text": "Choosing a Filter\nOne usually has to choose between\n\nlinear Kalman filters\nensemble Kalman filters\nnonlinear filters\nhybrid variational-filter methods."
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#section-33",
    "href": "slides/week-06/08-DA-Bayes.html#section-33",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "These questions are resumed in the following Table:\n\n\n\nTable 1: Decision matrix for choice of Kalman filters.\n\n\n\n\n\nEstimator\nModel type\npdf\nCPU-time\n\n\n\n\nKF\nlinear\nGaussian\nlow\n\n\nEKF\nlocally linear\nGaussian\nlow-medium\n\n\nUKF\nnonlinear\nGaussian\nmedium\n\n\nEnKF\nnonlinear\nGaussian\nmedium-high\n\n\nPF\nnonlinear\nnon-Gaussian\nhigh"
  },
  {
    "objectID": "slides/week-06/08-DA-Bayes.html#codes",
    "href": "slides/week-06/08-DA-Bayes.html#codes",
    "title": "Bayesian Data Assimilation",
    "section": "Codes",
    "text": "Codes\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen."
  },
  {
    "objectID": "slides/week-01/01-intro.html#course-outline",
    "href": "slides/week-01/01-intro.html#course-outline",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Course outline",
    "text": "Course outline\nThis is a new advanced course that is being developed during this term.\n\nSyllabus\nSchedule\netc.\n\nare all made available and constantly updated on\nhttps://flexie.github.io/CSE-8803-Twin//"
  },
  {
    "objectID": "slides/week-01/01-intro.html#objectives",
    "href": "slides/week-01/01-intro.html#objectives",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Objectives",
    "text": "Objectives\nBy the end of the semester, you will be made familiar with …\n\nthe concept of Digital Twins and how they interact with their environment\nStatistical Inverse Problems and Bayesian Inference\ntechniques from Data Assimilation (DAT), Simulation-Based Inference (SBI), and Recursive Bayesian Inference (RBI), and Uncertainty Quantification [UQ]\n\nFor more on the Course outline, Topics, and Learning goals, see Goals."
  },
  {
    "objectID": "slides/week-01/01-intro.html#textbooks",
    "href": "slides/week-01/01-intro.html#textbooks",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\nData assimilation: methods, algorithms, and applications\nAsch, Bocquet, and Nodet (2016), Mark\nSIAM, 2016\n\n\nA toolbox for digital twins: from model-based to data-driven1\nAsch (2022), Mark\nSIAM, 2022\n\n\n\nThis 1000 page book is rather comprehensive. While the complete material is beyond this course, you are encouraged to use the extensive cross-referencing in this book to your advantage. This book is available in electronic form (pdf) when online on Georgia Tech Campus."
  },
  {
    "objectID": "slides/week-01/01-intro.html#journal-papers",
    "href": "slides/week-01/01-intro.html#journal-papers",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Journal Papers",
    "text": "Journal Papers\n\n\n\nA comprehensive review of digital twin—part 1: modeling and twinning enabling technologies\nThelen et al. (2022)\nSpringer, 2022\n\n\nA comprehensive review of digital twin—part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives\nThelen et al. (2023)\nSpringer, 2022\n\n\nSequential Bayesian inference for uncertain nonlinear dynamic systems: A tutorial\nTatsis, Dertimanis, and Chatzi (2022)\nArxiv, 2022"
  },
  {
    "objectID": "slides/week-01/01-intro.html#introduction",
    "href": "slides/week-01/01-intro.html#introduction",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Introduction",
    "text": "Introduction\n\nDifferent definitions of Digital Twins\nData Flows\nDimensions Digital Twins\nthe Inference Cycle"
  },
  {
    "objectID": "slides/week-01/01-intro.html#definition-of-digital-twin",
    "href": "slides/week-01/01-intro.html#definition-of-digital-twin",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Definition of Digital Twin",
    "text": "Definition of Digital Twin\nDefinition from (Asch 2022): “A set of virtual information constructs that mimics the structure, context, and behavior of an individual/unique physical asset, or a group of physical assets, is dynamically updated with data from its physical twin throughout its life cycle and informs decisions that realize value.”\n\nDefinition by the Aerospace Industries Association\nMirroring physical assets in a dynamic manner\n\nDefinition by IBM: “A digital twin is a virtual representation of an object or system that spans its lifecycle, is updated from real-time data, and uses simulation, machine learning and reasoning to help decision making.”"
  },
  {
    "objectID": "slides/week-01/01-intro.html#definition-of-digital-twin-conted",
    "href": "slides/week-01/01-intro.html#definition-of-digital-twin-conted",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Definition of Digital Twin (cont’ed)",
    "text": "Definition of Digital Twin (cont’ed)\nDefinition from (National Academies of Sciences, Medicine, et al. 2023): “A digital twin is a set of virtual information constructs that mimics the structure, context, and behavior of a natural, engineered, or social system (or system-of-systems), is dynamically updated with data from its physical twin, has a predictive capability, and informs decisions that realize value. The bidirectional interaction between the virtual and the physical is central to the digital twin.”\nAlso see discussion Section 2 of “A comprehensive review of digital twin — part 1”."
  },
  {
    "objectID": "slides/week-01/01-intro.html#cyber-physical-systems-cps",
    "href": "slides/week-01/01-intro.html#cyber-physical-systems-cps",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Cyber-Physical Systems (CPS)",
    "text": "Cyber-Physical Systems (CPS)\n\n\n\nModel Systems of Systems (SoS) by equations\nHow the two worlds—digital and physical—intertwine?\nHow does the digital inform the physical, and how does the physical shape our understanding of digital processes?\n\n\n\n\n\nSource"
  },
  {
    "objectID": "slides/week-01/01-intro.html#data-flows",
    "href": "slides/week-01/01-intro.html#data-flows",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Data flows",
    "text": "Data flows\n\n\n\nFor a digital model, data flow between the physical space and virtual space is optional\nFor a digital shadow, data flow is unidirectional from physical to digital.\nBut for digital twin, the data flow has to be bidirectional. See Figure.\n\n\n\n\n\nfrom (Thelen et al. 2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#five-dimensional-digital-twin",
    "href": "slides/week-01/01-intro.html#five-dimensional-digital-twin",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Five dimensional Digital Twin",
    "text": "Five dimensional Digital Twin\n\n\n\\[\\mathrm{DT} = \\mathbb{F} (\\mathrm{PS, DS, P2V, V2P, OPT})\\]\nfive- dimensional digital twin model consists of\n\na physical system (PS),\na digital system (DS),\nan updating engine (P2V),\na prediction engine (V2P),\nand an optimization dimension (OPT).\n\n\\(\\mathbb{F}(⋅)\\) integrates all five dimensions together to define a Digital Twin.\n\n\n\n\nfrom (Thelen et al. 2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#five-dimensions",
    "href": "slides/week-01/01-intro.html#five-dimensions",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Five dimensions",
    "text": "Five dimensions\n\\[\\mathrm{DT} = \\mathbb{F} (\\mathrm{PS, DS, P2V, V2P, OPT})\\]\n\nfrom (Thelen et al. 2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#the-inference-cycle",
    "href": "slides/week-01/01-intro.html#the-inference-cycle",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "The Inference Cycle",
    "text": "The Inference Cycle\n\n\n\nScientific method — inferential process\nAbduction — going from (unexplained) effect to (possible) cause\nDeduction — going from cause to effect\nInduction — going from specific to general\n\n\n\n\n\nSource Asch (2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#the-concept-of-a-digital-twin",
    "href": "slides/week-01/01-intro.html#the-concept-of-a-digital-twin",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "The Concept of a Digital Twin",
    "text": "The Concept of a Digital Twin\n\nthe availability of (large) volumes of (often real-time) data,\nthe accessibility to this data,\nthe tools and implementations of AI-based algorithms,\nthe body of knowledge of mathematical models,\nthe readiness and low cost of computational devices,\n\nNecessary ingredients for a Digital Twin that learns during its life cycle\n\nconsists of static part — initial model, design, and\ndynamic part — includes the simulation process, coupled with data acquisition, and finally autoupdating."
  },
  {
    "objectID": "slides/week-01/01-intro.html#the-spectrum-of-digital-twins",
    "href": "slides/week-01/01-intro.html#the-spectrum-of-digital-twins",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "The Spectrum of Digital Twins",
    "text": "The Spectrum of Digital Twins\n\nFrom model-driven to data-driven\nImportance of models, data, and competencies"
  },
  {
    "objectID": "slides/week-01/01-intro.html#digital-twins-in-the-digital-continuum",
    "href": "slides/week-01/01-intro.html#digital-twins-in-the-digital-continuum",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Digital Twins in the Digital Continuum",
    "text": "Digital Twins in the Digital Continuum\n\n\n\nInteraction with digital infrastructure\nCloud computing, IoT, and cybersecurity\nEmphasis in this course will be on monitoring & control of physical systems ruled by PDEs\n\ngeological CO2 storage\nbattery life\n\n\n\n\n\n\nSource Asch (2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#geological-carbon-storage",
    "href": "slides/week-01/01-intro.html#geological-carbon-storage",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Geological Carbon Storage",
    "text": "Geological Carbon Storage\n\n\ncoupling of fluid-flow physics and\nwave physics"
  },
  {
    "objectID": "slides/week-01/01-intro.html#models-data-and-coupling-in-digital-twins",
    "href": "slides/week-01/01-intro.html#models-data-and-coupling-in-digital-twins",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Models, Data, and Coupling in Digital Twins",
    "text": "Models, Data, and Coupling in Digital Twins\nQuestions:\n\nWhat is meant by a “model”?\nWhat are data to be used for?\nHow, if possible, can we couple the above two to construct the most informative DT?\n\nWill be discussing two types of models throughout the book:\n\nEquation-based models that are derived, most often, from some conservation laws.\nStatistical, data-driven models that are based on measured data and its analysis.\n\nA good statistical model: should attain a good balance between under/overfitting.\nPhysical models: need to capture the higher order terms and neglect small terms.\nBloated models will often be difficult to solve numerically."
  },
  {
    "objectID": "slides/week-01/01-intro.html#examples-and-use-cases",
    "href": "slides/week-01/01-intro.html#examples-and-use-cases",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Examples and Use Cases",
    "text": "Examples and Use Cases\n\nPredictive maintenance, personalized medicine, sports, agriculture, geophysics"
  },
  {
    "objectID": "slides/week-01/01-intro.html#recommended-approachthe-triple-loop-method",
    "href": "slides/week-01/01-intro.html#recommended-approachthe-triple-loop-method",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Recommended Approach—The Triple-Loop Method",
    "text": "Recommended Approach—The Triple-Loop Method\nThree loops: Space and time, optimization, decision-making\n\nLoops over space and time and solution of the physical problem in the inner loop.\nOptimization in the outer loop, including control, solution of an inverse problem, parameter estimation, uncertainty quantification, and multifidelity modeling using surrogates.\nDecision making in the outer-outer loop— preventative maintenance, shutting down operations …\n\nImportant to ensure trustworthiness of the DT - checking operational and validity regimes of the model, and - implement an expert system based on engineering experience."
  },
  {
    "objectID": "slides/week-01/01-intro.html#future-directions",
    "href": "slides/week-01/01-intro.html#future-directions",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "Future Directions",
    "text": "Future Directions\n\n\n\nEvolution from simple to AI-integrated systems\nTheoretical and practical advancements\nFacilitates a more general interpretation where we can map machine learning techniques, or AI, to any of the stages\n\n\n\n\n\nfrom Asch (2022)"
  },
  {
    "objectID": "slides/week-01/01-intro.html#references",
    "href": "slides/week-01/01-intro.html#references",
    "title": "Welcome to Digital Twins for Physical Systems",
    "section": "References",
    "text": "References\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. SIAM.\n\n\nAsch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. Data Assimilation: Methods, Algorithms, and Applications. SIAM.\n\n\nNational Academies of Sciences, Engineering, Medicine, et al. 2023. “Foundational Research Gaps and Future Directions for Digital Twins.”\n\n\nTatsis, Konstantinos E, Vasilis K Dertimanis, and Eleni N Chatzi. 2022. “Sequential Bayesian Inference for Uncertain Nonlinear Dynamic Systems: A Tutorial.” arXiv Preprint arXiv:2201.08180.\n\n\nThelen, Adam, Xiaoge Zhang, Olga Fink, Yan Lu, Sayan Ghosh, Byeng D Youn, Michael D Todd, Sankaran Mahadevan, Chao Hu, and Zhen Hu. 2022. “A Comprehensive Review of Digital Twin—Part 1: Modeling and Twinning Enabling Technologies.” Structural and Multidisciplinary Optimization 65 (12): 354.\n\n\n———. 2023. “A Comprehensive Review of Digital Twin—Part 2: Roles of Uncertainty Quantification and Optimization, a Battery Digital Twin, and Perspectives.” Structural and Multidisciplinary Optimization 66 (1): 1."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "",
    "text": "Course overview from CSE : Digital Twins for Physical Systems\nIBM defines “A digital twin is a virtual representation of an object or system that spans its lifecycle, is updated from real-time data, and uses simulation, machine learning and reasoning to help decision-making.” During this course, we will explore these concepts and their significance in addressing the challenges of monitoring and control of physical systems described by partial-differential equations. After introducing deterministic & statistical data assimilation techniques, the course switches gears towards scientific machine learning to introduce the technique of simulation-based inference, during which uncertainty is captured with generative conditional neural networks, and neural operators where Fourier Neural Operators act as surrogates for solutions of partial-differential equations. The course concludes by incorporating these techniques into uncertainty-aware Digital Twins that can be used to monitor and control complicated processes such as underground storage of CO2 or management of batteries.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "",
    "text": "Course overview from CSE : Digital Twins for Physical Systems\nIBM defines “A digital twin is a virtual representation of an object or system that spans its lifecycle, is updated from real-time data, and uses simulation, machine learning and reasoning to help decision-making.” During this course, we will explore these concepts and their significance in addressing the challenges of monitoring and control of physical systems described by partial-differential equations. After introducing deterministic & statistical data assimilation techniques, the course switches gears towards scientific machine learning to introduce the technique of simulation-based inference, during which uncertainty is captured with generative conditional neural networks, and neural operators where Fourier Neural Operators act as surrogates for solutions of partial-differential equations. The course concludes by incorporating these techniques into uncertainty-aware Digital Twins that can be used to monitor and control complicated processes such as underground storage of CO2 or management of batteries.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#class-meetings",
    "href": "index.html#class-meetings",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nHowey Physics N210\nMon & Wed 5:00 - 6:15PM",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Prerequisites",
    "text": "Prerequisites\nNumerical Linear Algebra, Statistics, Machine Learning, Experience w/ Python, or Julia",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nFelix J. Herrmann (Instructor)\nTBD\nZoom\n\n\nRafael Orozco (TA)\nTBD\nZoom",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#access-to-piazza",
    "href": "index.html#access-to-piazza",
    "title": "Digital Twins for Physical Systems Course Website",
    "section": "Access to Piazza",
    "text": "Access to Piazza\nStudents are encouraged to post their questions on Piazza on Canvas or Piazza direct, which will be monitored by Rafael Orozco.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "seminar.html",
    "href": "seminar.html",
    "title": "Seminar",
    "section": "",
    "text": "Seminar\nThere are 9 time slots until the end the term. So, please prepare a 30 minute lecture on a paper of interest. This contents of the paper should be aligned with the general topics areas of this course. We encourage you to also choose a topic in the general area of your PhD research. When in doubt please reach out.\n\nCalendar for seminar\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "Seminar"
    ]
  },
  {
    "objectID": "hw/w3-hw02.html",
    "href": "hw/w3-hw02.html",
    "title": "HW 02: Data visualization",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 03",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nAdjoint state\n\n\nInverse Problems\n\n\nMon, Jan 22\n\n\n\n\nDifferential Programming\n\n\nAutomatic Differentiation\n\n\nMon, Jan 22\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nLab 02: Data visualization with ggplot2!\n\n\nFri, Sep 23\n\n\n\n\nLab\n\n\nDifferentiable Programming\n\n\nFri, Jan 26\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 8 — A toolbox for digital twins section 8.7\n\n\nAutomatic Differentiation in Machine Learning: a Survey\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-06.html",
    "href": "weeks/week-06.html",
    "title": "Week 06",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nBayesian Data Assimilation\n\n\nData Assimilation\n\n\nWed, Feb 14\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nEnsemble Kalman Filter\n\n\nFri, Feb 16\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 9 — A toolbox for digital twins section 9.4.1—9.4.7\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 05",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nStatistical Data Assimilation\n\n\nData Assimilation\n\n\nWed, Feb 07\n\n\n\n\nBayesian Data Assimilation\n\n\nData Assimilation\n\n\nSat, Dec 23\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\n1-D Kalman Filter\n\n\nFri, Feb 09\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 9 — A toolbox for digital twins section 9.4.1—9.4.7\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 09",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nDeep Learning\n\n\nDiffusion models\n\n\nMon, Mar 11\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nNormalizing flow\n\n\nFri, Mar 01\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 17-18 — Understanding Deep Learning\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/w2-lab01-PyTorch.html",
    "href": "labs/w2-lab01-PyTorch.html",
    "title": "PyTorch intro",
    "section": "",
    "text": "Read over (or run if you need practice) notebooks 1-Basics. Run all of notebook 2-NN basics and workflows. Rerun notebook 2 and make the task a bit more difficult by:\n\nadding Gaussian standard noise to the data. (w/ at least sigma=0.01)\nadjust training hyperparameters as necessary to achieve good results.\n\nFor extra credit run notebook 3. If you do not have access to a GPU, use free tier Google collab GPU with instructions outlined in https://flexie.github.io/CSE-8803-Twin/schedule.html.\nThe notebooks can be downloaded from here\n\nBasics - pytorch_101.ipynb\nNN Basics & Workflows - pytorch_102.ipynb\nGPUs - Torch_test_GPU_CPU.ipynb\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/01intro/underfitting_overfitting.html",
    "href": "labs/01intro/underfitting_overfitting.html",
    "title": "Digital Twins for Physical Systems",
    "section": "",
    "text": "%matplotlib inline\n\n============================ Underfitting vs. Overfitting ============================\nThis example shows how underfitting and overfitting arise when using polynomial regression to approximate a nonlinear function, \\(y =1.5 \\cos (\\pi x).\\)\nThe plots shows the function \\(y(x)\\) and the estimated curves of of different degrees.\nWe observe the following:\n\nThe linear function (polynomial with degree 1) is not sufficient to fit the training samples—this is underfitting.\nA polynomial of degree 4 approximates the true function almost perfectly and gives the smallest MSE.\nFor higher degrees, the model overfits the training data, and the mean-squared errors (MSE) become very large–the model is learning the noise in the training data.\n\nWe evaluate quantitatively overfitting / underfitting by using cross-validation and then calculating the mean squared error (MSE) on the validation set. The higher the value, the less likely the model generalizes correctly from the training data since it is brittle.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 10, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 10))\nfor i in range(len(degrees)):\n    ax = plt.subplot(2, 2, i + 1) \n    plt.setp(ax, xticks=(), yticks=())\n    polynomial_features = PolynomialFeatures(degree=degrees[i],                                            include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using cross-validation\n    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/w2-lab01.html",
    "href": "labs/w2-lab01.html",
    "title": "Lab 01: Hello R!",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html",
    "title": "One Dimensional Kalman Filters",
    "section": "",
    "text": "Table of Contents"
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#problem-description",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#problem-description",
    "title": "One Dimensional Kalman Filters",
    "section": "Problem Description",
    "text": "Problem Description\nAs in the Discrete Bayes Filter chapter we will be tracking a moving object in a long hallway at work. Assume that in our latest hackathon someone created an RFID tracker that provides a reasonably accurate position of the dog. The sensor returns the distance of the dog from the left end of the hallway in meters. So, 23.4 would mean the dog is 23.4 meters from the left end of the hallway.\nThe sensor is not perfect. A reading of 23.4 could correspond to the dog being at 23.7, or 23.0. However, it is very unlikely to correspond to a position of 47.6. Testing during the hackathon confirmed this result - the sensor is ‘reasonably’ accurate, and while it had errors, the errors are small. Furthermore, the errors seemed to be evenly distributed on both sides of the true position; a position of 23 m would equally likely be measured as 22.9 or 23.1. Perhaps we can model this with a Gaussian.\nWe predict that the dog is moving. This prediction is not perfect. Sometimes our prediction will overshoot, sometimes it will undershoot. We are more likely to undershoot or overshoot by a little than a lot. Perhaps we can also model this with a Gaussian."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#beliefs-as-gaussians",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#beliefs-as-gaussians",
    "title": "One Dimensional Kalman Filters",
    "section": "Beliefs as Gaussians",
    "text": "Beliefs as Gaussians\nWe can express our belief in the dog’s position with a Gaussian. Say we believe that our dog is at 10 meters, and the variance in that belief is 1 m\\(^2\\), or \\(\\mathcal{N}(10,\\, 1)\\). A plot of the pdf follows:\n\nimport filterpy.stats as stats\nstats.plot_gaussian_pdf(mean=10., variance=1., \n                        xlim=(4, 16), ylim=(0, .5));\n\nThis plot depicts our uncertainty about the dog’s position. It represents a fairly inexact belief. While we believe that it is most likely that the dog is at 10 m, any position from 9 m to 11 m or so are quite likely as well. Assume the dog is standing still, and we query the sensor again. This time it returns 10.2 m. Can we use this additional information to improve our estimate?\nIntuition suggests we can. Consider: if we read the sensor 500 times and each time it returned a value between 8 and 12, all centered around 10, we should be very confident that the dog is near 10. Of course, a different interpretation is possible. Perhaps our dog was randomly wandering back and forth in a way that exactly emulated random draws from a normal distribution. But that seems extremely unlikely - I’ve never seen a dog do that. Let’s look at 500 draws from \\(\\mathcal N(10, 1)\\):\n\nimport numpy as np\nfrom numpy.random import randn\nimport matplotlib.pyplot as plt\n\nxs = range(500)\nys = randn(500)*1. + 10.\nplt.plot(xs, ys)\nprint(f'Mean of readings is {np.mean(ys):.3f}')\n\nEyeballing this confirms our intuition - no dog moves like this. However, noisy sensor data certainly looks this way. The computed mean of the readings is almost exactly 10. Assuming the dog is standing still, we say the dog is at position 10 with a variance of 1."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#tracking-with-gaussian-probabilities",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#tracking-with-gaussian-probabilities",
    "title": "One Dimensional Kalman Filters",
    "section": "Tracking with Gaussian Probabilities",
    "text": "Tracking with Gaussian Probabilities\nThe discrete Bayes filter used a histogram of probabilities to track the dog. Each bin in the histogram represents a position, and the value is the probability of the dog being in that position.\nTracking was performed with a cycle of predictions and updates. We used the equations\n\\[\\begin{aligned}\n\\bar {\\mathbf x} &= \\mathbf x \\ast f_{\\mathbf x}(\\bullet)\\, \\, &\\text{Predict} \\\\\n\\mathbf x &= \\mathcal L \\cdot \\bar{\\mathbf x}\\, \\, &\\text{Update}\n\\end{aligned}\\]\nto compute the new probability distributions. Recall that \\(\\bar{\\mathbf x}\\) is the prior, \\(\\mathcal L\\) is the likelihood of a measurement given the prior \\(\\bar{\\mathbf x}\\), \\(f_{\\mathbf x}(\\bullet)\\) is the process model, and \\(\\ast\\) denotes convolution. \\(\\mathbf x\\) is bold to denote that it is a histogram of numbers, or a vector.\nThis method works, but led to histograms that implied the dog could be in multiple places at once. Also, the computations are very slow for large problems.\nCan we replace \\(\\mathbf x\\), the histogram, with a Gaussian \\(\\mathcal N(x, \\sigma^2)\\)? Absolutely! We’ve learned how to express belief as a Gaussian. A Gaussian, which is a single number pair \\(\\mathcal N(\\mu, \\sigma^2),\\) can replace an entire histogram of probabilities:\n\nimport kf_book.kf_internal as kf_internal\nkf_internal.gaussian_vs_histogram()\n\nI hope you see the power of this. We can replace hundreds to thousands of numbers with a single pair of numbers: \\(x = \\mathcal N(\\mu, \\sigma^2)\\).\nThe tails of the Gaussian extend to infinity on both sides, so it incorporates arbitrarily many bars in the histogram. If this represents our belief in the position of the dog in the hallway, this one Gaussian covers the entire hallway (and the entire universe on that axis). We think that it is likely the dog is at 10, but he could be at 8, 14, or, with infinitesimally small probability, at 10\\(^{80}\\).\nIn this chapter we replace histograms with Gaussians:\n\\[\\begin{array}{l|l|c}\n\\text{discrete Bayes} & \\text{Gaussian} & \\text{Step}\\\\\n\\hline\n\\bar {\\mathbf x} = \\mathbf x \\ast f(\\mathbf x) &\n\\bar {x}_\\mathcal{N} =  x_\\mathcal{N} \\, \\oplus \\, f_{x_\\mathcal{N}}(\\bullet) &\n\\text{Predict} \\\\\n\\mathbf x = \\|\\mathcal L \\bar{\\mathbf x}\\| & x_\\mathcal{N} = L \\, \\otimes \\, \\bar{x}_\\mathcal{N} & \\text{Update}\n\\end{array}\\]\nwhere \\(\\oplus\\) and \\(\\otimes\\) is meant to express some unknown operator on Gaussians. I won’t do it in the rest of the book, but the subscript indicates that \\(x_\\mathcal{N}\\) is a Gaussian.\nThe discrete Bayes filter used convolution for the prediction. We showed that it used the total probabability theorem, computed as a sum, so maybe we can add the Gaussians. It used multiplications to incorporate the measurement into the prior, so maybe we can multiply the Gaussians. Could it be this easy:\n\\[\\begin{aligned}\n\\bar x &\\stackrel{?}{=} x + f_x(\\bullet) \\\\\nx &\\stackrel{?}{=} \\mathcal L \\cdot \\bar x\n\\end{aligned}\\]\nThis will only work if the sum and product of two Gaussians is another Gaussian. Otherwise after the first epoch \\(x\\) would not be Gaussian, and this scheme falls apart."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#predictions-with-gaussians",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#predictions-with-gaussians",
    "title": "One Dimensional Kalman Filters",
    "section": "Predictions with Gaussians",
    "text": "Predictions with Gaussians\nWe use Newton’s equation of motion to compute current position based on the current velocity and previous position:\n\\[ \\begin{aligned}\\bar{x}_k &= x_{k-1} + v_k \\Delta t \\\\\n&= x_{k-1} + f_x\\end{aligned}\\]\nI’ve dropped the notation \\(f_x(\\bullet)\\) in favor of \\(f_x\\) to keep the equations uncluttered.\nIf the dog is at 10 m, his velocity is 15 m/s, and the epoch is 2 seconds long, we have\n\\[ \\begin{aligned} f_x &= v\\Delta t = 15\\cdot 2\\\\\n\\bar{x}_k &= 10 + (15\\cdot 2) = 40 \\end{aligned}\\]\nWe are uncertain about his current position and velocity, so this will not do. We need to express the uncertainty with a Gaussian.\nPosition is easy. We define \\(x\\) as a Gaussian. If we think the dog is at 10 m, and the standard deviation of our uncertainty is 0.2 m, we get \\(x=\\mathcal N(10, 0.2^2)\\).\nWhat about our uncertainty in his movement? We define \\(f_x\\) as a Gaussian. If the dog’s velocity is 15 m/s, the epoch is 1 second, and the standard deviation of our uncertainty is 0.7 m/s, we get \\(f_x = \\mathcal N (15, 0.7^2)\\).\nThe equation for the prior is\n\\[\\bar x = x + f_x\\]\nWhat is the sum of two Gaussians? In the last chapter I proved that:\n\\[\\begin{gathered}\n\\mu = \\mu_1 + \\mu_2 \\\\\n\\sigma^2 = \\sigma^2_1 + \\sigma^2_2\n\\end{gathered}\\]\nThis is fantastic news; the sum of two Gaussians is another Gaussian!\nThe math works, but does this make intuitive sense? Think of the physical representation of this abstract equation. We have\n\\[\\begin{gathered}\nx=\\mathcal N(10, 0.2^2)\\\\\nf_x = \\mathcal N (15, 0.7^2)\n\\end{gathered}\\]\nIf we add these we get:\n\\[\\begin{aligned}\\bar x &= \\mu_x + \\mu_{f_x} = 10 + 15 &&= 25 \\\\\n\\bar\\sigma^2 &= \\sigma_x^2 + \\sigma_{f_x}^2 = 0.2^2 + 0.7^2 &&= 0.53\\end{aligned}\\]\nIt makes sense that the predicted position is the previous position plus the movement. What about the variance? It is harder to form an intuition about this. However, recall that with the predict() function for the discrete Bayes filter we always lost information. We don’t really know where the dog is moving, so the confidence should get smaller (variance gets larger). \\(\\sigma_{f_x}^2\\) is the amount of uncertainty added to the system due to the imperfect prediction about the movement, and so we would add that to the existing uncertainty.\nLet’s take advantage of the namedtuple class in Python’s collection module to implement a Gaussian object. We could implement a Gaussian using a tuple, where \\(\\mathcal N(10, 0.04)\\) is implemented in Python as g = (10., 0.04). We would access the mean with g[0] and the variance with g[1].\nnamedtuple works the same as a tuple, except you provide it with a type name and field names. It’s not important to understand, but I modified the __repr__ method to display its value using the notation in this chapter.\n\nfrom collections import namedtuple\ngaussian = namedtuple('Gaussian', ['mean', 'var'])\ngaussian.__repr__ = lambda s: f'𝒩(μ={s[0]:.3f}, 𝜎²={s[1]:.3f})'\n\nNow we can create a print a Gaussian with:\n\ng1 = gaussian(3.4, 10.1)\ng2 = gaussian(mean=4.5, var=0.2**2)\nprint(g1)\nprint(g2)\n\n𝒩(μ=3.400, 𝜎²=10.100)\n𝒩(μ=4.500, 𝜎²=0.040)\n\n\nWe can access the mean and variance with either subscripts or field names:\n\ng1.mean, g1[0], g1[1], g1.var\n\n(3.4, 3.4, 10.1, 10.1)\n\n\nHere is our implementation of the predict function, where pos and movement are Gaussian tuples in the form (\\(\\mu\\), \\(\\sigma^2\\)):\n\ndef predict(pos, movement):\n    return gaussian(pos.mean + movement.mean, pos.var + movement.var)\n\nLet’s test it. What is the prior if the intitial position is the Gaussian \\(\\mathcal N(10, 0.2^2)\\) and the movement is the Gaussian \\(\\mathcal N (15, 0.7^2)\\)?\n\npos = gaussian(10., .2**2)\nmove = gaussian(15., .7**2)\npredict(pos, move)\n\n𝒩(μ=25.000, 𝜎²=0.530)\n\n\nThe prior states that the dog is at 25 m with a variance of 0.53 m\\(^2\\), which is what we computed by hand."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#updates-with-gaussians",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#updates-with-gaussians",
    "title": "One Dimensional Kalman Filters",
    "section": "Updates with Gaussians",
    "text": "Updates with Gaussians\nThe discrete Bayes filter encodes our belief about the position of our dog in a histogram of probabilities. The distribution is discrete and multimodal. It can express strong belief that the dog is in two positions at once, and the positions are discrete.\nWe are proposing that we replace the histogram with a Gaussian. The discrete Bayes filter used this code to compute the posterior:\ndef update(likelihood, prior):\n    posterior = likelihood * prior\n    return normalize(posterior)\nwhich is an implementation of the equation:\n\\[x = \\| \\mathcal L\\bar x \\|\\]\nWe’ve just shown that we can represent the prior with a Gaussian. What about the likelihood? The likelihood is the probability of the measurement given the current state. We’ve learned how to represent measurements as Gaussians. For example, maybe our sensor states that the dog is at 23 m, with a standard deviation of 0.4 meters. Our measurement, expressed as a likelihood, is \\(z = \\mathcal N (23, 0.16)\\).\nBoth the likelihood and prior are modeled with Gaussians. Can we multiply Gaussians? Is the product of two Gaussians another Gaussian?\nYes to the former, and almost to the latter! In the last chapter I proved that the product of two Gaussians is proportional to another Gausian.\n\\[\\begin{aligned}\n\\mu &= \\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1} {\\sigma_1^2 + \\sigma_2^2}, \\\\\n\\sigma^2 &= \\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2}\n\\end{aligned}\\]\nWe can immediately infer several things. If we normalize the result, the product is another Gaussian. If one Gaussian is the likelihood, and the second is the prior, then the mean is a scaled sum of the prior and the measurement. The variance is a combination of the variances of the prior and measurement. Finally, the variances are completely unaffected by the values of the mean!\nWe put this in Bayesian terms like so:\n\\[\\begin{aligned}\n\\mathcal N(\\mu, \\sigma^2) &= \\| prior \\cdot likelihood \\|\\\\\n&= \\| \\mathcal{N}(\\bar\\mu, \\bar\\sigma^2)\\cdot \\mathcal{N}(\\mu_z, \\sigma_z^2) \\|\\\\\n&= \\mathcal N(\\frac{\\bar\\sigma^2 \\mu_z + \\sigma_z^2 \\bar\\mu}{\\bar\\sigma^2 + \\sigma_z^2},\\frac{\\bar\\sigma^2\\sigma_z^2}{\\bar\\sigma^2 + \\sigma_z^2})\n\\end{aligned}\\]\nIf we implemented that in a function gaussian_multiply() we could implement our filter’s update step as\n\ndef gaussian_multiply(g1, g2):\n    mean = (g1.var * g2.mean + g2.var * g1.mean) / (g1.var + g2.var)\n    variance = (g1.var * g2.var) / (g1.var + g2.var)\n    return gaussian(mean, variance)\n\ndef update(prior, likelihood):\n    posterior = gaussian_multiply(likelihood, prior)\n    return posterior\n\n# test the update function\npredicted_pos = gaussian(10., .2**2)\nmeasured_pos = gaussian(11., .1**2)\nestimated_pos = update(predicted_pos, measured_pos)\nestimated_pos\n\n𝒩(μ=10.800, 𝜎²=0.008)\n\n\nPerhaps this would be clearer if we used more specific names:\ndef update_dog(dog_pos, measurement):\n    estimated_pos = gaussian_multiply(measurement, dog_pos)\n    return estimated_pos  \nThat is less abstract, which perhaps helps with comprehension, but it is poor coding practice. We are writing a Kalman filter that works for any problem, not just tracking dogs in a hallway, so we won’t use variable names with ‘dog’ in them. Also, this form obscures the fact that we are multiplying the likelihood by the prior.\nWe have the majority of our filter implemented, but I fear this step is still a bit confusing. I’ve asserted that we can multiply Gaussians and that it correctly performs the update step, but why is this true? Let’s take a detour and spend some time multiplying Gaussians.\n\nUnderstanding Gaussian Multiplication\nLet’s plot the pdf of \\(\\mathcal{N}(10,\\, 1) \\times \\mathcal{N}(10,\\, 1)\\). Can you determine its shape without looking at the result? What should the new mean be? Will the curve be wider, narrower, or the same as \\(\\mathcal{N}(10,\\, 1)\\)?\n\nz = gaussian(10., 1.)  # Gaussian N(10, 1)\n\nproduct = gaussian_multiply(z, z)\n\nxs = np.arange(5, 15, 0.1)\nys = [stats.gaussian(x, z.mean, z.var) for x in xs]\nplt.plot(xs, ys, label='$\\mathcal{N}(10,1)$')\n\nys = [stats.gaussian(x, product.mean, product.var) for x in xs]\nplt.plot(xs, ys, label='$\\mathcal{N}(10,1) \\\\times \\mathcal{N}(10,1)$', ls='--')\nplt.legend()\nprint(product)\n\nThe result of the multiplication is taller and narrow than the original Gaussian but the mean is unchanged. Does this match your intuition?\nThink of the Gaussians as two measurements. If I measure twice and get 10 meters each time, I should conclude that the length is close to 10 meters. Thus the mean should be 10. It would make no sense to conclude the length is actually 11, or 9.5. Also, I am more confident with two measurements than with one, so the variance of the result should be smaller.\n“Measure twice, cut once” is a well known saying. Gaussian multiplication is a mathematical model of this physical fact.\nI’m unlikely to get the same measurement twice in a row. Now let’s plot the pdf of \\(\\mathcal{N}(10.2,\\, 1) \\times \\mathcal{N}(9.7,\\, 1)\\). What do you think the result will be? Think about it, and then look at the graph.\n\nm = 3.3\nf'$({m},{m})$'\n\n'$(3.3,3.3)$'\n\n\n\ndef plot_products(g1, g2): \n    plt.figure()\n    product = gaussian_multiply(g1, g2)\n\n    xs = np.arange(5, 15, 0.1)\n    ys = [stats.gaussian(x, g1.mean, g1.var) for x in xs]\n    plt.plot(xs, ys, label='$\\mathcal{N}$' + f'$({g1.mean},{g1.var})$')\n\n    ys = [stats.gaussian(x, g2.mean, g2.var) for x in xs]\n    plt.plot(xs, ys, label='$\\mathcal{N}$' + '$({g2.mean},{ge.var})$')\n\n    ys = [stats.gaussian(x, product.mean, product.var) for x in xs]\n    plt.plot(xs, ys, label='product', ls='--')\n    plt.legend();\n    plt.show()\n    \nz1 = gaussian(10.2, 1)\nz2 = gaussian(9.7, 1)\n \nplot_products(z1, z2)\n\nIf you ask two people to measure the distance of a table from a wall, and one gets 10.2 meters, and the other got 9.7 meters, your best guess must be the average, 9.95 meters if you trust the skills of both equally.\nRecall the g-h filter. We agreed that if I weighed myself on two scales, and the first read 160 lbs while the second read 170 lbs, and both were equally accurate, the best estimate was 165 lbs. Furthermore I should be a bit more confident about 165 lbs vs 160 lbs or 170 lbs because I now have two readings, both near this estimate, increasing my confidence that neither is wildly wrong.\nThis becomes counter-intuitive in more complicated situations, so let’s consider it further. Perhaps a more reasonable assumption would be that one person made a mistake, and the true distance is either 10.2 or 9.7, but certainly not 9.95. Surely that is possible. But we know we have noisy measurements, so we have no reason to think one of the measurements has no noise, or that one person made a gross error that allows us to discard their measurement. Given all available information, the best estimate must be 9.95.\nIn the update step of the Kalman filter we are not combining two measurements, but one measurement and the prior, our estimate before incorporating the measurement. We went through this logic for the g-h filter. It doesn’t matter if we are incorporating information from two measurements, or a measurement and a prediction, the math is the same.\nLet’s look at that. I’ll create a fairly inaccurate prior of \\(\\mathcal N(8.5, 1.5)\\) and a more accurate measurement of \\(\\mathcal N(10.2, 0.5).\\) By “accurate” I mean the sensor variance is smaller than the prior’s variance, not that I somehow know that the dog is closer to 10.2 than 8.5. Next I’ll plot the reverse relationship: an accurate prior of \\(\\mathcal N(8.5, 0.5)\\) and a inaccurate measurement of \\(\\mathcal N(10.2, 1.5)\\).\n\nprior, z = gaussian(8.5, 1.5), gaussian(10.2, 0.5)\nplot_products(prior, z)\n\nprior, z = gaussian(8.5, 0.5), gaussian(10.2, 1.5)\nplot_products(prior, z)\n\nThe result is a Gaussian that is taller than either input. This makes sense - we have incorporated information, so our variance should have been reduced. And notice how the result is far closer to the the input with the smaller variance. We have more confidence in that value, so it makes sense to weight it more heavily.\nIt seems to work, but is it really correct? There is more to say about this, but I want to get a working filter going so you can experience it in concrete terms. After that we will revisit Gaussian multiplication and determine why it is correct.\n\n\nInteractive Example\nThis interactive code provides sliders to alter the mean and variance of two Gaussians that are being multiplied together. As you move the sliders the plot is redrawn. Place your cursor inside the code cell and press CTRL+Enter to execute it.\n\nfrom ipywidgets import interact\n\ndef interactive_gaussian(m1, m2, v1, v2):\n    g1 = gaussian(m1, v1)\n    g2 = gaussian(m2, v2)\n    plot_products(g1, g2)\n    \ninteract(interactive_gaussian,\n         m1=(5, 10., .5), m2=(10, 15, .5), \n         v1=(.1, 2, .1), v2=(.1, 2, .1));"
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#first-kalman-filter",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#first-kalman-filter",
    "title": "One Dimensional Kalman Filters",
    "section": "First Kalman Filter",
    "text": "First Kalman Filter\nLet’s get back to concrete terms and implement a Kalman filter. We’ve implemented the update() and predict() functions. We just need to write some boilerplate code to simulate a dog and create the measurements. I’ve put a DogSimulation class in kf_internal to avoid getting distracted with that task.\nThis boilerplate code sets up the problem by definine the means, variances, and generating the simulated dog movement.\n\nimport kf_book.kf_internal as kf_internal\nfrom kf_book.kf_internal import DogSimulation\n\nnp.random.seed(13)\n\nprocess_var = 1. # variance in the dog's movement\nsensor_var = 2. # variance in the sensor\n\nx = gaussian(0., 20.**2)  # dog's position, N(0, 20**2)\nvelocity = 1\ndt = 1. # time step in seconds\nprocess_model = gaussian(velocity*dt, process_var) # displacement to add to x\n  \n# simulate dog and get measurements\ndog = DogSimulation(\n    x0=x.mean, \n    velocity=process_model.mean, \n    measurement_var=sensor_var, \n    process_var=process_model.var)\n\n# create list of measurements\nzs = [dog.move_and_sense() for _ in range(10)]\n\nAnd here is the Kalman filter.\n\nprint('PREDICT\\t\\t\\tUPDATE')\nprint('     x      var\\t\\t  z\\t    x      var')\n\n# perform Kalman filter on measurement z\nfor z in zs:    \n    prior = predict(x, process_model)\n    likelihood = gaussian(z, sensor_var)\n    x = update(prior, likelihood)\n\n    kf_internal.print_gh(prior, x, z)\n\nprint()\nprint(f'final estimate:        {x.mean:10.3f}')\nprint(f'actual final position: {dog.x:10.3f}')\n\nPREDICT         UPDATE\n     x      var       z     x      var\n  1.000  401.000    1.354     1.352   1.990\n  2.352    2.990    1.882     2.070   1.198\n  3.070    2.198    4.341     3.736   1.047\n  4.736    2.047    7.156     5.960   1.012\n  6.960    2.012    6.939     6.949   1.003\n  7.949    2.003    6.844     7.396   1.001\n  8.396    2.001    9.847     9.122   1.000\n 10.122    2.000    12.553   11.338   1.000\n 12.338    2.000    16.273   14.305   1.000\n 15.305    2.000    14.800   15.053   1.000\n\nfinal estimate:            15.053\nactual final position:     14.838\n\n\nHere is an animation of the filter. Predictions are plotted with a red triangle. After the prediction, the filter receives the next measurement, plotted as a black circle. The filter then forms an estimate part way between the two.\n\nfrom kf_book import book_plots as book_plots\nfrom ipywidgets.widgets import IntSlider\n\n# save output in these lists for plotting\nxs, predictions = [], []\n\nprocess_model = gaussian(velocity, process_var) \n\n# perform Kalman filter\nx = gaussian(0., 20.**2)\nfor z in zs:    \n    prior = predict(x, process_model)\n    likelihood = gaussian(z, sensor_var)\n    x = update(prior, likelihood)\n\n    # save results\n    predictions.append(prior.mean)\n    xs.append(x.mean)\n\ndef plot_filter(step):\n    plt.cla()\n    step -= 1\n    i = step // 3 + 1\n \n    book_plots.plot_predictions(predictions[:i])    \n    if step % 3 == 0:\n        book_plots.plot_measurements(zs[:i-1])\n        book_plots.plot_filter(xs[:i-1])\n    elif step % 3 == 1:\n        book_plots.plot_measurements(zs[:i])\n        book_plots.plot_filter(xs[:i-1])\n    else:\n        book_plots.plot_measurements(zs[:i])\n        book_plots.plot_filter(xs[:i])\n    \n    plt.xlim(-1, 10)\n    plt.ylim(0, 20)\n    plt.legend(loc=2);\n    plt.show()\n    \ninteract(plot_filter, step=IntSlider(value=1, min=1, max=len(predictions)*3));\n\n\n\n\nI’ve plotted the prior (labeled prediction), the measurements, and the filter output. For each iteration of the loop we form a prior, take a measurement, form a likelihood from the measurement, and then incorporate the likelihood into the prior.\nIf you look at the plot you can see that the filter estimate is always between the measurement and prediction. Recall that for the g-h filter we argued that the estimate must always be between the measurement and prior. It makes no sense to choose a value outside of the two values. If I predict I am at 10, but measure that I am at 9, it would be foolish to decide that I must be at 8, or 11."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#code-walkthrough",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#code-walkthrough",
    "title": "One Dimensional Kalman Filters",
    "section": "Code Walkthrough",
    "text": "Code Walkthrough\nNow let’s walk through the code.\nprocess_var = 1.\nsensor_var = 2.\nThese are the variances for the process model and sensor. The meaning of sensor variance should be clear - it is how much variance there is in each measurement. The process variance is how much error there is in the process model. We are predicting that at each time step the dog moves forward one meter. Dogs rarely do what we expect, and things like hills or the whiff of a squirrel will change his progress. If this was a robot responding to digital commands the performance would be much better, and perhaps the variance would be \\(\\sigma^2=.05\\). These are not ‘magic’ numbers; the square root of the variance is the distance error in meters. It is easy to get a Kalman filter working by just plugging in numbers, but if the numbers do not reflect reality the performance of the filter will be poor.\nx = gaussian(0., 20.**2)\nThis is the dog’s initial position expressed as a Gaussian. The position is 0 meters, and the variance to 400 m\\(^2\\), which is a standard deviation of 20 meters. You can think of this as saying “I believe with 99.7% accuracy the position is 0 plus or minus 60 meters”. This is because with Gaussians ~99.7% of values fall within \\(\\pm3\\sigma\\) of the mean.\nprocess_model = gaussian(velocity, process_var)\nThis is the process model - the description of how we think the dog moves. How do I know the velocity? Magic? Consider it a prediction, or perhaps we have a secondary velocity sensor. If this is a robot then this would be a control input to the robot. In subsequent chapters we will learn how to handle situations where you don’t have a velocity sensor or input, so please accept this simplification for now.\nNext we initialize the simulation and create 10 measurements:\ndog = DogSimulation(\n    x0=x.mean, \n    velocity=process_model.mean, \n    measurement_var=sensor_var, \n    process_var=process_model.var)\n\nzs = [dog.move_and_sense() for _ in range(10)]\nNow we enter our predict() ... update() loop.\nfor z in zs:\n    prior = predict(x, process_model)\n    likelihood = gaussian(z, sensor_var)\n    x = update(prior, likelihood)\nThe first time through the loop prior is (1.0, 401.0), as can be seen in the printed table. After the prediction, we believe that we are at 1.0, and the variance is now 401, up from 400. The variance got worse, which is what always happens during the prediction step because it involves a loss of information.\nThen we call the update function using prior as the current position.\nFor this I get this as the result: pos = (1.352, 1.990), z = 1.354.\nWhat is happening? The dog is actually at 1.0 but the measured position is 1.354 due to sensor noise. That is pretty far from the predicted value of 1. The variance of the prior is 401 m\\(^2\\). A large variance implies that confidence is very low, so the filter estimates the position to be very close to the measurement: 1.352.\nNow look at the variance: 1.99 m\\(^2\\). It has dropped tremendously from 401 m\\(^2\\). Why? Well, the RFID has a reasonably small variance of 2.0 m\\(^2\\), so we trust it far more than the prior. However, the previous belief does contain a bit of useful information, so our variance is now slightly smaller than 2.0.\nNow the software loops, calling predict() and update() in turn. By the end the final estimated position is 15.053 vs the actual position of 14.838. The variance has converged to 1.0 m\\(^2\\).\nNow look at the plot. The noisy measurements are plotted with black circles, and the filter results are drawn with a solid blue line. Both are quite noisy, but notice how much noisier the measurements are. I plotted the prediction (prior) with red triangles. The estimate always lies between the prior and the measurement. This is your first Kalman filter and it seems to work!\nThe filtering is implemented in only a few lines of code. Most of the code is either initialization, storing of data, simulating the dog movement, and printing results. The code that performs the filtering is very succinct:\nprior = predict(x, process_model)\nlikelihood = gaussian(z, sensor_var)\nx = update(prior, likelihood)\nIf we didn’t use the predict and update functions the code might be:\nfor z in zs:\n    # predict\n    dx = velocity*dt\n    pos = pos + dx\n    var = var + process_var\n\n    # update\n    pos  = (var*z + sensor_var*pos) / (var + sensor_var)\n    var = (var * sensor_var) / (var + sensor_var)\nJust 5 lines of very simple math implements the entire filter!\nIn this example I only plotted 10 data points so the output from the print statements would not overwhelm us. Now let’s look at the filter’s performance with more data. The variance is plotted as a lightly shaded yellow area between dotted lines. I’ve increased the size of the process and sensor variance so they are easier to see on the chart - for a real Kalman filter of course you will not be randomly changing these values.\n\nprocess_var = 2.\nsensor_var = 4.5\nx = gaussian(0., 400.)\nprocess_model = gaussian(1., process_var)\nN = 25\n\ndog = DogSimulation(x.mean, process_model.mean, sensor_var, process_var)\nzs = [dog.move_and_sense() for _ in range(N)]\n\nxs, priors = np.zeros((N, 2)), np.zeros((N, 2))\nfor i, z in enumerate(zs):\n    prior = predict(x, process_model)    \n    x = update(prior, gaussian(z, sensor_var))\n    priors[i] = prior\n    \n    xs[i] = x\n\nbook_plots.plot_measurements(zs)\nbook_plots.plot_filter(xs[:, 0], var=priors[:, 1])\nbook_plots.plot_predictions(priors[:, 0])\nbook_plots.show_legend()\nkf_internal.print_variance(xs)\n\nHere we can see that the variance converges to 2.1623 in 9 steps. This means that we have become very confident in our position estimate. It is equal to \\(\\sigma=1.47\\) meters. Contrast this to the sensor’s \\(\\sigma=2.12\\) meters. The first few measurements are unsure due to our uncertainty of the initial position, but the filter quickly converges to an estimate with lower variance than the sensor!\nThis code fully implements a Kalman filter. If you have tried to read the literature you are perhaps surprised, because this looks nothing like the endless pages of math in those books. So long as we worry about using the equations rather than deriving them the topic is approachable. Moreover, I hope you’ll agree that you have a decent intuitive grasp of what is happening. We represent beliefs with Gaussians, and they get better over time because more measurements means we have more data to work with.\n\nExercise: Modify Variance Values\nModify the values of process_var and sensor_var and note the effect on the filter and on the variance. Which has a larger effect on the variance convergence? For example, which results in a smaller variance:\nprocess_var = 40\nsensor_var = 2\nor:\nprocess_var = 2\nsensor_var = 40\n\n\nKF Animation\nIf you are reading this in a browser you will be able to see an animation of the filter tracking the dog directly below this sentence. \nThe top plot shows the output of the filter in green, and the measurements with a dashed red line. The bottom plot shows the Gaussian at each step.\nWhen the track first starts you can see that the measurements varies quite a bit from the initial prediction. At this point the Gaussian probability is small (the curve is low and wide) so the filter does not trust its prediction. As a result, the filter adjusts its estimate a large amount. As the filter innovates you can see that as the Gaussian becomes taller, indicating greater certainty in the estimate, the filter’s output becomes very close to a straight line. At x = 15 and greater you can see that there is a large amount of noise in the measurement, but the filter does not react much to it compared to how much it changed for the first noisy measurement."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#kalman-gain",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#kalman-gain",
    "title": "One Dimensional Kalman Filters",
    "section": "Kalman Gain",
    "text": "Kalman Gain\nWe see that the filter works. Now let’s go back to the math to understand what is happening. The posterior \\(x\\) is computed as the likelihood times the prior (\\(\\mathcal L \\bar x\\)), where both are Gaussians.\nTherefore the mean of the posterior is given by:\n\\[\n\\mu=\\frac{\\bar\\sigma^2\\, \\mu_z + \\sigma_z^2 \\, \\bar\\mu} {\\bar\\sigma^2 + \\sigma_z^2}\n\\]\nI use the subscript \\(z\\) to denote the measurement. We can rewrite this as:\n\\[\\mu = \\left( \\frac{\\bar\\sigma^2}{\\bar\\sigma^2 + \\sigma_z^2}\\right) \\mu_z + \\left(\\frac{\\sigma_z^2}{\\bar\\sigma^2 + \\sigma_z^2}\\right)\\bar\\mu\\]\nIn this form it is easy to see that we are scaling the measurement and the prior by weights:\n\\[\\mu = W_1 \\mu_z + W_2 \\bar\\mu\\]\nThe weights sum to one because the denominator is a normalization term. We introduce a new term, \\(K=W_1\\), giving us:\n\\[\\begin{aligned}\n\\mu &= K \\mu_z + (1-K) \\bar\\mu\\\\\n&= \\bar\\mu + K(\\mu_z - \\bar\\mu)\n\\end{aligned}\\]\nwhere\n\\[K = \\frac {\\bar\\sigma^2}{\\bar\\sigma^2 + \\sigma_z^2}\\]\n\\(K\\) is the Kalman gain. It’s the crux of the Kalman filter. It is a scaling term that chooses a value partway between \\(\\mu_z\\) and \\(\\bar\\mu\\).\nLet’s work a few examples. If the measurement is nine times more accurate than the prior, then \\(\\bar\\sigma^2 = 9\\sigma_z^2\\), and\n\\[\\begin{aligned}\n\\mu&=\\frac{9 \\sigma_z^2 \\mu_z + \\sigma_z^2\\, \\bar\\mu} {9 \\sigma_z^2 + \\sigma_\\mathtt{z}^2} \\\\\n&= \\left(\\frac{9}{10}\\right) \\mu_z + \\left(\\frac{1}{10}\\right) \\bar\\mu\n\\end{aligned}\n\\]\nHence \\(K = \\frac 9 {10}\\), and to form the posterior we take nine tenths of the measurement and one tenth of the prior.\nIf the measurement and prior are equally accurate, then \\(\\bar\\sigma^2 = \\sigma_z^2\\) and\n\\[\\begin{gathered}\n\\mu=\\frac{\\sigma_z^2\\,  (\\bar\\mu + \\mu_z)}{2\\sigma_\\mathtt{z}^2} \\\\\n= \\left(\\frac{1}{2}\\right)\\bar\\mu + \\left(\\frac{1}{2}\\right)\\mu_z\n\\end{gathered}\\]\nwhich is the average of the two means. It makes intuitive sense to take the average of two equally accurate values.\nWe can also express the variance in terms of the Kalman gain:\n\\[\\begin{aligned}\n\\sigma^2 &= \\frac{\\bar\\sigma^2 \\sigma_z^2 } {\\bar\\sigma^2 + \\sigma_z^2} \\\\\n&= K\\sigma_z^2 \\\\\n&= (1-K)\\bar\\sigma^2\n\\end{aligned}\\]\nWe can understand this by looking at this chart:\n\nimport kf_book.book_plots as book_plots\nbook_plots.show_residual_chart()\n\n\n\n\n\n\n\n\nThe Kalman gain \\(K\\) is a scale factor that chooses a value along the residual. This leads to an alternative but equivalent implementation for update() and predict():\n\ndef update(prior, measurement):\n    x, P = prior        # mean and variance of prior\n    z, R = measurement  # mean and variance of measurement\n    \n    y = z - x        # residual\n    K = P / (P + R)  # Kalman gain\n\n    x = x + K*y      # posterior\n    P = (1 - K) * P  # posterior variance\n    return gaussian(x, P)\n\ndef predict(posterior, movement):\n    x, P = posterior # mean and variance of posterior\n    dx, Q = movement # mean and variance of movement\n    x = x + dx\n    P = P + Q\n    return gaussian(x, P)\n\nWhy have I written it in this form, and why have I chosen these terrible variable names? A few related reasons. A majority of books and papers present the Kalman filter in this form. My derivation of the filter from Bayesian principles is not unknown, but it is not used nearly as often. Alternative derivations naturally lead to this form of the equations. Also, the equations for the multivariate Kalman filter look almost exactly like these equations. So, you need to learn and understand them.\nWhere do the names z, P, Q, and R come from? You will see them used in the rest of this book. In the literature \\(R\\) is nearly universally used for the measurement noise, \\(Q\\) for the process noise and \\(P\\) for the variance of the state. Using \\(z\\) for the measurement is common, albeit not universal. Almost every book and paper you read will use these variable names. Get used to them.\nThis is also a powerful way to think about filtering. This is the way we reasoned about the g-h filter. It emphasizes taking the residual \\(y = \\mu_z - \\bar\\mu\\), finding the Kalman gain as a ratio of our uncertainty in the prior and measurement \\(K = P/(P+R)\\), and computing the posterior by adding \\(Ky\\) to the prior.\nThe Bayesian aspect is obscured in this form, as is the fact that we are multiplying the likelihood by the prior. Both viewpoints are equivalent because the math is identical. I chose the Bayesian approach because I think it give a much more intuitive yet deep understanding of the probabilistic reasoning. This alternative form using \\(K\\) gives a deep understanding of what is known as the orthogonal projection approach. Dr. Kalman used that derivation, not Bayesian reasoning, when he invented this filter. You will understand more about this in the next few chapters."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#full-description-of-the-algorithm",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#full-description-of-the-algorithm",
    "title": "One Dimensional Kalman Filters",
    "section": "Full Description of the Algorithm",
    "text": "Full Description of the Algorithm\nRecall the diagram we used for the g-h filter: \nWe’ve been doing the same thing in this chapter. The Kalman filter makes a prediction, takes a measurement, and then forms a new estimate somewhere between the two.\nThis is extremely important to understand: Every filter in this book implements the same algorithm, just with different mathematical details. The math can become challenging in later chapters, but the idea is easy to understand.\nIt is important to see past the details of the equations of a specific filter and understand what the equations are calculating and why. There are a tremendous number of filters. They all use different math to implement the same algorithm. The choice of math affects the quality of results and what problems can be represented, but not the underlying ideas.\nHere is the generic algorithm:\nInitialization\n1. Initialize the state of the filter\n2. Initialize our belief in the state\nPredict\n1. Use system behavior to predict state at the next time step\n2. Adjust belief to account for the uncertainty in prediction\nUpdate\n1. Get a measurement and associated belief about its accuracy\n2. Compute residual between estimated state and measurement\n3. Compute scaling factor based on whether the measurement\nor prediction is more accurate\n4. set state between the prediction and measurement based \non scaling factor\n5. update belief in the state based on how certain we are \nin the measurement\nYou will be hard pressed to find a Bayesian filter algorithm that does not fit into this form. Some filters will not include some aspects, such as error in the prediction, and others will have very complicated methods of computation, but this is what they all do.\nThe equations for the univariate Kalman filter are:\nPredict\n\\(\\begin{array}{|l|l|l|} \\hline \\text{Equation} & \\text{Implementation} & \\text{Kalman Form}\\\\ \\hline  \\bar x = x + f_x & \\bar\\mu = \\mu + \\mu_{f_x} & \\bar x = x + dx\\\\ & \\bar\\sigma^2 = \\sigma^2 + \\sigma_{f_x}^2 & \\bar P = P + Q\\\\ \\hline \\end{array}\\)\nUpdate\n\\(\\begin{array}{|l|l|l|} \\hline \\text{Equation} & \\text{Implementation}& \\text{Kalman Form}\\\\ \\hline  x = \\| \\mathcal L\\bar x\\| & y = z - \\bar\\mu & y = z - \\bar x\\\\  & K = \\frac {\\bar\\sigma^2} {\\bar\\sigma^2 + \\sigma_z^2} & K = \\frac {\\bar P}{\\bar P+R}\\\\  & \\mu = \\bar \\mu + Ky & x = \\bar x + Ky\\\\  & \\sigma^2 = \\frac {\\bar\\sigma^2 \\sigma_z^2} {\\bar\\sigma^2 + \\sigma_z^2} & P = (1-K)\\bar P\\\\ \\hline \\end{array}\\)"
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#comparison-with-g-h-and-discrete-bayes-filters",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#comparison-with-g-h-and-discrete-bayes-filters",
    "title": "One Dimensional Kalman Filters",
    "section": "Comparison with g-h and discrete Bayes Filters",
    "text": "Comparison with g-h and discrete Bayes Filters\nNow is a good time to understand the differences between these three filters in terms of how we model errors. For the g-h filter we modeled our measurements as shown in this graph:\n\nbook_plots.plot_errorbars([(160, 3, 'A'), (170, 9, 'B')], xlims=(150, 180))\n\nSensor A returned a measurement of 160, and sensor B returned 170. The bars are error bars - they illustrate the possible range of error for the measurement. Hence, the actual value that A is measuring can be between 157 to 163, and B is measuring a value between 161 to 179.\nI did not define it at the time, but this is a [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)). A uniform distribution assigns equal probability to any event in the range. According to this model it is equally likely for sensor A to read 157, 160, or 163. Any value outside these ranges have 0 probability.\nWe can model this situation with Gaussians. I’ll use \\(\\mathcal{N}(160, 3^2)\\) for sensor A, and \\(\\mathcal{N}(170, 9^2)\\) for sensor B. I’ve plotted these below with the uniform distribution error bars for comparison.\n\nxs = np.arange(145, 190, 0.1)\nys = [stats.gaussian(x, 160, 3**2) for x in xs]\nplt.plot(xs, ys, label='A', color='g')\n\nys = [stats.gaussian(x, 170, 9**2) for x in xs]\nplt.plot(xs, ys, label='B', color='b')\nplt.legend();\nplt.errorbar(160, [0.04], xerr=[3], fmt='o', color='g', capthick=2, capsize=10)    \nplt.errorbar(170, [0.015], xerr=[9], fmt='o', color='b', capthick=2, capsize=10);\n\nUsing a uniform or Gaussian distribution is a modeling choice. Neither exactly describes reality. In most cases the Gaussian distribution is more realistic. Most sensors are more likely to return readings near the value being measured, and unlikely to return a reading far from that value. The Gaussian models this tendency. In contrast the uniform distribution assumes that any measurement within a range is equally likely.\nNow let’s see the discrete distribution used in the discrete Bayes filter. This model divides the range of possible values into discrete ranges and assigns a probability to each bucket. This assignment can be entirely arbitrary so long as the probabilities sum to one.\nLet’s plot the data for one sensor using a uniform distribution, a Gaussian distribution, and a discrete distribution.\n\nfrom random import random\nxs = np.arange(145, 190, 0.1)\nys = [stats.gaussian(x, 160, 3**2) for x in xs]\nbelief = np.array([random() for _ in range(40)])\nbelief = belief / sum(belief)\n\nx = np.linspace(155, 165, len(belief))\nplt.gca().bar(x, belief, width=0.2)\nplt.plot(xs, ys, label='A', color='g')\nplt.errorbar(160, [0.04], xerr=[3], fmt='o', color='k', capthick=2, capsize=10)    \nplt.xlim(150, 170);\n\nI used random numbers to form the discrete distribution to illustrate that it can model any arbitrary probability distribution. This provides it with enormous power. With enough discrete buckets we can model the error characteristics of any sensor no matter how complicated. But with this power comes mathematical intractability. Multiplying or adding Gaussians takes two lines of math, and the result is another Gaussian. This regularity allows us to perform powerful analysis on the performance and behavior of our filters. Multiplying or adding a discrete distribution requires looping over the data, and we have no easy way to characterize the result. Analyzing the performance characteristics of a filter based on a discrete distribution is extremely difficult to impossible.\nThere is no ‘correct’ choice here. Later in the book we will introduce the particle filter which uses a discrete distribution. It is an extremely powerful technique because it can handle arbitrarily complex situations. This comes at the cost of slow performance, and resistance to analytical analysis.\nFor now we will ignore these matters and return to using Gaussians for the next several chapters. As we progress you will learn the strengths and limitations of using Gaussians in our mathematical models."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#introduction-to-designing-a-filter",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#introduction-to-designing-a-filter",
    "title": "One Dimensional Kalman Filters",
    "section": "Introduction to Designing a Filter",
    "text": "Introduction to Designing a Filter\nSo far we have developed filters for a position sensor. We are used to this problem by now, and may feel ill-equipped to implement a Kalman filter for a different problem. To be honest, there is still quite a bit of information missing from this presentation. Following chapters will fill in the gaps. Still, let’s get a feel for it by designing and implementing a Kalman filter for a thermometer. The sensor for the thermometer outputs a voltage that corresponds to the temperature that is being measured. We have read the manufacturer’s specifications for the sensor, and it tells us that the sensor exhibits white noise with a standard deviation of 0.13 volts.\nWe can simulate the temperature sensor measurement with this function:\n\ndef volt(voltage, std):\n    return voltage + (randn() * std)\n\nNow we need to write the Kalman filter processing loop. As with our previous problem, we need to perform a cycle of predicting and updating. The sensing step probably seems clear - call volt() to get the measurement, pass the result into update() method, but what about the predict step? We do not have a sensor to detect ‘movement’ in the voltage, and for any small duration we expect the voltage to remain constant. How shall we handle this?\nAs always, we will trust in the math. We have no known movement, so we will set that to zero. However, that means that we are predicting that the temperature will never change. If that is true, then over time we should become extremely confident in our results. Once the filter has enough measurements it will become very confident that it can predict the subsequent temperatures, and this will lead it to ignoring measurements that result due to an actual temperature change. This is called a smug filter, and is something you want to avoid. So we will add a bit of error to our prediction step to tell the filter not to discount changes in voltage over time. In the code below I set process_var = .05**2. This is the expected variance in the change of voltage over each time step. I chose this value merely to be able to show how the variance changes through the update and predict steps. For a real sensor you would set this value for the actual amount of change you expect. For example, this would be an extremely small number if it is a thermometer for ambient air temperature in a house, and a high number if this is a thermocouple in a chemical reaction chamber. We will say more about selecting the actual value in the later chapters.\nLet’s see what happens.\n\ntemp_change = 0\nvoltage_std = .13\nprocess_var = .05**2\nactual_voltage = 16.3\n\nx = gaussian(25., 1000.) # initial state\nprocess_model = gaussian(0., process_var)\n\nN = 50\nzs = [volt(actual_voltage, voltage_std) for i in range(N)]\nps = []\nestimates = []\n\nfor z in zs:\n    prior = predict(x, process_model)\n    x = update(prior, gaussian(z, voltage_std**2))\n\n    # save for latter plotting\n    estimates.append(x.mean)\n    ps.append(x.var)\n\n# plot the filter output and the variance\nbook_plots.plot_measurements(zs)\nbook_plots.plot_filter(estimates, var=np.array(ps))\nbook_plots.show_legend()\nplt.ylim(16, 17)\nbook_plots.set_labels(x='step', y='volts')\nplt.show()\n    \nplt.plot(ps)\nplt.title('Variance')\nprint(f'Variance converges to {ps[-1]:.3f}')\n\nThe first plot shows the individual sensor measurements vs the filter output. Despite a lot of noise in the sensor we quickly discover the approximate voltage of the sensor. In the run I just completed at the time of authorship, the last voltage output from the filter is \\(16.213\\), which is quite close to the \\(16.4\\) used by the volt() function. On other runs I have gotten larger and smaller results.\nSpec sheets are what they sound like - specifications. Any individual sensor will exhibit different performance based on normal manufacturing variations. Values are often maximums - the spec is a guarantee that the performance will be at least that good. If you buy an expensive piece of equipment it often comes with a sheet of paper displaying the test results of your specific item; this is usually very trustworthy. On the other hand, if this is a cheap sensor it is likely it received little to no testing prior to being sold. Manufacturers typically test a small subset of their output to verify that a sample falls within the desired performance range. If you have a critical application you will need to read the specification sheet carefully to figure out exactly what they mean by their ranges. Do they guarantee their number is a maximum, or is it, say, the \\(3\\sigma\\) error rate? Is every item tested? Is the variance normal, or some other distribution? Finally, manufacturing is not perfect. Your part might be defective and not match the performance on the sheet.\nFor example, I am looking at a data sheet for an airflow sensor. There is a field Repeatability, with the value \\(\\pm 0.50\\%\\). Is this a Gaussian? Is there a bias? For example, perhaps the repeatability is nearly \\(0.0\\%\\) at low temperatures, and always nearly \\(+0.50\\%\\) at high temperatures. Data sheets for electrical components often contain a section of “Typical Performance Characteristics”. These are used to capture information that cannot be easily conveyed in a table. For example, I am looking at a chart showing output voltage vs current for a LM555 timer. There are three curves showing the performance at different temperatures. The response is ideally linear, but all three lines are curved. This clarifies that errors in voltage outputs are probably not Gaussian - in this chip’s case higher temperatures lead to lower voltage output, and the voltage output is quite nonlinear if the input current is very high.\nAs you might guess, modeling the performance of your sensors is one of the harder parts of creating a Kalman filter that performs well.\n\nAnimation\nFor those reading this in a browser here is an animation showing the filter working. If you are not using a browser you can see this plot at https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/animations/05_volt_animate.gif.\n\nThe top plot in the animation draws a green line for the predicted next voltage, then a red ‘+’ for the actual measurement, draws a light red line to show the residual, and then draws a blue line to the filter’s output. You can see that when the filter starts the corrections made are quite large, but after only a few updates the filter only adjusts its output by a small amount even when the measurement is far from it.\nThe lower plot shows the Gaussian belief as the filter innovates. When the filter starts the Gaussian curve is centered over 25, our initial guess for the voltage, and is very wide and short due to our initial uncertainty. But as the filter innovates, the Gaussian quickly moves to about 16.0 and becomes taller, reflecting the growing confidence that the filter has in it’s estimate for the voltage. You will also note that the Gaussian’s height bounces up and down a little bit. If you watch closely you will see that the Gaussian becomes a bit shorter and more spread out during the prediction step, and becomes taller and narrower as the filter incorporates another measurement.\nThink of this animation in terms of the g-h filter. At each step the g-h filter makes a prediction, takes a measurement, computes the residual (the difference between the prediction and the measurement), and then selects a point on the residual line based on the scaling factor \\(g\\). The Kalman filter is doing exactly the same thing, except that the scaling factor \\(g\\) varies with time. As the filter becomes more confident in its state the scaling factor favors the filter’s prediction over the measurement."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-extreme-amounts-of-noise",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-extreme-amounts-of-noise",
    "title": "One Dimensional Kalman Filters",
    "section": "Example: Extreme Amounts of Noise",
    "text": "Example: Extreme Amounts of Noise\nWith the dog filter I didn’t put a lot of noise in the signal, and I ‘guessed’ that the dog was at position 0. How does the filter perform in real world conditions? I will start by injecting more noise in the RFID sensor while leaving the process variance at 2 m\\(^2\\). I will inject an extreme amount of noise - noise that apparently swamps the actual measurement. What does your intuition say about the filter’s performance if the sensor has a standard deviation of 300 meters? In other words, an actual position of 1.0 m might be reported as 287.9 m, or -589.6 m, or any other number in roughly that range. Think about it before you scroll down.\n\nsensor_var = 300.**2\nprocess_var = 2.\nprocess_model = gaussian(1., process_var)\npos = gaussian(0., 500.)\nN = 1000\ndog = DogSimulation(pos.mean, 1., sensor_var, process_var)\nzs = [dog.move_and_sense() for _ in range(N)]\nps = []\n\nfor i in range(N):\n    prior = predict(pos, process_model)    \n    pos = update(prior, gaussian(zs[i], sensor_var))\n    ps.append(pos.mean)\n\nbook_plots.plot_measurements(zs, lw=1)\nbook_plots.plot_filter(ps)\nplt.legend(loc=4);\n\nIn this example the noise is extreme yet the filter still outputs a nearly straight line! This is an astonishing result! What do you think might be the cause of this performance?\nWe get a nearly straight line because our process error is small. A small process error tells the filter that the prediction is very trustworthy, and the prediction is a straight line, so the filter outputs a nearly straight line."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-incorrect-process-variance",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-incorrect-process-variance",
    "title": "One Dimensional Kalman Filters",
    "section": "Example: Incorrect Process Variance",
    "text": "Example: Incorrect Process Variance\nThat last filter looks fantastic! Why wouldn’t we set the process variance very low, as it guarantees the result will be straight and smooth?\nThe process variance tells the filter how much the system is changing over time. If you lie to the filter by setting this number artificially low the filter will not be able to react to changes that are happening. Let’s have the dog increase his velocity by a small amount at each time step and see how the filter performs with a process variance of 0.001 m\\(^2\\).\n\nsensor_var = 20.\nprocess_var = .001\nprocess_model = gaussian(1., process_var)\npos = gaussian(0., 500.)\nN = 100\ndog = DogSimulation(pos.mean, 1, sensor_var, process_var*10000)\nzs, ps = [], []\nfor _ in range(N):\n    dog.velocity += 0.04\n    zs.append(dog.move_and_sense())\n\nfor z in zs:\n    prior = predict(pos, process_model)    \n    pos = update(prior, gaussian(z, sensor_var))\n    ps.append(pos.mean)\n\nbook_plots.plot_measurements(zs, lw=1)\nbook_plots.plot_filter(ps)\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nIt is easy to see that the filter is not correctly responding to the measurements. The measurements clearly indicate that the dog is changing speed but the filter has been told that it’s predictions are nearly perfect so it almost entirely ignores them. I encourage you to adjust the amount of movement in the dog vs process variance. We will also be studying this topic much more in the later chapters. The key point is to recognize that math requires that the variances correctly describe your system. The filter does not ‘notice’ that it is diverging from the measurements and correct itself. It computes the Kalman gain from the variance of the prior and the measurement, and forms the estimate depending on which is more accurate."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-bad-initial-estimate",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-bad-initial-estimate",
    "title": "One Dimensional Kalman Filters",
    "section": "Example: Bad Initial Estimate",
    "text": "Example: Bad Initial Estimate\nNow let’s look at the results when we make a bad initial estimate of position. To avoid obscuring the results I’ll reduce the sensor variance to 30, but set the initial position to 1000 meters. Can the filter recover from a 1000 meter error?\n\nsensor_var = 5.**2\nprocess_var = 2.\npos = gaussian(1000., 500.)\nprocess_model = gaussian(1., process_var)\nN = 100\ndog = DogSimulation(0, 1, sensor_var, process_var)\nzs = [dog.move_and_sense() for _ in range(N)]\nps = []\n\nfor z in zs:\n    prior = predict(pos, process_model)    \n    pos = update(prior, gaussian(z, sensor_var))\n    ps.append(pos.mean)\n\nbook_plots.plot_measurements(zs, lw=1)\nbook_plots.plot_filter(ps)\nplt.legend(loc=4);\n\nAgain the answer is yes! Because we are relatively sure about our belief in the sensor (\\(\\sigma^2=5^2\\)) after only the first step we have changed our position estimate from 1000 m to roughly 50 m. After another 5-10 measurements we have converged to the correct value. This is how we get around the chicken and egg problem of initial guesses. In practice we would likely assign the first measurement from the sensor as the initial value, but you can see it doesn’t matter much if we wildly guess at the initial conditions - the Kalman filter still converges so long as the filter variances are chosen to match the actual process and measurement variances."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-large-noise-and-bad-initial-estimate",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#example-large-noise-and-bad-initial-estimate",
    "title": "One Dimensional Kalman Filters",
    "section": "Example: Large Noise and Bad Initial Estimate",
    "text": "Example: Large Noise and Bad Initial Estimate\nWhat about the worst of both worlds, large noise and a bad initial estimate?\n\nsensor_var = 30000.\nprocess_var = 2.\npos = gaussian(1000., 500.)\nprocess_model = gaussian(1., process_var)\n\nN = 1000\ndog = DogSimulation(0, 1, sensor_var, process_var)\nzs = [dog.move_and_sense() for _ in range(N)]\nps = []\n\nfor z in zs:\n    prior = predict(pos, process_model) \n    pos = update(prior, gaussian(z, sensor_var))\n    ps.append(pos.mean)\n\nbook_plots.plot_measurements(zs, lw=1)\nbook_plots.plot_filter(ps)\nplt.legend(loc=4);\n\nThis time the filter struggles. Notice that the previous example only computed 100 updates, whereas this example uses 1000. By my eye it takes the filter 400 or so iterations to become reasonable accurate, but maybe over 600 before the results are good. Kalman filters are good, but we cannot expect miracles. If we have extremely noisy data and extremely bad initial conditions, this is as good as it gets.\nFinally, let’s implement the suggestion of using the first measurement as the initial position.\n\nsensor_var = 30000.\nprocess_var = 2.\nprocess_model = gaussian(1., process_var)\nN = 1000\ndog = DogSimulation(0, 1, sensor_var, process_var)\nzs = [dog.move_and_sense() for _ in range(N)]\n\npos = gaussian(zs[0], 500.)\nps = []\nfor z in zs:\n    prior = predict(pos, process_model) \n    pos = update(prior, gaussian(z, sensor_var))\n    ps.append(pos.mean)\n\nbook_plots.plot_measurements(zs, lw=1)\nbook_plots.plot_filter(ps)\nplt.legend(loc='best');\n\nThis simple change significantly improves the results. On some runs it takes 200 iterations or so to settle to a good solution, but other runs it converges very rapidly. This all depends on the amount of noise in the first measurement. A large amount of noise causes the initial estimate to be far from the dog’s position.\n200 iterations may seem like a lot, but the amount of noise we are injecting is truly huge. In the real world we use sensors like thermometers, laser range finders, GPS satellites, computer vision, and so on. None have the enormous errors in these examples. A reasonable variance for a cheap thermometer might be 0.2 C\\(^{\\circ 2}\\), and our code is using 30,000 C\\(^{\\circ 2}\\)."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#exercise---nonlinear-systems",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#exercise---nonlinear-systems",
    "title": "One Dimensional Kalman Filters",
    "section": "Exercise - Nonlinear Systems",
    "text": "Exercise - Nonlinear Systems\nOur equations for the Kalman filter are linear:\n\\[\\begin{aligned}\n\\mathcal{N}(\\bar\\mu,\\, \\bar\\sigma^2) &= \\mathcal{N}(\\mu,\\, \\sigma^2) + \\mathcal{N}(\\mu_\\mathtt{move},\\, \\sigma^2_\\mathtt{move})\\\\\n\\mathcal{N}(\\mu,\\, \\sigma^2) &= \\mathcal{N}(\\bar\\mu,\\, \\bar\\sigma^2)  \\times \\mathcal{N}(\\mu_\\mathtt{z},\\, \\sigma^2_\\mathtt{z})\n\\end{aligned}\\]\nDo you suppose that this filter works well or poorly with nonlinear systems?\nImplement a Kalman filter that uses the following equation to generate the measurement value\nfor i in range(100):\n    z = math.sin(i/3.) * 2\nAdjust the variance and initial positions to see the effect. What is, for example, the result of a very bad initial guess?\n\n#enter your code here."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#fixed-gain-filters",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#fixed-gain-filters",
    "title": "One Dimensional Kalman Filters",
    "section": "Fixed Gain Filters",
    "text": "Fixed Gain Filters\nEmbedded computers usually have extremely limited processors. Many do not have floating point circuitry. These simple equations can impose a heavy burden on the chip. This is less true as technology advances, but do not underestimate the value of spending one dollar less on a processor when you will be buying millions of them.\nIn the example above the variance of the filter converged to a fixed value. This will always happen if the variance of the measurement and process is a constant. You can take advantage of this fact by running simulations to determine what the variance converges to. Then you can hard code this value into your filter. So long as you initialize the filter to a good starting guess (I recommend using the first measurement as your initial value) the filter will perform very well. For example, the dog tracking filter can be reduced to this:\ndef update(x, z):\n    K = .13232  # experimentally derived Kalman gain\n    y = z - x   # residual\n    x = x + K*y # posterior\n    return x\n    \ndef predict(x):\n    return x + vel*dt\nI used the Kalman gain form of the update function to emphasize that we do not need to consider the variances at all. If the variances converge to a single value so does the Kalman gain."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#filterpys-implementation",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#filterpys-implementation",
    "title": "One Dimensional Kalman Filters",
    "section": "FilterPy’s Implementation",
    "text": "FilterPy’s Implementation\nFilterPy implements predict() and update(). They work not only for the univariate case developed in this chapter, but the more general multivariate case that we learn in subsequent chapters. Because of this their interface is slightly different. They do not take Gaussians as tuples, but as two separately named variables.\npredict() takes several arguments, but we will only need to use these four:\npredict(x, P, u, Q)\nx is the state of the system. P is the variance of the system. u is the movement due to the process, and Q is the noise in the process. You will need to used named arguments when you call predict() because most of the arguments are optional. The third argument to predict() is not u.\nThese may strike you as terrible variable names. They are! As I already mentioned they come from a long history of control theory, and every paper or book you read will use these names. So, we just have to get used to it. Refusing to memorize them means you will never be able to read the literature.\nLet’s try it for the state \\(\\mathcal N(10, 3)\\) and the movement \\(\\mathcal N(1, 4)\\). We’d expect a final position of 11 (10+1) with a variance of 7 (3+4).\n\nimport filterpy.kalman as kf\nkf.predict(x=10., P=3., u=1., Q=4.)\n\n(11.0, 7.0)\n\n\nupdate also takes several arguments, but for now you will be interested in these four:\nupdate(x, P, z, R)\nAs before, x and P are the state and variance of the system. z is the measurement, and R is the measurement variance. Let’s perform the last predict statement to get our prior, and then perform an update:\n\nx, P = kf.predict(x=10., P=3., u=1., Q=2.**2)\nprint(f'{x:.3f}')\n\nx, P = kf.update(x=x, P=P, z=12., R=3.5**2)\nprint(f'{x:.3f} {P:.3f}')\n\n11.000\n11.364 4.455\n\n\nI gave it a noisy measurement with a big variance, so the estimate remained close to the prior of 11.\nOne final point. I did not use the variable name prior for the output of the predict step. I will not use that variable name in the rest of the book. The Kalman filter equations just use \\(\\mathbf x\\). Both the prior and the posterior are the estimated state of the system, the former is the estimate before the measurement is incorporated, and the latter is after the measurement has been incorporated."
  },
  {
    "objectID": "labs/05Examples/One-Dimensional-Kalman-Filters.html#summary",
    "href": "labs/05Examples/One-Dimensional-Kalman-Filters.html#summary",
    "title": "One Dimensional Kalman Filters",
    "section": "Summary",
    "text": "Summary\nThe Kalman filter that we describe in this chapter is a special, restricted case of the more general filter we will learn next. Most texts do not discuss this one dimensional form. However, I think it is a vital stepping stone. We started the book with the g-h filter, then implemented the discrete Bayes filter, and now implemented the one dimensional Kalman filter. I have tried to show you that each of these filters use the same algorithm and reasoning. The mathematics of the Kalman filter that we will learn shortly is fairly sophisticated, and it can be difficult to understand the underlying simplicity of the filter. That sophistication comes with significant benefits: the generalized filter will markedly outperform the filters in this chapter.\nThis chapter takes time to assimilate. To truly understand it you will probably have to work through this chapter several times. I encourage you to change the various constants in the code and observe the results. Convince yourself that Gaussians are a good representation of a unimodal belief of the position of a dog in a hallway, the position of an aircraft in the sky, or the temperature of a chemical reaction chamber. Then convince yourself that multiplying Gaussians truly does compute a new belief from your prior belief and the new measurement. Finally, convince yourself that if you are measuring movement, that adding the Gaussians together updates your belief.\nMost of all, spend enough time with the Full Description of the Algorithm section to ensure you understand the algorithm and how it relates to the g-h filter and discrete Bayes filter. There is just one ‘trick’ here - selecting a value somewhere between a prediction and a measurement. Each algorithm performs that trick with different math, but all use the same logic."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html",
    "href": "labs/07Examples/Particle-Filters.html",
    "title": "Particle Filters",
    "section": "",
    "text": "Table of Contents"
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#motivation",
    "href": "labs/07Examples/Particle-Filters.html#motivation",
    "title": "Particle Filters",
    "section": "Motivation",
    "text": "Motivation\nHere is our problem. We have moving objects that we want to track. Maybe the objects are fighter jets and missiles, or maybe we are tracking people playing cricket in a field. It doesn’t really matter. Which of the filters that we have learned can handle this problem? Unfortunately, none of them are ideal. Let’s think about the characteristics of this problem.\n\nmultimodal: We want to track zero, one, or more than one object simultaneously.\nocclusions: One object can hide another, resulting in one measurement for multiple objects.\nnonlinear behavior: Aircraft are buffeted by winds, balls move in parabolas, and people collide into each other.\nnonlinear measurements: Radar gives us the distance to an object. Converting that to an (x,y,z) coordinate requires a square root, which is nonlinear.\nnon-Gaussian noise: as objects move across a background the computer vision can mistake part of the background for the object.\ncontinuous: the object’s position and velocity (i.e. the state space) can smoothly vary over time.\nmultivariate: we want to track several attributes, such as position, velocity, turn rates, etc.\nunknown process model: we may not know the process model of the system.\n\nNone of the filters we have learned work well with all of these constraints.\n\nDiscrete Bayes filter: This has most of the attributes. It is multimodal, can handle nonlinear measurements, and can be extended to work with nonlinear behavior. However, it is discrete and univariate.\nKalman filter: The Kalman filter produces optimal estimates for unimodal linear systems with Gaussian noise. None of these are true for our problem.\nUnscented Kalman filter: The UKF handles nonlinear, continuous, multivariate problems. However, it is not multimodal nor does it handle occlusions. It can handle noise that is modestly non-Gaussian, but does not do well with distributions that are very non-Gaussian or problems that are very nonlinear.\nExtended Kalman filter: The EKF has the same strengths and limitations as the UKF, except that is it even more sensitive to strong nonlinearities and non-Gaussian noise."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#monte-carlo-sampling",
    "href": "labs/07Examples/Particle-Filters.html#monte-carlo-sampling",
    "title": "Particle Filters",
    "section": "Monte Carlo Sampling",
    "text": "Monte Carlo Sampling\nIn the UKF chapter I generated a plot similar to this to illustrate the effects of nonlinear systems on Gaussians:\n\n#import kf_book.pf_internal as pf_internal #version difference caused error here, dont worry about running this\n#pf_internal.plot_monte_carlo_ukf() #version difference caused error here, dont worry about running this\n\n\n\n\n\n\n\n\nThe left plot shows 3,000 points normally distributed based on the Gaussian\n\\[\\mu = \\begin{bmatrix}0\\\\0\\end{bmatrix},\\, \\, \\, \\Sigma = \\begin{bmatrix}32&15\\\\15&40\\end{bmatrix}\\]\nThe right plots shows these points passed through this set of equations:\n\\[\\begin{aligned}x&=x+y\\\\\ny &= 0.1x^2 + y^2\\end{aligned}\\]\nUsing a finite number of randomly sampled points to compute a result is called a Monte Carlo (MC) method. The idea is simple. Generate enough points to get a representative sample of the problem, run the points through the system you are modeling, and then compute the results on the transformed points.\nIn a nutshell this is what particle filtering does. The Bayesian filter algorithm we have been using throughout the book is applied to thousands of particles, where each particle represents a possible state for the system. We extract the estimated state from the thousands of particles using weighted statistics of the particles."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#generic-particle-filter-algorithm",
    "href": "labs/07Examples/Particle-Filters.html#generic-particle-filter-algorithm",
    "title": "Particle Filters",
    "section": "Generic Particle Filter Algorithm",
    "text": "Generic Particle Filter Algorithm\n\nRandomly generate a bunch of particles\n\nParticles can have position, heading, and/or whatever other state variable you need to estimate. Each has a weight (probability) indicating how likely it matches the actual state of the system. Initialize each with the same weight.\n\nPredict next state of the particles\n\nMove the particles based on how you predict the real system is behaving.\n\nUpdate\n\nUpdate the weighting of the particles based on the measurement. Particles that closely match the measurements are weighted higher than particles which don’t match the measurements very well.\n\nResample\n\nDiscard highly improbable particle and replace them with copies of the more probable particles.\n\nCompute Estimate\n\nOptionally, compute weighted mean and covariance of the set of particles to get a state estimate.\nThis naive algorithm has practical difficulties which we will need to overcome, but this is the general idea. Let’s see an example. I wrote a particle filter for the robot localization problem from the UKF and EKF chapters. The robot has steering and velocity control inputs. It has sensors that measures distance to visible landmarks. Both the sensors and control mechanism have noise in them, and we need to track the robot’s position.\nHere I run a particle filter and plotted the positions of the particles. The plot on the left is after one iteration, and on the right is after 10. The red ‘X’ shows the actual position of the robot, and the large circle is the computed weighted mean position.\n\npf_internal.show_two_pf_plots()\n\nIf you are viewing this in a browser, this animation shows the entire sequence:\n\nAfter the first iteration the particles are still largely randomly scattered around the map, but you can see that some have already collected near the robot’s position. The computed mean is quite close to the robot’s position. This is because each particle is weighted based on how closely it matches the measurement. The robot is near (1,1), so particles that are near (1, 1) will have a high weight because they closely match the measurements. Particles that are far from the robot will not match the measurements, and thus have a very low weight. The estimated position is computed as the weighted mean of positions of the particles. Particles near the robot contribute more to the computation so the estimate is quite accurate.\nSeveral iterations later you can see that all the particles have clustered around the robot. This is due to the resampling step. Resampling discards particles that are very improbable (very low weight) and replaces them with particles with higher probability.\nI haven’t fully shown why this works nor fully explained the algorithms for particle weighting and resampling, but it should make intuitive sense. Make a bunch of random particles, move them so they ‘kind of’ follow the robot, weight them according to how well they match the measurements, only let the likely ones live. It seems like it should work, and it does."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#probability-distributions-via-monte-carlo",
    "href": "labs/07Examples/Particle-Filters.html#probability-distributions-via-monte-carlo",
    "title": "Particle Filters",
    "section": "Probability distributions via Monte Carlo",
    "text": "Probability distributions via Monte Carlo\nSuppose we want to know the area under the curve \\(y= \\mathrm{e}^{\\sin(x)}\\) in the interval [0, \\(\\pi\\)]. The area is computed with the definite integral \\(\\int_0^\\pi \\mathrm{e}^{\\sin(x)}\\, \\mathrm{d}x\\). As an exercise, go ahead and find the answer; I’ll wait.\nIf you are wise you did not take that challenge; \\(\\mathrm{e}^{\\sin(x)}\\) cannot be integrated analytically. The world is filled with equations which we cannot integrate. For example, consider calculating the luminosity of an object. An object reflects some of the light that strike it. Some of the reflected light bounces off of other objects and restrikes the original object, increasing the luminosity. This creates a recursive integral. Good luck with that one.\nHowever, integrals are trivial to compute using a Monte Carlo technique. To find the area under a curve create a bounding box that contains the curve in the desired interval. Generate randomly positioned point within the box, and compute the ratio of points that fall under the curve vs the total number of points. For example, if 40% of the points are under the curve and the area of the bounding box is 1, then the area under the curve is approximately 0.4. As you tend towards infinite points you can achieve any arbitrary precision. In practice, a few thousand points will give you a fairly accurate result.\nYou can use this technique to numerically integrate a function of any arbitrary difficulty. this includes non-integrable and noncontinuous functions. This technique was invented by Stanley Ulam at Los Alamos National Laboratory to allow him to perform computations for nuclear reactions which were unsolvable on paper.\nLet’s compute \\(\\pi\\) by finding the area of a circle. We will define a circle with a radius of 1, and bound it in a square. The side of the square has length 2, so the area is 4. We generate a set of uniformly distributed random points within the box, and count how many fall inside the circle. The area of the circle is computed as the area of the box times the ratio of points inside the circle vs. the total number of points. Finally, we know that \\(A = \\pi r^2\\), so we compute \\(\\pi = A / r^2\\).\nWe start by creating the points.\nN = 20000\npts = uniform(-1, 1, (N, 2))\nA point is inside a circle if its distance from the center of the circle is less than or equal to the radius. We compute the distance with numpy.linalg.norm, which computes the magnitude of a vector. Since vectors start at (0, 0) calling norm will compute the point’s distance from the origin.\ndist = np.linalg.norm(pts, axis=1)\nNext we compute which of this distances fit the criteria. This code returns a bool array that contains True if it meets the condition dist &lt;= 1:\nin_circle = dist &lt;= 1\nAll that is left is to count the points inside the circle, compute pi, and plot the results. I’ve put it all in one cell so you can experiment with alternative values for N, the number of points.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import uniform\n\nN = 20000  # number of points\nradius = 1.\narea = (2*radius)**2\n\npts = uniform(-1, 1, (N, 2))\n\n# distance from (0,0)\ndist = np.linalg.norm(pts, axis=1)\nin_circle = dist &lt;= 1\n\npts_in_circle = np.count_nonzero(in_circle)\npi = 4 * (pts_in_circle / N)\n\n# plot results\nplt.scatter(pts[in_circle,0], pts[in_circle,1],\n            marker=',', edgecolor='k', s=1)\nplt.scatter(pts[~in_circle,0], pts[~in_circle,1],\n            marker=',', edgecolor='r', s=1)\nplt.axis('equal')\n\nprint(f'mean pi(N={N})= {pi:.4f}')\nprint(f'err  pi(N={N})= {np.pi-pi:.4f}')\n\nThis insight leads us to the realization that we can use Monte Carlo to compute the probability density of any probability distribution. For example, suppose we have this Gaussian:\n\nfrom filterpy.stats import plot_gaussian_pdf\nplot_gaussian_pdf(mean=2, variance=3);\n\nThe probability density function (PDF) gives the probability that the random value falls between 2 values. For example, we may want to know the probability of x being between 0 and 2 in the graph above. This is a continuous function, so we need to take the integral to find the area under the curve, as the area is equal to the probability for that range of values to occur.\n\\[P[a \\le X \\le b] = \\int_a^b f_X(x) \\, dx\\]\nIt is easy to compute this integral for a Gaussian. But real life is not so easy. For example, the plot below shows a probability distribution. There is no way to analytically describe an arbitrary curve, let alone integrate it.\n\npf_internal.plot_random_pd()\n\nWe can use Monte Carlo methods to compute any integral. The PDF is computed with an integral, hence we can compute the PDF of this curve using Monte Carlo."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#the-particle-filter",
    "href": "labs/07Examples/Particle-Filters.html#the-particle-filter",
    "title": "Particle Filters",
    "section": "The Particle Filter",
    "text": "The Particle Filter\nAll of this brings us to the particle filter. Consider tracking a robot or a car in an urban environment. For consistency I will use the robot localization problem from the EKF and UKF chapters. In this problem we tracked a robot that has a sensor which measures the range and bearing to known landmarks.\nParticle filters are a family of algorithms. I’m presenting a specific form of a particle filter that is intuitive to grasp and relates to the problems we have studied in this book. This will leave a few of the steps seeming a bit ‘magical’ since I haven’t offered a full explanation. That will follow later in the chapter.\nTaking insight from the discussion in the previous section we start by creating several thousand particles. Each particle has a position that represents a possible belief of where the robot is in the scene, and perhaps a heading and velocity. Suppose that we have no knowledge of the location of the robot. We would want to scatter the particles uniformly over the entire scene. If you think of all of the particles representing a probability distribution, locations where there are more particles represent a higher belief, and locations with fewer particles represents a lower belief. If there was a large clump of particles near a specific location that would imply that we were more certain that the robot is there.\nEach particle needs a weight - ideally the probability that it represents the true position of the robot. This probability is rarely computable, so we only require it be proportional to that probability, which is computable. At initialization we have no reason to favor one particle over another, so we assign a weight of \\(1/N\\), for \\(N\\) particles. We use \\(1/N\\) so that the sum of all probabilities equals one.\nThe combination of particles and weights forms the probability distribution for our problem. Think back to the Discrete Bayes chapter. In that chapter we modeled positions in a hallway as discrete and uniformly spaced. This is very similar except the particles are randomly distributed in a continuous space rather than constrained to discrete locations. In this problem the robot can move on a plane of some arbitrary dimension, with the lower right corner at (0,0).\nTo track our robot we need to maintain states for x, y, and heading. We will store N particles in a (N, 3) shaped array. The three columns contain x, y, and heading, in that order.\nIf you are passively tracking something (no control input), then you would need to include velocity in the state and use that estimate to make the prediction. More dimensions requires exponentially more particles to form a good estimate, so we always try to minimize the number of random variables in the state.\nThis code creates a uniform and Gaussian distribution of particles over a region:\n\nfrom numpy.random import uniform\n\ndef create_uniform_particles(x_range, y_range, hdg_range, N):\n    particles = np.empty((N, 3))\n    particles[:, 0] = uniform(x_range[0], x_range[1], size=N)\n    particles[:, 1] = uniform(y_range[0], y_range[1], size=N)\n    particles[:, 2] = uniform(hdg_range[0], hdg_range[1], size=N)\n    particles[:, 2] %= 2 * np.pi\n    return particles\n\ndef create_gaussian_particles(mean, std, N):\n    particles = np.empty((N, 3))\n    particles[:, 0] = mean[0] + (randn(N) * std[0])\n    particles[:, 1] = mean[1] + (randn(N) * std[1])\n    particles[:, 2] = mean[2] + (randn(N) * std[2])\n    particles[:, 2] %= 2 * np.pi\n    return particles\n\nFor example:\n\ncreate_uniform_particles((0,1), (0,1), (0, np.pi*2), 4)\n\narray([[0.772, 0.336, 4.171],\n       [0.333, 0.34 , 4.319],\n       [0.6  , 0.274, 5.02 ],\n       [0.054, 0.022, 5.034]])\n\n\n\nPredict Step\nThe predict step in the Bayes algorithm uses the process model to update the belief in the system state. How would we do that with particles? Each particle represents a possible position for the robot. Suppose we send a command to the robot to move 0.1 meters while turning by 0.007 radians. We could move each particle by this amount. If we did that we would soon run into a problem. The robot’s controls are not perfect so it will not move exactly as commanded. Therefore we need to add noise to the particle’s movements to have a reasonable chance of capturing the actual movement of the robot. If you do not model the uncertainty in the system the particle filter will not correctly model the probability distribution of our belief in the robot’s position.\n\ndef predict(particles, u, std, dt=1.):\n    \"\"\" move according to control input u (heading change, velocity)\n    with noise Q (std heading change, std velocity)`\"\"\"\n\n    N = len(particles)\n    # update heading\n    particles[:, 2] += u[0] + (randn(N) * std[0])\n    particles[:, 2] %= 2 * np.pi\n\n    # move in the (noisy) commanded direction\n    dist = (u[1] * dt) + (randn(N) * std[1])\n    particles[:, 0] += np.cos(particles[:, 2]) * dist\n    particles[:, 1] += np.sin(particles[:, 2]) * dist\n\n\n\nUpdate Step\nNext we get a set of measurements - one for each landmark currently in view. How should these measurements be used to alter our probability distribution as modeled by the particles?\nThink back to the Discrete Bayes chapter. In that chapter we modeled positions in a hallway as discrete and uniformly spaced. We assigned a probability to each position which we called the prior. When a new measurement came in we multiplied the current probability of that position (the prior) by the likelihood that the measurement matched that location:\ndef update(likelihood, prior):\n    posterior = prior * likelihood\n    return normalize(posterior)\nwhich is an implementation of the equation\n\\[x = \\| \\mathcal L \\bar x \\|\\]\nwhich is a realization of Bayes theorem:\n\\[\\begin{aligned}P(x \\mid z) &= \\frac{P(z \\mid x)\\, P(x)}{P(z)} \\\\\n&= \\frac{\\mathtt{likelihood}\\times \\mathtt{prior}}{\\mathtt{normalization}}\\end{aligned}\\]\nWe do the same with our particles. Each particle has a position and a weight which estimates how well it matches the measurement. Normalizing the weights so they sum to one turns them into a probability distribution. The particles those that are closest to the robot will generally have a higher weight than ones far from the robot.\n\ndef update(particles, weights, z, R, landmarks):\n    for i, landmark in enumerate(landmarks):\n        distance = #FILL IN CODE! calculate norm between particles and landmark\n        weights *= scipy.stats.norm(distance, R).pdf(z[i])\n\n    weights += 1.e-300      # avoid round-off to zero\n    #FILL IN CODE! normalize so that they sum to one\n\nIn the literature this part of the algorithm is called Sequential Importance Sampling, or SIS. The equation for the weights is called the importance density. I will give these theoretical underpinnings in a following section. For now I hope that this makes intuitive sense. If we weight the particles according to how well they match the measurements they are probably a good sample for the probability distribution of the system after incorporating the measurements. Theory proves this is so. The weights are the likelihood in Bayes theorem. Different problems will need to tackle this step in slightly different ways but this is the general idea.\n\n\nComputing the State Estimate\nIn most applications you will want to know the estimated state after each update, but the filter consists of nothing but a collection of particles. Assuming that we are tracking one object (i.e. it is unimodal) we can compute the mean of the estimate as the sum of the weighted values of the particles.\n\\[\\displaystyle \\mu = \\frac{1}{N}\\sum_{i=1}^N w^ix^i\\]\nHere I adopt the notation \\(x^i\\) to indicate the \\(\\mathtt{i}^{th}\\) particle. A superscript is used because we often need to use subscripts to denote time steps, yielding the unwieldy \\(x^i_{k+1}\\) for the \\(\\mathtt{k+1}^{th}\\) time step for example.\nThis function computes both the mean and variance of the particles:\n\ndef estimate(particles, weights):\n    \"\"\"returns mean and variance of the weighted particles\"\"\"\n\n    pos = particles[:, 0:2]\n    mean = np.average(pos, weights=weights, axis=0)\n    var  = np.average((pos - mean)**2, weights=weights, axis=0)\n    return mean, var\n\nIf we create a uniform distribution of points in a 1x1 square with equal weights we get a mean position very near the center of the square at (0.5, 0.5) and a small variance.\n\nparticles = create_uniform_particles((0,1), (0,1), (0, 5), 1000)\nweights = np.array([.25]*1000)\nestimate(particles, weights)\n\n(array([0.494, 0.514]), array([0.083, 0.085]))\n\n\n\n\nParticle Resampling\nThe SIS algorithm suffers from the degeneracy problem. It starts with uniformly distributed particles with equal weights. There may only be a handful of particles near the robot. As the algorithm runs any particle that does not match the measurements will acquire an extremely low weight. Only the particles which are near the robot will have an appreciable weight. We could have 5,000 particles with only 3 contributing meaningfully to the state estimate! We say the filter has degenerated.This problem is usually solved by some form of resampling of the particles.\nParticles with very small weights do not meaningfully describe the probability distribution of the robot. The resampling algorithm discards particles with very low probability and replaces them with new particles with higher probability. It does that by duplicating particles with relatively high probability. The duplicates are slightly dispersed by the noise added in the predict step. This results in a set of points in which a large majority of the particles accurately represent the probability distribution.\nThere are many resampling algorithms. For now let’s look at one of the simplest, simple random resampling, also called multinomial resampling. It samples from the current particle set \\(N\\) times, making a new set of particles from the sample. The probability of selecting any given particle should be proportional to its weight.\nWe accomplish this with NumPy’s cumsum function. cumsum computes the cumulative sum of an array. That is, element one is the sum of elements zero and one, element two is the sum of elements zero, one and two, etc. Then we generate random numbers in the range of 0.0 to 1.0 and do a binary search to find the weight that most closely matches that number:\n\ndef simple_resample(particles, weights):\n    N = len(particles)\n    cumulative_sum = np.cumsum(weights)\n    cumulative_sum[-1] = 1. # avoid round-off error\n    indexes = np.searchsorted(cumulative_sum, random(N))\n\n    # resample according to indexes\n    particles[:] = particles[indexes]\n    weights.fill(1.0 / N)\n\nWe don’t resample at every epoch. For example, if you received no new measurements you have not received any information from which the resample can benefit. We can determine when to resample by using something called the effective N, which approximately measures the number of particles which meaningfully contribute to the probability distribution. The equation for this is\n\\[\\hat{N}_\\text{eff} = \\frac{1}{\\sum w^2}\\]\nand we can implement this in Python with\n\ndef neff(weights):\n    return 1. / np.sum(np.square(weights))\n\nIf \\(\\hat{N}_\\text{eff}\\) falls below some threshold it is time to resample. A useful starting point is \\(N/2\\), but this varies by problem. It is also possible for \\(\\hat{N}_\\text{eff} = N\\), which means the particle set has collapsed to one point (each has equal weight). It may not be theoretically pure, but if that happens I create a new distribution of particles in the hopes of generating particles with more diversity. If this happens to you often, you may need to increase the number of particles, or otherwise adjust your filter. We will talk more of this later."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#sir-filter---a-complete-example",
    "href": "labs/07Examples/Particle-Filters.html#sir-filter---a-complete-example",
    "title": "Particle Filters",
    "section": "SIR Filter - A Complete Example",
    "text": "SIR Filter - A Complete Example\nThere is more to learn, but we know enough to implement a full particle filter. We will implement the Sampling Importance Resampling filter, or SIR.\nI need to introduce a more sophisticated resampling method than I gave above. FilterPy provides several resampling methods. I will describe them later. They take an array of weights and returns indexes to the particles that have been chosen for the resampling. We just need to write a function that performs the resampling from these indexes:\n\ndef resample_from_index(particles, weights, indexes):\n    particles[:] = particles[indexes]\n    weights.resize(len(particles))\n    weights.fill (1.0 / len(weights))\n\nTo implement the filter we need to create the particles and the landmarks. We then execute a loop, successively calling predict, update, resampling, and then computing the new state estimate with estimate.\n\nfrom filterpy.monte_carlo import systematic_resample\nfrom numpy.linalg import norm\nfrom numpy.random import randn\nimport scipy.stats\n\ndef run_pf1(N, iters=18, sensor_std_err=.1,\n            do_plot=True, plot_particles=False,\n            xlim=(0, 20), ylim=(0, 20),\n            initial_x=None):\n    landmarks = np.array([[-1, 2], [5, 10], [12,14], [18,21]])\n    NL = len(landmarks)\n\n    plt.figure()\n\n    # create particles and weights\n    if initial_x is not None:\n        particles = create_gaussian_particles(\n            mean=initial_x, std=(5, 5, np.pi/4), N=N)\n    else:\n        particles = create_uniform_particles((0,20), (0,20), (0, 6.28), N)\n    weights = np.ones(N) / N\n\n    if plot_particles:\n        alpha = .20\n        if N &gt; 5000:\n            alpha *= np.sqrt(5000)/np.sqrt(N)\n        plt.scatter(particles[:, 0], particles[:, 1],\n                    alpha=alpha, color='g')\n\n    xs = []\n    robot_pos = np.array([0., 0.])\n    for x in range(iters):\n        robot_pos += (1, 1)\n\n        # distance from robot to each landmark\n        zs = (norm(landmarks - robot_pos, axis=1) +\n              (randn(NL) * sensor_std_err))\n\n        # move diagonally forward to (x+1, x+1)\n        predict(particles, u=(0.00, 1.414), std=(.2, .05))\n\n        # incorporate measurements\n        update(particles, weights, z=zs, R=sensor_std_err,\n               landmarks=landmarks)\n\n        # resample if too few effective particles\n        if neff(weights) &lt; N/2:\n            indexes = systematic_resample(weights)\n            resample_from_index(particles, weights, indexes)\n            assert np.allclose(weights, 1/N)\n        mu, var = estimate(particles, weights)\n        xs.append(mu)\n\n        if plot_particles:\n            plt.scatter(particles[:, 0], particles[:, 1],\n                        color='k', marker=',', s=1)\n        p1 = plt.scatter(robot_pos[0], robot_pos[1], marker='+',\n                         color='k', s=180, lw=3)\n        p2 = plt.scatter(mu[0], mu[1], marker='s', color='r')\n\n    xs = np.array(xs)\n    #plt.plot(xs[:, 0], xs[:, 1])\n    plt.legend([p1, p2], ['Actual', 'PF'], loc=4, numpoints=1)\n    plt.xlim(*xlim)\n    plt.ylim(*ylim)\n    print('final position error, variance:\\n\\t', mu - np.array([iters, iters]), var)\n    plt.show()\n\nfrom numpy.random import seed\nseed(2)\nrun_pf1(N=5000, plot_particles=False)\n\nMost of this code is devoted to initialization and plotting. The entirety of the particle filter processing consists of these lines:\n# move diagonally forward to (x+1, x+1)\npredict(particles, u=(0.00, 1.414), std=(.2, .05))\n\n # incorporate measurements\nupdate(particles, weights, z=zs, R=sensor_std_err,\n       landmarks=landmarks)\n       \n# resample if too few effective particles\nif neff(weights) &lt; N/2:\n    indexes = systematic_resample(weights)\n    resample_from_index(particles, weights, indexes)\n\nmu, var = estimate(particles, weights)\nThe first line predicts the position of the particles with the assumption that the robot is moving in a straight line (u[0] == 0) and moving 1 unit in both the x and y axis (u[1]==1.414). The standard deviation for the error in the turn is 0.2, and the standard deviation for the distance is 0.05. When this call returns the particles will all have been moved forward, but the weights are no longer correct as they have not been updated.\nThe next line incorporates the measurement into the filter. This does not alter the particle positions, it only alters the weights. If you recall the weight of the particle is computed as the probability that it matches the Gaussian of the sensor error model. The further the particle from the measured distance the less likely it is to be a good representation.\nThe final two lines example the effective particle count (\\(\\hat{N}_\\text{eff})\\). If it falls below \\(N/2\\) we perform resampling to try to ensure our particles form a good representation of the actual probability distribution.\nNow let’s look at this with all the particles plotted. Seeing this happen interactively is much more instructive, but this format still gives us useful information. I plotted the original random distribution of points in a very pale green and large circles to help distinguish them from the subsequent iterations where the particles are plotted with black pixels. The number of particles makes it hard to see the details, so I limited the number of iterations to 8 so we can zoom in and look more closely.\n\nseed(2)\nrun_pf1(N=5000, iters=8, plot_particles=True,\n        xlim=(0,8), ylim=(0,8))\n\nFrom the plot it looks like there are only a few particles at the first two robot positions. This is not true; there are 5,000 particles, but due to resampling most are duplicates of each other. The reason for this is the Gaussian for the sensor is very narrow. This is called sample impoverishment and can lead to filter divergence. I’ll address this in detail below. For now, looking at the second step at x=2 we can see that the particles have dispersed a bit. This dispersion is due to the motion model noise. All particles are projected forward according to the control input u, but noise is added to each particle proportional to the error in the control mechanism in the robot. By the third step the particles have dispersed enough to make a convincing cloud of particles around the robot.\nThe shape of the particle cloud is an ellipse. This is not a coincidence. The sensors and robot control are both modeled as Gaussian, so the probability distribution of the system is also a Gaussian. The particle filter is a sampling of the probability distribution, so the cloud should be an ellipse.\nIt is important to recognize that the particle filter algorithm does not require the sensors or system to be Gaussian or linear. Because we represent the probability distribution with a cloud of particles we can handle any probability distribution and strongly nonlinear problems. There can be discontinuities and hard limits in the probability model.\n\nEffect of Sensor Errors on the Filter\nThe first few iterations of the filter resulted in many duplicate particles. This happens because the model for the sensors is Gaussian, and we gave it a small standard deviation of \\(\\sigma=0.1\\). This is counterintuitive at first. The Kalman filter performs better when the noise is smaller, yet the particle filter can perform worse.\nWe can reason about why this is true. If \\(\\sigma=0.1\\), the robot is at (1, 1) and a particle is at (2, 2) the particle is 14 standard deviations away from the robot. This gives it a near zero probability. It contributes nothing to the estimate of the mean, and it is extremely unlikely to survive after the resampling. If \\(\\sigma=1.4\\) then the particle is only \\(1\\sigma\\) away and thus it will contribute to the estimate of the mean. During resampling it is likely to be copied one or more times.\nThis is very important to understand - a very accurate sensor can lead to poor performance of the filter because few of the particles will be a good sample of the probability distribution. There are a few fixes available to us. First, we can artificially increase the sensor noise standard deviation so the particle filter will accept more points as matching the robots probability distribution. This is non-optimal because some of those points will be a poor match. The real problem is that there aren’t enough points being generated such that enough are near the robot. Increasing N usually fixes this problem. This decision is not cost free as increasing the number of particles significantly increase the computation time. Still, let’s look at the result of using 100,000 particles.\n\nseed(2)\nrun_pf1(N=100000, iters=8, plot_particles=True,\n        xlim=(0,8), ylim=(0,8))"
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#fill-in-observe-results-and-write-conclusions",
    "href": "labs/07Examples/Particle-Filters.html#fill-in-observe-results-and-write-conclusions",
    "title": "Particle Filters",
    "section": "FILL IN: Observe results and write conclusions",
    "text": "FILL IN: Observe results and write conclusions\nAnother approach is to be smarter about generating the initial particle cloud. Suppose we guess that the robot is near (0, 0). This is not exact, as the simulation actually places the robot at (1, 1), but it is close. If we create a normally distributed cloud near (0, 0) there is a much greater chance of the particles matching the robot’s position.\nrun_pf1() has an optional parameter initial_x. Use this to specify the initial position guess for the robot. The code then uses create_gaussian_particles(mean, std, N) to create particles distributed normally around the initial guess. We will use this in the next section.\n\nFilter Degeneracy From Inadequate Samples\nPlease rerun the filter but with 5000 samples instead of 100000.\n\n# FILL IN CODE: RERUN WITH 5000 samples."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#fill-in-observe-results-and-write-conclusions-1",
    "href": "labs/07Examples/Particle-Filters.html#fill-in-observe-results-and-write-conclusions-1",
    "title": "Particle Filters",
    "section": "FILL IN: Observe results and write conclusions",
    "text": "FILL IN: Observe results and write conclusions\nLet’s make use of the create_gaussian_particles() method to try to generate more points near the robot. We can do this by using the initial_x parameter to specify a location to create the particles.\n\nseed(6)\nrun_pf1(N=5000, plot_particles=True, initial_x=(1,1, np.pi/4))"
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#fill-in-observe-results-and-write-conclusions-2",
    "href": "labs/07Examples/Particle-Filters.html#fill-in-observe-results-and-write-conclusions-2",
    "title": "Particle Filters",
    "section": "FILL IN: Observe results and write conclusions",
    "text": "FILL IN: Observe results and write conclusions"
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#summary",
    "href": "labs/07Examples/Particle-Filters.html#summary",
    "title": "Particle Filters",
    "section": "Summary",
    "text": "Summary\nThis chapter only touches the surface of what is a vast topic. My goal was not to teach you the field, but to expose you to practical Bayesian Monte Carlo techniques for filtering.\nParticle filters are a type of ensemble filtering. Kalman filters represents state with a Gaussian. Measurements are applied to the Gaussian using Bayes Theorem, and the prediction is done using state-space methods. These techniques are applied to the Gaussian - the probability distribution.\nIn contrast, ensemble techniques represent a probability distribution using a discrete collection of points and associated probabilities. Measurements are applied to these points, not the Gaussian distribution. Likewise, the system model is applied to the points, not a Gaussian. We then compute the statistical properties of the resulting ensemble of points.\nThese choices have many trade-offs. The Kalman filter is very efficient, and is an optimal estimator if the assumptions of linearity and Gaussian noise are true. If the problem is nonlinear than we must linearize the problem. If the problem is multimodal (more than one object being tracked) then the Kalman filter cannot represent it. The Kalman filter requires that you know the state model. If you do not know how your system behaves the performance is poor.\nIn contrast, particle filters work with any arbitrary, non-analytic probability distribution. The ensemble of particles, if large enough, form an accurate approximation of the distribution. It performs wonderfully even in the presence of severe nonlinearities. Importance sampling allows us to compute probabilities even if we do not know the underlying probability distribution. Monte Carlo techniques replace the analytic integrals required by the other filters.\nThis power comes with a cost. The most obvious costs are the high computational and memory burdens the filter places on the computer. Less obvious is the fact that they are fickle. You have to be careful to avoid particle degeneracy and divergence. It can be very difficult to prove the correctness of your filter. If you are working with multimodal distributions you have further work to cluster the particles to determine the paths of the multiple objects. This can be very difficult when the objects are close to each other.\nThere are many different classes of particle filter; I only described the naive SIS algorithm, and followed that with a SIR algorithm that performs well. There are many classes of filters, and many examples of filters in each class. It would take a small book to describe them all.\nWhen you read the literature on particle filters you will find that it is strewn with integrals. We perform computations on probability distributions using integrals, so using integrals gives the authors a powerful and compact notation. You must recognize that when you reduce these equations to code you will be representing the distributions with particles, and integrations are replaced with sums over the particles. If you keep in mind the core ideas in this chapter the material shouldn’t be daunting."
  },
  {
    "objectID": "labs/07Examples/Particle-Filters.html#references",
    "href": "labs/07Examples/Particle-Filters.html#references",
    "title": "Particle Filters",
    "section": "References",
    "text": "References\n[1] Importance Sampling, Wikipedia. https://en.wikipedia.org/wiki/Importance_sampling"
  },
  {
    "objectID": "labs/w8-lab-normalizing-flow.html",
    "href": "labs/w8-lab-normalizing-flow.html",
    "title": "Normalizing flow",
    "section": "",
    "text": "Please follow instructions in the notebook and send results to TA in .pdf format.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/w1-lab01-intro.html",
    "href": "labs/w1-lab01-intro.html",
    "title": "First Lab",
    "section": "",
    "text": "Rerun the examples of this Notebook and redo the experiment for \\(y=-2\\tan(\\pi x)\\). underfitting_overfitting.ipynb\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html",
    "href": "labs/02Examples/pytorch/pytorch_102.html",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Based on Bourkes’s https://www.learnpytorch.io/\nHere’s a standard PyTorch workflow.\n\n\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'\n\n\n\n\n\nHere we will generate our own data, a straight line, then use PyTorch to find the slope (weight) and intercept (bias).\n\n# Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend   = 1\nstep  = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\n\n\n\ntraining set -&gt; model learns from this data\nvalidation ste -&gt; model is tuned on this data\ntest set -&gt; model is evluated on this data\n\n\n# Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test,  y_test  = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\nCreate a function to visualize the data.\n\ndef plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n      \"\"\"\n      Plots training data, test data and compares predictions.\n      \"\"\"\n      plt.figure(figsize=(10, 7))\n\n      # Plot training data in blue\n      plt.scatter(train_data, train_labels, c=\"b\", s=20, label=\"Training data\")\n\n      # Plot test data in green\n      plt.scatter(test_data, test_labels, c=\"orange\", s=20, label=\"Testing data\")\n\n      if predictions is not None:\n        # Plot the predictions in red (predictions were made on the test data)\n        plt.scatter(test_data, predictions, c=\"r\", s=20, label=\"Predictions\")\n\n      # Show the legend\n      plt.legend(prop={\"size\": 14});\n\n\nplot_predictions()\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 4 essential modules for creating any NN\n\ntorch.nn contains all the building blocks of the computational graph\ntorch.optim contains the different optimization algorithms\ntorch.utils.data.Dataset selects data\ntorch.utils.data.DataLoader loads the data\n\nThe NN itself, defined by torch.nn, contains the following sub-modules\n\nnn.Module has the layers\nnn.Parameter has the weights and biases\n\nFinally, all the nn.Module subclasses require a forward() method that defines the flow of the computation, or structure of the NN.\nWe create a standard linear regression class.\n\n# Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                    dtype=torch.float), # &lt;- PyTorch uses float32 by default\n                                    requires_grad=True) # &lt;- update this value with gradient descent\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                dtype=torch.float), # &lt;- PyTorch uses float32 by default\n                                requires_grad=True) # &lt;- update this value with gradient descent\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n\nNow, let’s create an instance of the model and check its prameters.\n\n# Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model \n# (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n\n[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]\n\n\nWe can retrieve the state of the model, with .stat_dict()\n\n# List named parameters \nmodel_0.state_dict()\n\nOrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n\n\n\n\n\nBefore optimizing, we can already make (bad) predictions with this randomly initialized model. We will use the torch.inference_mode() function that streamlines the inference process.\n\n# Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n\n# Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n\nplot_predictions(predictions=y_preds)\n\nNumber of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n\n\n\n\n\n\n\n\n\nThese predicitions are way off, as expected, since they are based on a random initialization. Let us compute the errors.\n\n# errors\ny_test - y_preds\n\ntensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])\n\n\n\n\n\nWe need to define\n\na loss function\nan optimizer\n\nHere we will use\n\nMAE torch.nn.L1Loss()\nSGD torch.optim.SGD(params, lr)\n\nparams are the model parameters that we want to adjust optimally\nlr is the learning rate (step-size) of the gradient descent, a hyperparameter\n\n\n.\n\n# Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate \n                                     # (how much the optimizer should change parameters at each \n                                     # step, higher=more (less stable), lower=less (might take a long time))\n\nFinally, we need the training and testing loops.\n\n\n\nHere are the 5 basic steps:\n\nforward pass through the training data -&gt; model(x_train)\ncompute the loss -&gt; loss=loss_fn(y_pred,y_train)\nset gradients to zero -&gt; optimizer.zero_grad()\ndo backprop on the loss to compute gradient -&gt; loss.backward()\nupdate the parametes with the gradient -&gt; optimizer.step()\n\n\n\n\n\nIn 3 steps:\n\nforward pass -&gt; model(x_test)\ncompute the loss -&gt; loss=loss_fn(y_pred,y_test)\ncompute evaluation metrics/scores\n\nWe put it all together, and train for 1000 epochs of the SGD.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Advance the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening every 10 steps\n      if epoch % 20 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n\nEpoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \n\n\nFinally, plot the loss curves.\n\n# Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\nNow, inspect the model’s state_dict() to see how close we got.\n\n# Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nThe original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n\n\n\n\n\n\nTo do inference with a PyTorch model:\n\nSet model in evaluation mode -&gt; model.eval()\nMake predictions using the inference mode context manager -&gt; with torch.inference_mode()\nAll predictions should be on objects on the same device - GPU/CPU\n\n\n# 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n\n# plot the result\nplot_predictions(predictions=y_preds)\n\n\n\n\n\n\n\n\nFinal error plot, showing noise “ball” limit.\n\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.yscale(\"log\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n\nThree main methods:\n\ntorch.save uses pickle to save anything\ntorch.load unpickles\ntorch.nn.Module.load_state_dict loads a model’s parameter dictionary (model_save_dict())\n\n\nfrom pathlib import Path\n\n# 1. Create 'models' directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_0.pth\n\n\n\n# Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n\n-rw-r--r--@ 1 markasch  staff  1207 Feb 14 14:15 models/01_pytorch_workflow_model_0.pth\n\n\n\n\nWe have saved the model’s state dictionary at a given path. We can now load it using\n\ntorch.nn.Module.load_state_dict(torch.load(f=))\n\nTo test this, we create anew instance of the LinearRegressionModel(), which being a subclass of torch.nn.Module has all its built-in methods, and in particular load_state_dict().\n\n# Instantiate a new instance of our model (this will be instantiated \n# with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new \n# instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n&lt;All keys matched successfully&gt;\n\n\nNow, we are ready to perform inference.\n\n# 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n    \n# Compare previous model predictions with loaded model predictions \n# (these should be the same)\ny_preds == loaded_model_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])\n\n\n\n\n\n\nUsing all the above snippets, we can now write a complete, device agnostic workflow.\n\n# Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'\n\n\n\n# Setup device agnostic code\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nUsing device: mps\n\n\n\n# Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:5], y[:5]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560]]))\n\n\n\n# Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\n\nplot_predictions(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\nInstead of manually defining weight and bias parmeters by “hand”, using nn.Parameter(), we will use e pre-built torch.nn module,\n\nnn.Linear(dim_in_features, dim_out_features)\n\n\n\n# Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need \n# but is used for demonstrative purposes, try commenting it out and \n# seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n\n(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))\n\n\nNow, we can place the model onto the gpu device (after checking)\n\n# Check model device\nnext(model_1.parameters()).device\n\ndevice(type='cpu')\n\n\n\n# Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\"/\"mps\" \n                   # if available or \"cpu\" if not\nnext(model_1.parameters()).device\n\ndevice(type='mps', index=0)\n\n\n\n\n\nWe use the same functions and hyperparameters as before\n\nnn.L1Loss()\ntorch.optim.SGD()\n\n\n# Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n\nBefore training on the gpu, we must place the data there too.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n\n/Users/markasch/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nEpoch: 0 | Train loss: 0.5551779270172119 | Test loss: 0.5739762783050537\nEpoch: 100 | Train loss: 0.0062156799249351025 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 300 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 400 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 500 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 600 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 700 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 800 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 900 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\n\n\n\n# Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='mps:0')),\n             ('linear_layer.bias', tensor([0.3025], device='mps:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n\n\n\n\n\nUse inference mode.\n\n# Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n\ntensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='mps:0')\n\n\n\n# plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, save, load and perform inference.\n\nfrom pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_1.pth\n\n\n\n# Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n\nLoaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\nmps:0\n\n\n\n# Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='mps:0')\n\n\n\n\n\nA more complete tutorial on torch.nn is available on the official pytorch website\n\nhttps://pytorch.org/tutorials/beginner/nn_tutorial.html"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#pytorch-workflows",
    "href": "labs/02Examples/pytorch/pytorch_102.html#pytorch-workflows",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Based on Bourkes’s https://www.learnpytorch.io/\nHere’s a standard PyTorch workflow.\n\n\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#prepare-the-data",
    "href": "labs/02Examples/pytorch/pytorch_102.html#prepare-the-data",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Here we will generate our own data, a straight line, then use PyTorch to find the slope (weight) and intercept (bias).\n\n# Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend   = 1\nstep  = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\n\n\n\ntraining set -&gt; model learns from this data\nvalidation ste -&gt; model is tuned on this data\ntest set -&gt; model is evluated on this data\n\n\n# Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test,  y_test  = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\nCreate a function to visualize the data.\n\ndef plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n      \"\"\"\n      Plots training data, test data and compares predictions.\n      \"\"\"\n      plt.figure(figsize=(10, 7))\n\n      # Plot training data in blue\n      plt.scatter(train_data, train_labels, c=\"b\", s=20, label=\"Training data\")\n\n      # Plot test data in green\n      plt.scatter(test_data, test_labels, c=\"orange\", s=20, label=\"Testing data\")\n\n      if predictions is not None:\n        # Plot the predictions in red (predictions were made on the test data)\n        plt.scatter(test_data, predictions, c=\"r\", s=20, label=\"Predictions\")\n\n      # Show the legend\n      plt.legend(prop={\"size\": 14});\n\n\nplot_predictions()"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#build-the-model",
    "href": "labs/02Examples/pytorch/pytorch_102.html#build-the-model",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "There are 4 essential modules for creating any NN\n\ntorch.nn contains all the building blocks of the computational graph\ntorch.optim contains the different optimization algorithms\ntorch.utils.data.Dataset selects data\ntorch.utils.data.DataLoader loads the data\n\nThe NN itself, defined by torch.nn, contains the following sub-modules\n\nnn.Module has the layers\nnn.Parameter has the weights and biases\n\nFinally, all the nn.Module subclasses require a forward() method that defines the flow of the computation, or structure of the NN.\nWe create a standard linear regression class.\n\n# Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                    dtype=torch.float), # &lt;- PyTorch uses float32 by default\n                                    requires_grad=True) # &lt;- update this value with gradient descent\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                dtype=torch.float), # &lt;- PyTorch uses float32 by default\n                                requires_grad=True) # &lt;- update this value with gradient descent\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n\nNow, let’s create an instance of the model and check its prameters.\n\n# Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model \n# (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n\n[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]\n\n\nWe can retrieve the state of the model, with .stat_dict()\n\n# List named parameters \nmodel_0.state_dict()\n\nOrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#make-predictions-with-torch.inference_mode",
    "href": "labs/02Examples/pytorch/pytorch_102.html#make-predictions-with-torch.inference_mode",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Before optimizing, we can already make (bad) predictions with this randomly initialized model. We will use the torch.inference_mode() function that streamlines the inference process.\n\n# Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n\n# Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n\nplot_predictions(predictions=y_preds)\n\nNumber of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n\n\n\n\n\n\n\n\n\nThese predicitions are way off, as expected, since they are based on a random initialization. Let us compute the errors.\n\n# errors\ny_test - y_preds\n\ntensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#train-the-model",
    "href": "labs/02Examples/pytorch/pytorch_102.html#train-the-model",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "We need to define\n\na loss function\nan optimizer\n\nHere we will use\n\nMAE torch.nn.L1Loss()\nSGD torch.optim.SGD(params, lr)\n\nparams are the model parameters that we want to adjust optimally\nlr is the learning rate (step-size) of the gradient descent, a hyperparameter\n\n\n.\n\n# Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate \n                                     # (how much the optimizer should change parameters at each \n                                     # step, higher=more (less stable), lower=less (might take a long time))\n\nFinally, we need the training and testing loops.\n\n\n\nHere are the 5 basic steps:\n\nforward pass through the training data -&gt; model(x_train)\ncompute the loss -&gt; loss=loss_fn(y_pred,y_train)\nset gradients to zero -&gt; optimizer.zero_grad()\ndo backprop on the loss to compute gradient -&gt; loss.backward()\nupdate the parametes with the gradient -&gt; optimizer.step()\n\n\n\n\n\nIn 3 steps:\n\nforward pass -&gt; model(x_test)\ncompute the loss -&gt; loss=loss_fn(y_pred,y_test)\ncompute evaluation metrics/scores\n\nWe put it all together, and train for 1000 epochs of the SGD.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Advance the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening every 10 steps\n      if epoch % 20 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n\nEpoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \n\n\nFinally, plot the loss curves.\n\n# Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\nNow, inspect the model’s state_dict() to see how close we got.\n\n# Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nThe original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#use-trained-model-for-predictions",
    "href": "labs/02Examples/pytorch/pytorch_102.html#use-trained-model-for-predictions",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "To do inference with a PyTorch model:\n\nSet model in evaluation mode -&gt; model.eval()\nMake predictions using the inference mode context manager -&gt; with torch.inference_mode()\nAll predictions should be on objects on the same device - GPU/CPU\n\n\n# 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n\n# plot the result\nplot_predictions(predictions=y_preds)\n\n\n\n\n\n\n\n\nFinal error plot, showing noise “ball” limit.\n\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.yscale(\"log\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#saving-and-loading-trained-models",
    "href": "labs/02Examples/pytorch/pytorch_102.html#saving-and-loading-trained-models",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Three main methods:\n\ntorch.save uses pickle to save anything\ntorch.load unpickles\ntorch.nn.Module.load_state_dict loads a model’s parameter dictionary (model_save_dict())\n\n\nfrom pathlib import Path\n\n# 1. Create 'models' directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_0.pth\n\n\n\n# Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n\n-rw-r--r--@ 1 markasch  staff  1207 Feb 14 14:15 models/01_pytorch_workflow_model_0.pth\n\n\n\n\nWe have saved the model’s state dictionary at a given path. We can now load it using\n\ntorch.nn.Module.load_state_dict(torch.load(f=))\n\nTo test this, we create anew instance of the LinearRegressionModel(), which being a subclass of torch.nn.Module has all its built-in methods, and in particular load_state_dict().\n\n# Instantiate a new instance of our model (this will be instantiated \n# with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new \n# instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n&lt;All keys matched successfully&gt;\n\n\nNow, we are ready to perform inference.\n\n# 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n    \n# Compare previous model predictions with loaded model predictions \n# (these should be the same)\ny_preds == loaded_model_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#device-agnostic-version-of-pytorch-ml-workflow",
    "href": "labs/02Examples/pytorch/pytorch_102.html#device-agnostic-version-of-pytorch-ml-workflow",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Using all the above snippets, we can now write a complete, device agnostic workflow.\n\n# Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.13.1'\n\n\n\n# Setup device agnostic code\n#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nUsing device: mps\n\n\n\n# Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:5], y[:5]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560]]))\n\n\n\n# Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\n\nplot_predictions(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\nInstead of manually defining weight and bias parmeters by “hand”, using nn.Parameter(), we will use e pre-built torch.nn module,\n\nnn.Linear(dim_in_features, dim_out_features)\n\n\n\n# Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need \n# but is used for demonstrative purposes, try commenting it out and \n# seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n\n(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))\n\n\nNow, we can place the model onto the gpu device (after checking)\n\n# Check model device\nnext(model_1.parameters()).device\n\ndevice(type='cpu')\n\n\n\n# Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\"/\"mps\" \n                   # if available or \"cpu\" if not\nnext(model_1.parameters()).device\n\ndevice(type='mps', index=0)\n\n\n\n\n\nWe use the same functions and hyperparameters as before\n\nnn.L1Loss()\ntorch.optim.SGD()\n\n\n# Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n\nBefore training on the gpu, we must place the data there too.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n\n/Users/markasch/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nEpoch: 0 | Train loss: 0.5551779270172119 | Test loss: 0.5739762783050537\nEpoch: 100 | Train loss: 0.0062156799249351025 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 300 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 400 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 500 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 600 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 700 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 800 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\nEpoch: 900 | Train loss: 0.0012645028764382005 | Test loss: 0.013801807537674904\n\n\n\n# Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='mps:0')),\n             ('linear_layer.bias', tensor([0.3025], device='mps:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n\n\n\n\n\nUse inference mode.\n\n# Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n\ntensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='mps:0')\n\n\n\n# plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#save-and-load",
    "href": "labs/02Examples/pytorch/pytorch_102.html#save-and-load",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "Finally, save, load and perform inference.\n\nfrom pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_1.pth\n\n\n\n# Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n\nLoaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\nmps:0\n\n\n\n# Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='mps:0')"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_102.html#to-go-further",
    "href": "labs/02Examples/pytorch/pytorch_102.html#to-go-further",
    "title": "Intro to PyTorch",
    "section": "",
    "text": "A more complete tutorial on torch.nn is available on the official pytorch website\n\nhttps://pytorch.org/tutorials/beginner/nn_tutorial.html"
  },
  {
    "objectID": "labs/02Examples/pytorch/Torch_test_GPU_CPU.html",
    "href": "labs/02Examples/pytorch/Torch_test_GPU_CPU.html",
    "title": "Digital Twins for Physical Systems",
    "section": "",
    "text": "# check availability of GPU\nimport torch\nif torch.backends.mps.is_available():\n    mps_device = torch.device(\"mps\")\n    x = torch.ones(1, device=mps_device)\n    print (x)\nelse:\n    print (\"MPS device not found.\")\n\ntensor([1.], device='mps:0')\n# toy example on GPU\nimport timeit\nimport torch\nimport random\n\nx = torch.ones(5000, device=\"mps\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=100000)\n#Out[17]: 4.568202124999971\n\n# toy example cpu\n#x = torch.ones(5000, device=\"cpu\")\n# timeit.timeit(lambda: x * random.randint(0,100), number=100000)\n#Out[18]: 0.30446054200001527\n\n2.0122362919998977\n# toy example on cpu\nx = torch.ones(5000, device=\"cpu\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=100000)\n\n0.24692429099991386\nThe CPU is approximately 10 times faster than the GPU…\nHere is a slightly more complex examples, with a matrix-vector tensor multiplication.\na_cpu = torch.rand(250, device='cpu')\nb_cpu = torch.rand((250, 250), device='cpu')\na_mps = torch.rand(250, device='mps')\nb_mps = torch.rand((250, 250), device='mps')\n\nprint('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000))\nprint('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000))\n\ncpu 0.8405147910000323\nmps 2.3573820419999265\nNow, we drastically increase the problem size, using the tensor dimension.\nx = torch.ones(50000000, device=\"mps\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=1)\n\n0.00048149999997804116\nx = torch.ones(50000000, device=\"cpu\")\ntimeit.timeit(lambda: x * random.randint(0,100), number=1)\n\n0.03234533299996656\n.0323/.00048\n\n67.29166666666667"
  },
  {
    "objectID": "labs/02Examples/pytorch/Torch_test_GPU_CPU.html#conclusion",
    "href": "labs/02Examples/pytorch/Torch_test_GPU_CPU.html#conclusion",
    "title": "Digital Twins for Physical Systems",
    "section": "Conclusion",
    "text": "Conclusion\nGPU works well, but only for LARGE memory problems. This is because loading small data to memory and using GPU for calculation is overkill, so the CPU has an advantage in this case. But if you have large data dimensions, the GPU can compute efficiently and surpass the CPU.\nThis is well known with GPUs: they are only faster if you put a large computational load. It is not specific to pytorch or to MPS…"
  },
  {
    "objectID": "labs/02Examples/ad/diff_prog.html",
    "href": "labs/02Examples/ad/diff_prog.html",
    "title": "Differentiable Programming 101",
    "section": "",
    "text": "Differentiable Programming 101\nWe study some initial examples of\n\nnumerical differentiation\nsymbolic differentiation\nautomatic differentiation\n\n\n\nNumerical Differentiation\nConsider the sine function and its derivative,\n\\[ f(x) = \\sin(x), \\quad f'(x)=\\cos (x) \\]\nevaluated at the point \\(x = 0.1.\\)\n\nimport numpy as np\nf = lambda x: np.sin(x)\nx0 = 0.1\nexact = np.cos(x0)\nprint(\"True derivative:\", exact)\nprint(\"Forward Difference\\tError\\t\\t\\tCentral Difference\\tError\\n\")\nfor i in range(10):\n    h = 1/(10**i)\n    f1 = (f(x0+h)-f(x0))/h\n    f2 = (f(x0+h)-f(x0-h))/(2*h)\n    e1 = np.abs(f1 - exact)\n    e2 = np.abs(f2 - exact)\n    print('%.5e\\t\\t%.5e\\t\\t%.5e\\t\\t%.5e'%(f1,e1,f2,e2))\n\nTrue derivative: 0.9950041652780258\nForward Difference  Error           Central Difference  Error\n\n7.91374e-01     2.03630e-01     8.37267e-01     1.57737e-01\n9.88359e-01     6.64502e-03     9.93347e-01     1.65751e-03\n9.94488e-01     5.15746e-04     9.94988e-01     1.65833e-05\n9.94954e-01     5.00825e-05     9.95004e-01     1.65834e-07\n9.94999e-01     4.99333e-06     9.95004e-01     1.65828e-09\n9.95004e-01     4.99183e-07     9.95004e-01     1.66720e-11\n9.95004e-01     4.99136e-08     9.95004e-01     2.10021e-12\n9.95004e-01     4.96341e-09     9.95004e-01     3.25943e-11\n9.95004e-01     1.06184e-10     9.95004e-01     1.06184e-10\n9.95004e-01     2.88174e-09     9.95004e-01     2.88174e-09\n\n\n\n\nSymbolic Differentiation\nThough very useful in simple cases, symbolic differentiation often leads to complex and redundant expressions. In addition, balckbox routines cannot be differentiated.\n\nfrom sympy import *\nx = symbols('x')\n#\ndiff(cos(x), x)\n\n\\(\\displaystyle - \\sin{\\left(x \\right)}\\)\n\n\n\n# a more complicated esxpression\ndef sigmoid(x):\n  return 1 / (1 + exp(-x))\n\ndiff(sigmoid(x),x)\n\n\\(\\displaystyle \\frac{e^{- x}}{\\left(1 + e^{- x}\\right)^{2}}\\)\n\n\nNote that the derivative of \\[ \\sigma(x) = \\frac{1}{1+e^{-x}}\\] can be simply written as \\[ \\frac{d\\sigma }{dx}= (1-\\sigma(x)) \\sigma(x)\\]\n\n# much more complicated\nx,w1,w2,w3,b1,b2,b3 = symbols('x w1 w2 w3 b1 b2 b3')\ny = w3*sigmoid(w2*sigmoid(w1*x + b1) + b2) + b3\ndiff(y, w1)\n\n\\(\\displaystyle \\frac{w_{2} w_{3} x e^{- b_{1} - w_{1} x} e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}}}{\\left(e^{- b_{1} - w_{1} x} + 1\\right)^{2} \\left(e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}} + 1\\right)^{2}}\\)\n\n\n\ndydw1 = diff(y, w1)\nprint(dydw1)\n\nw2*w3*x*exp(-b1 - w1*x)*exp(-b2 - w2/(exp(-b1 - w1*x) + 1))/((exp(-b1 - w1*x) + 1)**2*(exp(-b2 - w2/(exp(-b1 - w1*x) + 1)) + 1)**2)\n\n\n\n\nAutomatic Differentiation\nHere we show the simplicity and efficiency of autograd from numpy.\n\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad as egrad  # for functions that vectorize over inputs\n\n# We could use np.tanh, but let's write our own as an example.\ndef tanh(x):\n    return (1.0 - np.exp(-x))  / (1.0 + np.exp(-x))\n\nx = np.linspace(-7, 7, 200)\nplt.plot(x, tanh(x),\n         x, egrad(tanh)(x),                                # first  derivative\n         x, egrad(egrad(tanh))(x),                          # second derivative\n         x, egrad(egrad(egrad(tanh)))(x),                    # third  derivative\n         x, egrad(egrad(egrad(egrad(tanh))))(x),              # fourth derivative\n         x, egrad(egrad(egrad(egrad(egrad(tanh)))))(x),        # fifth  derivative\n         x, egrad(egrad(egrad(egrad(egrad(egrad(tanh))))))(x))  # sixth  derivative\n\nplt.axis('off')\nplt.savefig(\"tanh.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom autograd import grad\ngrad_tanh = grad(tanh)            # Obtain its gradient function\ngA = grad_tanh(1.0)               # Evaluate the gradient at x = 1.0\ngN = (tanh(1.01) - tanh(0.99)) / 0.02  # Compare to finite differences\nprint(gA, gN)\n\n0.39322386648296376 0.3932226889551027\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_lin_reg.html",
    "href": "labs/02Examples/ad/autograd_lin_reg.html",
    "title": "Linear Regression with autograd",
    "section": "",
    "text": "We use autograd to perform a linear regression on some randomly distributed data, with added random noise. We then compare the results with a linear regression performed using sklearn.\nIn the autograd implementation, we will use a basic gradient descent that minimizes the mean-squared loss function to find the two coefficients, slope and intercept.\nIn a later example, this will be done using pytorch.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport autograd.numpy as ag_np\nfrom autograd import grad\n\n# Generate some random data and form a linear function\nnp.random.seed(42)\nX = np.random.rand(50, 1) * 10\ny = 2 * X + 3 + np.random.randn(50, 1) # noisy line\n\n# Define the linear regression model\ndef linear_regression(params, x):\n    return ag_np.dot(x, params[0]) + params[1]\n\n# Define the loss function = mean squared error\ndef mean_squared_error(params, x, y):\n    predictions = linear_regression(params, x)\n    return ag_np.mean((predictions - y) ** 2)\n\n# Initialize parameters\ninitial_params = [ag_np.ones((1, 1)), ag_np.zeros((1,))]\nlr = 0.01\nnum_epochs = 1000\n\n# Gradient of the loss function using autograd\ngrad_loss = grad(mean_squared_error)\n\n# Optimization loop\nparams = initial_params\nfor epoch in range(num_epochs):\n    gradient = grad_loss(params, X, y)\n    params[0] -= lr * gradient[0]\n    params[1] -= lr * gradient[1]\n\n# Extract the learned slope and intercept\nslope = params[0][0, 0]\nintercept = params[1][0]\n\n# Plot the data points and the resulting line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, label='Data Points')\nplt.plot(X, slope * X + intercept, color='red', label='Regression Line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression using Autograd')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nLet us compare with scikit_learn\n\nfrom sklearn.linear_model import LinearRegression\n# setup model\nmodel = LinearRegression()\n# fit\nres = model.fit(X, y)\n# predict\npredictions = model.predict(X)\n# plot\nplt.figure(figsize=(8, 6))\nplt.plot(X, predictions)\nplt.grid(True)\nplt.show()\nprint(\"sklearn: intercept = \",res.intercept_,\"slope = \", res.coef_[0],)\nprint(\"autograd: intercept = \",intercept,\"slope = \", slope,)\n\n\n\n\n\n\n\n\nsklearn: intercept =  [3.09668927] slope =  [1.9776566]\nautograd: intercept =  3.087098312274722 slope =  1.9791961905803472"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_lin_reg.html#linear-regression-with-autograd",
    "href": "labs/02Examples/ad/autograd_lin_reg.html#linear-regression-with-autograd",
    "title": "Linear Regression with autograd",
    "section": "",
    "text": "We use autograd to perform a linear regression on some randomly distributed data, with added random noise. We then compare the results with a linear regression performed using sklearn.\nIn the autograd implementation, we will use a basic gradient descent that minimizes the mean-squared loss function to find the two coefficients, slope and intercept.\nIn a later example, this will be done using pytorch.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport autograd.numpy as ag_np\nfrom autograd import grad\n\n# Generate some random data and form a linear function\nnp.random.seed(42)\nX = np.random.rand(50, 1) * 10\ny = 2 * X + 3 + np.random.randn(50, 1) # noisy line\n\n# Define the linear regression model\ndef linear_regression(params, x):\n    return ag_np.dot(x, params[0]) + params[1]\n\n# Define the loss function = mean squared error\ndef mean_squared_error(params, x, y):\n    predictions = linear_regression(params, x)\n    return ag_np.mean((predictions - y) ** 2)\n\n# Initialize parameters\ninitial_params = [ag_np.ones((1, 1)), ag_np.zeros((1,))]\nlr = 0.01\nnum_epochs = 1000\n\n# Gradient of the loss function using autograd\ngrad_loss = grad(mean_squared_error)\n\n# Optimization loop\nparams = initial_params\nfor epoch in range(num_epochs):\n    gradient = grad_loss(params, X, y)\n    params[0] -= lr * gradient[0]\n    params[1] -= lr * gradient[1]\n\n# Extract the learned slope and intercept\nslope = params[0][0, 0]\nintercept = params[1][0]\n\n# Plot the data points and the resulting line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, label='Data Points')\nplt.plot(X, slope * X + intercept, color='red', label='Regression Line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression using Autograd')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nLet us compare with scikit_learn\n\nfrom sklearn.linear_model import LinearRegression\n# setup model\nmodel = LinearRegression()\n# fit\nres = model.fit(X, y)\n# predict\npredictions = model.predict(X)\n# plot\nplt.figure(figsize=(8, 6))\nplt.plot(X, predictions)\nplt.grid(True)\nplt.show()\nprint(\"sklearn: intercept = \",res.intercept_,\"slope = \", res.coef_[0],)\nprint(\"autograd: intercept = \",intercept,\"slope = \", slope,)\n\n\n\n\n\n\n\n\nsklearn: intercept =  [3.09668927] slope =  [1.9776566]\nautograd: intercept =  3.087098312274722 slope =  1.9791961905803472"
  },
  {
    "objectID": "labs/08Examples/NF_homework.html",
    "href": "labs/08Examples/NF_homework.html",
    "title": "Digital Twins for Physical Systems",
    "section": "",
    "text": "In this tutorial, we will take a closer look at complex, deep normalizing flows. The most popular, current application of deep normalizing flows is to model datasets of images. As for other generative models, images are a good domain to start working on because (1) CNNs are widely studied and strong models exist, (2) images are high-dimensional and complex, and (3) images are discrete integers. In this tutorial, we will review current advances in normalizing flows for image modeling, and get hands-on experience on coding normalizing flows. Note that normalizing flows are commonly parameter heavy and therefore computationally expensive. We will use relatively simple and shallow flows to save computational cost and allow you to run the notebook on CPU, but keep in mind that a simple way to improve the scores of the flows we study here is to make them deeper.\nThroughout this notebook, we make use of PyTorch Lightning. The first cell imports our usual libraries.\n## Standard libraries\nimport os\nimport math\nimport time\nimport numpy as np\n\n## Imports for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('svg', 'pdf') # For export\nfrom matplotlib.colors import to_rgb\nimport matplotlib\nmatplotlib.rcParams['lines.linewidth'] = 2.0\nimport seaborn as sns\nsns.reset_orig()\n\n## Progress bar\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\n# Torchvision\nimport torchvision\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n# PyTorch Lightning\ntry:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n    !pip install --quiet pytorch-lightning&gt;=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\nDATASET_PATH = \"../data\"\n# Path to the folder where the pretrained models are saved\nCHECKPOINT_PATH = \"../saved_models/tutorial11\"\n\n# Setting the seed\npl.seed_everything(42)\n\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Fetching the device that will be used throughout this notebook\ndevice = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\nprint(\"Using device\", device)\n\nDeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  set_matplotlib_formats('svg', 'pdf') # For export\nINFO:lightning_fabric.utilities.seed:Seed set to 42\n\n\nUsing device cpu\nWe will use the MNIST dataset in this notebook. MNIST constitutes, despite its simplicity, a challenge for small generative models as it requires the global understanding of an image. At the same time, we can easily judge whether generated images come from the same distribution as the dataset (i.e. represent real digits), or not.\nTo deal better with the discrete nature of the images, we transform them from a range of 0-1 to a range of 0-255 as integers.\n# Convert images from 0-1 to 0-255 (integers)\ndef discretize(sample):\n    return (sample * 255).to(torch.int32)\n\n# Transformations applied on each image =&gt; make them a tensor and discretize\ntransform = transforms.Compose([transforms.ToTensor(),\n                                discretize])\n\n# Loading the training dataset. We need to split it into a training and validation part\ntrain_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\npl.seed_everything(42)\ntrain_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n\n# Loading the test set\ntest_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n\n# We define a set of data loaders that we can use for various purposes later.\n# Note that for actually training a model, we will use different data loaders\n# with a lower batch size.\ntrain_loader = data.DataLoader(train_set, batch_size=256, shuffle=False, drop_last=False)\nval_loader = data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\ntest_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 88727407.70it/s]\n100%|██████████| 28881/28881 [00:00&lt;00:00, 31204454.87it/s]\n100%|██████████| 1648877/1648877 [00:02&lt;00:00, 579297.24it/s]\n100%|██████████| 4542/4542 [00:00&lt;00:00, 15107477.21it/s]\nINFO:lightning_fabric.utilities.seed:Seed set to 42\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nIn addition, we will define below a function to simplify the visualization of images/samples. Some training examples of the MNIST dataset is shown below.\ndef show_imgs(imgs, title=None, row_size=4):\n    # Form a grid of pictures (we use max. 8 columns)\n    num_imgs = imgs.shape[0] if isinstance(imgs, torch.Tensor) else len(imgs)\n    is_int = imgs.dtype==torch.int32 if isinstance(imgs, torch.Tensor) else imgs[0].dtype==torch.int32\n    nrow = min(num_imgs, row_size)\n    ncol = int(math.ceil(num_imgs/nrow))\n    imgs = torchvision.utils.make_grid(imgs, nrow=nrow, pad_value=128 if is_int else 0.5)\n    np_imgs = imgs.cpu().numpy()\n    # Plot the grid\n    plt.figure(figsize=(1.5*nrow, 1.5*ncol))\n    plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n    plt.axis('off')\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n\nshow_imgs([train_set[i][0] for i in range(8)])"
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#normalizing-flows-as-generative-model",
    "href": "labs/08Examples/NF_homework.html#normalizing-flows-as-generative-model",
    "title": "Digital Twins for Physical Systems",
    "section": "Normalizing Flows as generative model",
    "text": "Normalizing Flows as generative model\nIn the previous lectures, we have seen Energy-based models, Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) as example of generative models. However, none of them explicitly learn the probability density function \\(p(x)\\) of the real input data. While VAEs model a lower bound, energy-based models only implicitly learn the probability density. GANs on the other hand provide us a sampling mechanism for generating new data, without offering a likelihood estimate. The generative model we will look at here, called Normalizing Flows, actually models the true data distribution \\(p(x)\\) and provides us with an exact likelihood estimate. Below, we can visually compare VAEs, GANs and Flows (figure credit - Lilian Weng):\n\n\n\nThe major difference compared to VAEs is that flows use invertible functions \\(f\\) to map the input data \\(x\\) to a latent representation \\(z\\). To realize this, \\(z\\) must be of the same shape as \\(x\\). This is in contrast to VAEs where \\(z\\) is usually much lower dimensional than the original input data. However, an invertible mapping also means that for every data point \\(x\\), we have a corresponding latent representation \\(z\\) which allows us to perform lossless reconstruction (\\(z\\) to \\(x\\)). In the visualization above, this means that \\(x=x'\\) for flows, no matter what invertible function \\(f\\) and input \\(x\\) we choose.\nNonetheless, how are normalizing flows modeling a probability density with an invertible function? The answer to this question is the rule for change of variables. Specifically, given a prior density \\(p_z(z)\\) (e.g. Gaussian) and an invertible function \\(f\\), we can determine \\(p_x(x)\\) as follows:\n\\[\n\\begin{split}\n    \\int p_x(x) dx & = \\int p_z(z) dz = 1 \\hspace{1cm}\\text{(by definition of a probability distribution)}\\\\\n    \\Leftrightarrow p_x(x) & = p_z(z) \\left|\\frac{dz}{dx}\\right| = p_z(f(x)) \\left|\\frac{df(x)}{dx}\\right|\n\\end{split}\n\\]\nHence, in order to determine the probability of \\(x\\), we only need to determine its probability in latent space, and get the derivate of \\(f\\). Note that this is for a univariate distribution, and \\(f\\) is required to be invertible and smooth. For a multivariate case, the derivative becomes a Jacobian of which we need to take the determinant. As we usually use the log-likelihood as objective, we write the multivariate term with logarithms below:\n\\[\n\\log p_x(\\mathbf{x}) = \\log p_z(f(\\mathbf{x})) + \\log{} \\left|\\det \\frac{df(\\mathbf{x})}{d\\mathbf{x}}\\right|\n\\]\nAlthough we now know how a normalizing flow obtains its likelihood, it might not be clear what a normalizing flow does intuitively. For this, we should look from the inverse perspective of the flow starting with the prior probability density \\(p_z(z)\\). If we apply an invertible function on it, we effectively “transform” its probability density. For instance, if \\(f^{-1}(z)=z+1\\), we shift the density by one while still remaining a valid probability distribution, and being invertible. We can also apply more complex transformations, like scaling: \\(f^{-1}(z)=2z+1\\), but there you might see a difference. When you scale, you also change the volume of the probability density, as for example on uniform distributions (figure credit - Eric Jang):\n\n\n\nYou can see that the height of \\(p(y)\\) should be lower than \\(p(x)\\) after scaling. This change in volume represents \\(\\left|\\frac{df(x)}{dx}\\right|\\) in our equation above, and ensures that even after scaling, we still have a valid probability distribution. We can go on with making our function \\(f\\) more complex. However, the more complex \\(f\\) becomes, the harder it will be to find the inverse \\(f^{-1}\\) of it, and to calculate the log-determinant of the Jacobian \\(\\log{} \\left|\\det \\frac{df(\\mathbf{x})}{d\\mathbf{x}}\\right|\\). An easier trick to stack multiple invertible functions \\(f_{1,...,K}\\) after each other, as all together, they still represent a single, invertible function. Using multiple, learnable invertible functions, a normalizing flow attempts to transform \\(p_z(z)\\) slowly into a more complex distribution which should finally be \\(p_x(x)\\). We visualize the idea below (figure credit - Lilian Weng):\n\n\n\nStarting from \\(z_0\\), which follows the prior Gaussian distribution, we sequentially apply the invertible functions \\(f_1,f_2,...,f_K\\), until \\(z_K\\) represents \\(x\\). Note that in the figure above, the functions \\(f\\) represent the inverted function from \\(f\\) we had above (here: \\(f:Z\\to X\\), above: \\(f:X\\to Z\\)). This is just a different notation and has no impact on the actual flow design because all \\(f\\) need to be invertible anyways. When we estimate the log likelihood of a data point \\(x\\) as in the equations above, we run the flows in the opposite direction than visualized above. Multiple flow layers have been proposed that use a neural network as learnable parameters, such as the planar and radial flow. However, we will focus here on flows that are commonly used in image modeling, and will discuss them in the rest of the notebook along with the details of how to train a normalizing flow."
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#normalizing-flows-on-images",
    "href": "labs/08Examples/NF_homework.html#normalizing-flows-on-images",
    "title": "Digital Twins for Physical Systems",
    "section": "Normalizing Flows on images",
    "text": "Normalizing Flows on images\nTo become familiar with normalizing flows, especially for the application of image modeling, it is best to discuss the different elements in a flow along with the implementation. As a general concept, we want to build a normalizing flow that maps an input image (here MNIST) to an equally sized latent space:\n\n\n\nAs a first step, we will implement a template of a normalizing flow in PyTorch Lightning. During training and validation, a normalizing flow performs density estimation in the forward direction. For this, we apply a series of flow transformations on the input \\(x\\) and estimate the probability of the input by determining the probability of the transformed point \\(z\\) given a prior, and the change of volume caused by the transformations. During inference, we can do both density estimation and sampling new points by inverting the flow transformations. Therefore, we define a function _get_likelihood which performs density estimation, and sample to generate new examples. The functions training_step, validation_step and test_step all make use of _get_likelihood.\nThe standard metric used in generative models, and in particular normalizing flows, is bits per dimensions (bpd). Bpd is motivated from an information theory perspective and describes how many bits we would need to encode a particular example in our modeled distribution. The less bits we need, the more likely the example in our distribution. When we test for the bits per dimension of our test dataset, we can judge whether our model generalizes to new samples of the dataset and didn’t memorize the training dataset. In order to calculate the bits per dimension score, we can rely on the negative log-likelihood and change the log base (as bits are binary while NLL is usually exponential):\n\\[\\text{bpd} = \\text{nll} \\cdot \\log_2\\left(\\exp(1)\\right) \\cdot \\left(\\prod d_i\\right)^{-1}\\]\nwhere \\(d_1,...,d_K\\) are the dimensions of the input. For images, this would be the height, width and channel number. We divide the log likelihood by these extra dimensions to have a metric which we can compare for different image resolutions. In the original image space, MNIST examples have a bits per dimension score of 8 (we need 8 bits to encode each pixel as there are 256 possible values).\n\nclass ImageFlow(pl.LightningModule):\n\n    def __init__(self, flows, import_samples=8):\n        \"\"\"\n        Inputs:\n            flows - A list of flows (each a nn.Module) that should be applied on the images.\n            import_samples - Number of importance samples to use during testing (see explanation below). Can be changed at any time\n        \"\"\"\n        super().__init__()\n        self.flows = nn.ModuleList(flows)\n        self.import_samples = import_samples\n        # Create prior distribution for final latent space\n        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n        # Example input for visualizing the graph\n        self.example_input_array = train_set[0][0].unsqueeze(dim=0)\n\n    def forward(self, imgs):\n        # The forward function is only used for visualizing the graph\n        return self._get_likelihood(imgs)\n\n    def encode(self, imgs):\n        # Given a batch of images, return the latent representation z and ldj of the transformations\n        z, ldj = imgs, torch.zeros(imgs.shape[0], device=self.device)\n        for flow in self.flows:\n            z, ldj = flow(z, ldj, reverse=False)\n        return z, ldj\n\n    def _get_likelihood(self, imgs, return_ll=False):\n        \"\"\"\n        Given a batch of images, return the likelihood of those.\n        If return_ll is True, this function returns the log likelihood of the input.\n        Otherwise, the ouptut metric is bits per dimension (scaled negative log likelihood)\n        \"\"\"\n        # Insert your code here!!!\n        # Calculate negative log likelihood\n        # (HINT: Step1- Use encoder, Step2- Calculate likelihood pz, Step3- Find likelihood px)\n        #\n        #\n        #\n        #\n        #-------------------------\n\n        # Calculating bits per dimension\n        bpd = nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n        return bpd.mean() if not return_ll else log_px\n\n    @torch.no_grad()\n    def sample(self, img_shape, z_init=None):\n        \"\"\"\n        Sample a batch of images from the flow.\n        \"\"\"\n        # Sample latent representation from prior\n        if z_init is None:\n            z = self.prior.sample(sample_shape=img_shape).to(device)\n        else:\n            z = z_init.to(device)\n\n        # Transform z to x by inverting the flows\n        ldj = torch.zeros(img_shape[0], device=device)\n        for flow in reversed(self.flows):\n            z, ldj = flow(z, ldj, reverse=True)\n        return z\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        # An scheduler is optional, but can help in flows to get the last bpd improvement\n        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        # Normalizing flows are trained by maximum likelihood =&gt; return bpd\n        loss = self._get_likelihood(batch[0])\n        self.log('train_bpd', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss = self._get_likelihood(batch[0])\n        self.log('val_bpd', loss)\n\n    def test_step(self, batch, batch_idx):\n        # Perform importance sampling during testing =&gt; estimate likelihood M times for each image\n        samples = []\n        for _ in range(self.import_samples):\n            img_ll = self._get_likelihood(batch[0], return_ll=True)\n            samples.append(img_ll)\n        img_ll = torch.stack(samples, dim=-1)\n\n        # To average the probabilities, we need to go from log-space to exp, and back to log.\n        # Logsumexp provides us a stable implementation for this\n        img_ll = torch.logsumexp(img_ll, dim=-1) - np.log(self.import_samples)\n\n        # Calculate final bpd\n        bpd = -img_ll * np.log2(np.exp(1)) / np.prod(batch[0].shape[1:])\n        bpd = bpd.mean()\n\n        self.log('test_bpd', bpd)\n\nThe test_step function differs from the training and validation step in that it makes use of importance sampling. We will discuss the motiviation and details behind this after understanding how flows model discrete images in continuous space.\n\nDequantization\nNormalizing flows rely on the rule of change of variables, which is naturally defined in continuous space. Applying flows directly on discrete data leads to undesired density models where arbitrarly high likelihood are placed on a few, particular values. See the illustration below:\n\n\n\nThe black points represent the discrete points, and the green volume the density modeled by a normalizing flow in continuous space. The flow would continue to increase the likelihood for \\(x=0,1,2,3\\) while having no volume on any other point. Remember that in continuous space, we have the constraint that the overall volume of the probability density must be 1 (\\(\\int p(x)dx=1\\)). Otherwise, we don’t model a probability distribution anymore. However, the discrete points \\(x=0,1,2,3\\) represent delta peaks with no width in continuous space. This is why the flow can place an infinite high likelihood on these few points while still representing a distribution in continuous space. Nonetheless, the learned density does not tell us anything about the distribution among the discrete points, as in discrete space, the likelihoods of those four points would have to sum to 1, not to infinity.\nTo prevent such degenerated solutions, a common solution is to add a small amount of noise to each discrete value, which is also referred to as dequantization. Considering \\(x\\) as an integer (as it is the case for images), the dequantized representation \\(v\\) can be formulated as \\(v=x+u\\) where \\(u\\in[0,1)^D\\). Thus, the discrete value \\(1\\) is modeled by a distribution over the interval \\([1.0, 2.0)\\), the value \\(2\\) by an volume over \\([2.0, 3.0)\\), etc. Our objective of modeling \\(p(x)\\) becomes:\n\\[ p(x) = \\int p(x+u)du = \\int \\frac{q(u|x)}{q(u|x)}p(x+u)du = \\mathbb{E}_{u\\sim q(u|x)}\\left[\\frac{p(x+u)}{q(u|x)} \\right]\\]\nwith \\(q(u|x)\\) being the noise distribution. For now, we assume it to be uniform, which can also be written as \\(p(x)=\\mathbb{E}_{u\\sim U(0,1)^D}\\left[p(x+u) \\right]\\).\nIn the following, we will implement Dequantization as a flow transformation itself. After adding noise to the discrete values, we additionally transform the volume into a Gaussian-like shape. This is done by scaling \\(x+u\\) between \\(0\\) and \\(1\\), and applying the invert of the sigmoid function \\(\\sigma(z)^{-1} = \\log z - \\log 1-z\\). If we would not do this, we would face two problems:\n\nThe input is scaled between 0 and 256 while the prior distribution is a Gaussian with mean \\(0\\) and standard deviation \\(1\\). In the first iterations after initializing the parameters of the flow, we would have extremely low likelihoods for large values like \\(256\\). This would cause the training to diverge instantaneously.\nAs the output distribution is a Gaussian, it is beneficial for the flow to have a similarly shaped input distribution. This will reduce the modeling complexity that is required by the flow.\n\nOverall, we can implement dequantization as follows:\n\nclass Dequantization(nn.Module):\n\n    def __init__(self, alpha=1e-5, quants=256):\n        \"\"\"\n        Inputs:\n            alpha - small constant that is used to scale the original input.\n                    Prevents dealing with values very close to 0 and 1 when inverting the sigmoid\n            quants - Number of possible discrete values (usually 256 for 8-bit image)\n        \"\"\"\n        super().__init__()\n        self.alpha = alpha\n        self.quants = quants\n\n    def forward(self, z, ldj, reverse=False):\n        if not reverse:\n            z, ldj = self.dequant(z, ldj)\n            z, ldj = self.sigmoid(z, ldj, reverse=True)\n        else:\n            z, ldj = self.sigmoid(z, ldj, reverse=False)\n            z = z * self.quants\n            ldj += np.log(self.quants) * np.prod(z.shape[1:])\n            z = torch.floor(z).clamp(min=0, max=self.quants-1).to(torch.int32)\n        return z, ldj\n\n    def sigmoid(self, z, ldj, reverse=False):\n        # Applies an invertible sigmoid transformation\n        if not reverse:\n            ldj += (-z-2*F.softplus(-z)).sum(dim=[1,2,3])\n            z = torch.sigmoid(z)\n            # Reversing scaling for numerical stability\n            ldj -= np.log(1 - self.alpha) * np.prod(z.shape[1:])\n            z = (z - 0.5 * self.alpha) / (1 - self.alpha)\n        else:\n            z = z * (1 - self.alpha) + 0.5 * self.alpha  # Scale to prevent boundaries 0 and 1\n            ldj += np.log(1 - self.alpha) * np.prod(z.shape[1:])\n            ldj += (-torch.log(z) - torch.log(1-z)).sum(dim=[1,2,3])\n            z = torch.log(z) - torch.log(1-z)\n        return z, ldj\n\n    def dequant(self, z, ldj):\n        # Transform discrete values to continuous volumes\n        z = z.to(torch.float32)\n        z = z + torch.rand_like(z).detach()\n        z = z / self.quants\n        ldj -= np.log(self.quants) * np.prod(z.shape[1:])\n        return z, ldj\n\nA good check whether a flow is correctly implemented or not, is to verify that it is invertible. Hence, we will dequantize a randomly chosen training image, and then quantize it again. We would expect that we would get the exact same image out:\n\n## Testing invertibility of dequantization layer\npl.seed_everything(42)\norig_img = train_set[0][0].unsqueeze(dim=0)\nldj = torch.zeros(1,)\ndequant_module = Dequantization()\ndeq_img, ldj = dequant_module(orig_img, ldj, reverse=False)\nreconst_img, ldj = dequant_module(deq_img, ldj, reverse=True)\n\nd1, d2 = torch.where(orig_img.squeeze() != reconst_img.squeeze())\nif len(d1) != 0:\n    print(\"Dequantization was not invertible.\")\n    for i in range(d1.shape[0]):\n        print(\"Original value:\", orig_img[0,0,d1[i], d2[i]].item())\n        print(\"Reconstructed value:\", reconst_img[0,0,d1[i], d2[i]].item())\nelse:\n    print(\"Successfully inverted dequantization\")\n\n# Layer is not strictly invertible due to float precision constraints\n# assert (orig_img == reconst_img).all().item()\n\nINFO:lightning_fabric.utilities.seed:Seed set to 42\n\n\nSuccessfully inverted dequantization\n\n\nThe test succeeds as we would expect. However, there is a chance that the test fails due to numerical inaccuracies in the sigmoid invert. While the input space to the inverted sigmoid is scaled between 0 and 1, the output space is between \\(-\\infty\\) and \\(\\infty\\). And as we use 32 bits to represent the numbers (in addition to applying logs over and over again), such inaccuries can occur and should not be worrisome. Nevertheless, it is good to be aware of them, and can be improved by using a double tensor (float64).\nFinally, we can take our dequantization and actually visualize the distribution it transforms the discrete values into:\n\ndef visualize_dequantization(quants, prior=None):\n    \"\"\"\n    Function for visualizing the dequantization values of discrete values in continuous space\n    \"\"\"\n    # Prior over discrete values. If not given, a uniform is assumed\n    if prior is None:\n        prior = np.ones(quants, dtype=np.float32) / quants\n    prior = prior / prior.sum()  # Ensure proper categorical distribution\n\n    inp = torch.arange(-4, 4, 0.01).view(-1, 1, 1, 1) # Possible continuous values we want to consider\n    ldj = torch.zeros(inp.shape[0])\n    dequant_module = Dequantization(quants=quants)\n    # Invert dequantization on continuous values to find corresponding discrete value\n    out, ldj = dequant_module.forward(inp, ldj, reverse=True)\n    inp, out, prob = inp.squeeze().numpy(), out.squeeze().numpy(), ldj.exp().numpy()\n    prob = prob * prior[out] # Probability scaled by categorical prior\n\n    # Plot volumes and continuous distribution\n    sns.set_style(\"white\")\n    fig = plt.figure(figsize=(6,3))\n    x_ticks = []\n    for v in np.unique(out):\n        indices = np.where(out==v)\n        color = to_rgb(f\"C{v}\")\n        plt.fill_between(inp[indices], prob[indices], np.zeros(indices[0].shape[0]), color=color+(0.5,), label=str(v))\n        plt.plot([inp[indices[0][0]]]*2,  [0, prob[indices[0][0]]],  color=color)\n        plt.plot([inp[indices[0][-1]]]*2, [0, prob[indices[0][-1]]], color=color)\n        x_ticks.append(inp[indices[0][0]])\n    x_ticks.append(inp.max())\n    plt.xticks(x_ticks, [f\"{x:.1f}\" for x in x_ticks])\n    plt.plot(inp,prob, color=(0.0,0.0,0.0))\n    # Set final plot properties\n    plt.ylim(0, prob.max()*1.1)\n    plt.xlim(inp.min(), inp.max())\n    plt.xlabel(\"z\")\n    plt.ylabel(\"Probability\")\n    plt.title(f\"Dequantization distribution for {quants} discrete values\")\n    plt.legend()\n    plt.show()\n    plt.close()\n\nvisualize_dequantization(quants=8)\n\n\n\n\n\n\n\n\nThe visualized distribution show the sub-volumes that are assigned to the different discrete values. The value \\(0\\) has its volume between \\([-\\infty, -1.9)\\), the value \\(1\\) is represented by the interval \\([-1.9, -1.1)\\), etc. The volume for each discrete value has the same probability mass. That’s why the volumes close to the center (e.g. 3 and 4) have a smaller area on the z-axis as others (\\(z\\) is being used to denote the output of the whole dequantization flow).\nEffectively, the consecutive normalizing flow models discrete images by the following objective:\n\\[\\log p(x) = \\log \\mathbb{E}_{u\\sim q(u|x)}\\left[\\frac{p(x+u)}{q(u|x)} \\right] \\geq \\mathbb{E}_{u}\\left[\\log \\frac{p(x+u)}{q(u|x)} \\right]\\]\nAlthough normalizing flows are exact in likelihood, we have a lower bound. Specifically, this is an example of the Jensen inequality because we need to move the log into the expectation so we can use Monte-carlo estimates. In general, this bound is considerably smaller than the ELBO in variational autoencoders. Actually, we can reduce the bound ourselves by estimating the expectation not by one, but by \\(M\\) samples. In other words, we can apply importance sampling which leads to the following inequality:\n\\[\\log p(x) = \\log \\mathbb{E}_{u\\sim q(u|x)}\\left[\\frac{p(x+u)}{q(u|x)} \\right] \\geq \\mathbb{E}_{u}\\left[\\log \\frac{1}{M} \\sum_{m=1}^{M} \\frac{p(x+u_m)}{q(u_m|x)} \\right] \\geq \\mathbb{E}_{u}\\left[\\log \\frac{p(x+u)}{q(u|x)} \\right]\\]\nThe importance sampling \\(\\frac{1}{M} \\sum_{m=1}^{M} \\frac{p(x+u_m)}{q(u_m|x)}\\) becomes \\(\\mathbb{E}_{u\\sim q(u|x)}\\left[\\frac{p(x+u)}{q(u|x)} \\right]\\) if \\(M\\to \\infty\\), so that the more samples we use, the tighter the bound is. During testing, we can make use of this property and have it implemented in test_step in ImageFlow. In theory, we could also use this tighter bound during training. However, related work has shown that this does not necessarily lead to an improvement given the additional computational cost, and it is more efficient to stick with a single estimate [5].\n\n\nVariational Dequantization\nDequantization uses a uniform distribution for the noise \\(u\\) which effectively leads to images being represented as hypercubes (cube in high dimensions) with sharp borders. However, modeling such sharp borders is not easy for a flow as it uses smooth transformations to convert it into a Gaussian distribution.\nAnother way of looking at it is if we change the prior distribution in the previous visualization. Imagine we have independent Gaussian noise on pixels which is commonly the case for any real-world taken picture. Therefore, the flow would have to model a distribution as above, but with the individual volumes scaled as follows:\n\nvisualize_dequantization(quants=8, prior=np.array([0.075, 0.2, 0.4, 0.2, 0.075, 0.025, 0.0125, 0.0125]))\n\n\n\n\n\n\n\n\nTransforming such a probability into a Gaussian is a difficult task, especially with such hard borders. Dequantization has therefore been extended to more sophisticated, learnable distributions beyond uniform in a variational framework. In particular, if we remember the learning objective \\(\\log p(x) = \\log \\mathbb{E}_{u}\\left[\\frac{p(x+u)}{q(u|x)} \\right]\\), the uniform distribution can be replaced by a learned distribution \\(q_{\\theta}(u|x)\\) with support over \\(u\\in[0,1)^D\\). This approach is called Variational Dequantization and has been proposed by Ho et al. [3]. How can we learn such a distribution? We can use a second normalizing flow that takes \\(x\\) as external input and learns a flexible distribution over \\(u\\). To ensure a support over \\([0,1)^D\\), we can apply a sigmoid activation function as final flow transformation.\nInheriting the original dequantization class, we can implement variational dequantization as follows:\n\nclass VariationalDequantization(Dequantization):\n\n    def __init__(self, var_flows, alpha=1e-5):\n        \"\"\"\n        Inputs:\n            var_flows - A list of flow transformations to use for modeling q(u|x)\n            alpha - Small constant, see Dequantization for details\n        \"\"\"\n        super().__init__(alpha=alpha)\n        self.flows = nn.ModuleList(var_flows)\n\n    def dequant(self, z, ldj):\n        z = z.to(torch.float32)\n        img = (z / 255.0) * 2 - 1 # We condition the flows on x, i.e. the original image\n\n        # Prior of u is a uniform distribution as before\n        # As most flow transformations are defined on [-infinity,+infinity], we apply an inverse sigmoid first.\n        deq_noise = torch.rand_like(z).detach()\n        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=True)\n        for flow in self.flows:\n            deq_noise, ldj = flow(deq_noise, ldj, reverse=False, orig_img=img)\n        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=False)\n\n        # After the flows, apply u as in standard dequantization\n        z = (z + deq_noise) / 256.0\n        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n        return z, ldj\n\nVariational dequantization can be used as a substitute for dequantization. We will compare dequantization and variational dequantization in later experiments.\n\n\nCoupling layers\nNext, we look at possible transformations to apply inside the flow. A recent popular flow layer, which works well in combination with deep neural networks, is the coupling layer introduced by Dinh et al. [1]. The input \\(z\\) is arbitrarily split into two parts, \\(z_{1:j}\\) and \\(z_{j+1:d}\\), of which the first remains unchanged by the flow. Yet, \\(z_{1:j}\\) is used to parameterize the transformation for the second part, \\(z_{j+1:d}\\). Various transformations have been proposed in recent time [3,4], but here we will settle for the simplest and most efficient one: affine coupling. In this coupling layer, we apply an affine transformation by shifting the input by a bias \\(\\mu\\) and scale it by \\(\\sigma\\). In other words, our transformation looks as follows:\n\\[z'_{j+1:d} = \\mu_{\\theta}(z_{1:j}) + \\sigma_{\\theta}(z_{1:j}) \\odot z_{j+1:d}\\]\nThe functions \\(\\mu\\) and \\(\\sigma\\) are implemented as a shared neural network, and the sum and multiplication are performed element-wise. The LDJ is thereby the sum of the logs of the scaling factors: \\(\\sum_i \\left[\\log \\sigma_{\\theta}(z_{1:j})\\right]_i\\). Inverting the layer can as simply be done as subtracting the bias and dividing by the scale:\n\\[z_{j+1:d} = \\left(z'_{j+1:d} - \\mu_{\\theta}(z_{1:j})\\right) / \\sigma_{\\theta}(z_{1:j})\\]\nWe can also visualize the coupling layer in form of a computation graph, where \\(z_1\\) represents \\(z_{1:j}\\), and \\(z_2\\) represents \\(z_{j+1:d}\\):\n\n\n\nIn our implementation, we will realize the splitting of variables as masking. The variables to be transformed, \\(z_{j+1:d}\\), are masked when passing \\(z\\) to the shared network to predict the transformation parameters. When applying the transformation, we mask the parameters for \\(z_{1:j}\\) so that we have an identity operation for those variables:\n\nclass CouplingLayer(nn.Module):\n\n    def __init__(self, network, mask, c_in):\n        \"\"\"\n        Coupling layer inside a normalizing flow.\n        Inputs:\n            network - A PyTorch nn.Module constituting the deep neural network for mu and sigma.\n                      Output shape should be twice the channel size as the input.\n            mask - Binary mask (0 or 1) where 0 denotes that the element should be transformed,\n                   while 1 means the latent will be used as input to the NN.\n            c_in - Number of input channels\n        \"\"\"\n        super().__init__()\n        self.network = network\n        self.scaling_factor = nn.Parameter(torch.zeros(c_in))\n        # Register mask as buffer as it is a tensor which is not a parameter,\n        # but should be part of the modules state.\n        self.register_buffer('mask', mask)\n\n    def forward(self, z, ldj, reverse=False, orig_img=None):\n        \"\"\"\n        Inputs:\n            z - Latent input to the flow\n            ldj - The current ldj of the previous flows.\n                  The ldj of this layer will be added to this tensor.\n            reverse - If True, we apply the inverse of the layer.\n            orig_img (optional) - Only needed in VarDeq. Allows external\n                                  input to condition the flow on (e.g. original image)\n        \"\"\"\n        # Apply network to masked input\n        z_in = z * self.mask\n        if orig_img is None:\n            nn_out = self.network(z_in)\n        else:\n            nn_out = self.network(torch.cat([z_in, orig_img], dim=1))\n        s, t = nn_out.chunk(2, dim=1)\n\n        # Stabilize scaling output\n        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n        s = torch.tanh(s / s_fac) * s_fac\n\n        # Mask outputs (only transform the second part)\n        s = s * (1 - self.mask)\n        t = t * (1 - self.mask)\n\n        # Affine transformation\n        if not reverse:\n            # Whether we first shift and then scale, or the other way round,\n            # is a design choice, and usually does not have a big impact\n            z = (z + t) * torch.exp(s)\n            ldj += s.sum(dim=[1,2,3])\n        else:\n            z = (z * torch.exp(-s)) - t\n            ldj -= s.sum(dim=[1,2,3])\n\n        return z, ldj\n\nFor stabilization purposes, we apply a \\(\\tanh\\) activation function on the scaling output. This prevents sudden large output values for the scaling that can destabilize training. To still allow scaling factors smaller or larger than -1 and 1 respectively, we have a learnable parameter per dimension, called scaling_factor. This scales the tanh to different limits. Below, we visualize the effect of the scaling factor on the output activation of the scaling terms:\n\nwith torch.no_grad():\n    x = torch.arange(-5,5,0.01)\n    scaling_factors = [0.5, 1, 2]\n    sns.set()\n    fig, ax = plt.subplots(1, 3, figsize=(12,3))\n    for i, scale in enumerate(scaling_factors):\n        y = torch.tanh(x / scale) * scale\n        ax[i].plot(x.numpy(), y.numpy())\n        ax[i].set_title(\"Scaling factor: \" + str(scale))\n        ax[i].set_ylim(-3, 3)\n    plt.subplots_adjust(wspace=0.4)\n    sns.reset_orig()\n    plt.show()\n\n\n\n\n\n\n\n\nCoupling layers generalize to any masking technique we could think of. However, the most common approach for images is to split the input \\(z\\) in half, using a checkerboard mask or channel mask. A checkerboard mask splits the variables across the height and width dimensions and assigns each other pixel to \\(z_{j+1:d}\\). Thereby, the mask is shared across channels. In contrast, the channel mask assigns half of the channels to \\(z_{j+1:d}\\), and the other half to \\(z_{1:j+1}\\). Note that when we apply multiple coupling layers, we invert the masking for each other layer so that each variable is transformed a similar amount of times.\nLet’s implement a function that creates a checkerboard mask and a channel mask for us:\n\ndef create_checkerboard_mask(h, w, invert=False):\n    x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n    xx, yy = torch.meshgrid(x, y, indexing='ij')\n    mask = torch.fmod(xx + yy, 2)\n    mask = mask.to(torch.float32).view(1, 1, h, w)\n    if invert:\n        mask = 1 - mask\n    return mask\n\ndef create_channel_mask(c_in, invert=False):\n    mask = torch.cat([torch.ones(c_in//2, dtype=torch.float32),\n                      torch.zeros(c_in-c_in//2, dtype=torch.float32)])\n    mask = mask.view(1, c_in, 1, 1)\n    if invert:\n        mask = 1 - mask\n    return mask\n\nWe can also visualize the corresponding masks for an image of size \\(8\\times 8\\times 2\\) (2 channels):\n\ncheckerboard_mask = create_checkerboard_mask(h=8, w=8).expand(-1,2,-1,-1)\nchannel_mask = create_channel_mask(c_in=2).expand(-1,-1,8,8)\n\nshow_imgs(checkerboard_mask.transpose(0,1), \"Checkerboard mask\")\nshow_imgs(channel_mask.transpose(0,1), \"Channel mask\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a last aspect of coupling layers, we need to decide for the deep neural network we want to apply in the coupling layers. The input to the layers is an image, and hence we stick with a CNN. Because the input to a transformation depends on all transformations before, it is crucial to ensure a good gradient flow through the CNN back to the input, which can be optimally achieved by a ResNet-like architecture. Specifically, we use a Gated ResNet that adds a \\(\\sigma\\)-gate to the skip connection, similarly to the input gate in LSTMs. The details are not necessarily important here, and the network is strongly inspired from Flow++ [3] in case you are interested in building even stronger models.\n\nclass ConcatELU(nn.Module):\n    \"\"\"\n    Activation function that applies ELU in both direction (inverted and plain).\n    Allows non-linearity while providing strong gradients for any input (important for final convolution)\n    \"\"\"\n\n    def forward(self, x):\n        return torch.cat([F.elu(x), F.elu(-x)], dim=1)\n\n\nclass LayerNormChannels(nn.Module):\n\n    def __init__(self, c_in, eps=1e-5):\n        \"\"\"\n        This module applies layer norm across channels in an image.\n        Inputs:\n            c_in - Number of channels of the input\n            eps - Small constant to stabilize std\n        \"\"\"\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(1, c_in, 1, 1))\n        self.beta = nn.Parameter(torch.zeros(1, c_in, 1, 1))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=1, keepdim=True)\n        var = x.var(dim=1, unbiased=False, keepdim=True)\n        y = (x - mean) / torch.sqrt(var + self.eps)\n        y = y * self.gamma + self.beta\n        return y\n\n\nclass GatedConv(nn.Module):\n\n    def __init__(self, c_in, c_hidden):\n        \"\"\"\n        This module applies a two-layer convolutional ResNet block with input gate\n        Inputs:\n            c_in - Number of channels of the input\n            c_hidden - Number of hidden dimensions we want to model (usually similar to c_in)\n        \"\"\"\n        super().__init__()\n        self.net = nn.Sequential(\n            ConcatELU(),\n            nn.Conv2d(2*c_in, c_hidden, kernel_size=3, padding=1),\n            ConcatELU(),\n            nn.Conv2d(2*c_hidden, 2*c_in, kernel_size=1)\n        )\n\n    def forward(self, x):\n        out = self.net(x)\n        val, gate = out.chunk(2, dim=1)\n        return x + val * torch.sigmoid(gate)\n\n\nclass GatedConvNet(nn.Module):\n\n    def __init__(self, c_in, c_hidden=32, c_out=-1, num_layers=3):\n        \"\"\"\n        Module that summarizes the previous blocks to a full convolutional neural network.\n        Inputs:\n            c_in - Number of input channels\n            c_hidden - Number of hidden dimensions to use within the network\n            c_out - Number of output channels. If -1, 2 times the input channels are used (affine coupling)\n            num_layers - Number of gated ResNet blocks to apply\n        \"\"\"\n        super().__init__()\n        c_out = c_out if c_out &gt; 0 else 2 * c_in\n        layers = []\n        layers += [nn.Conv2d(c_in, c_hidden, kernel_size=3, padding=1)]\n        for layer_index in range(num_layers):\n            layers += [GatedConv(c_hidden, c_hidden),\n                       LayerNormChannels(c_hidden)]\n        layers += [ConcatELU(),\n                   nn.Conv2d(2*c_hidden, c_out, kernel_size=3, padding=1)]\n        self.nn = nn.Sequential(*layers)\n\n        self.nn[-1].weight.data.zero_()\n        self.nn[-1].bias.data.zero_()\n\n    def forward(self, x):\n        return self.nn(x)\n\n\n\nTraining loop\nFinally, we can add Dequantization, Variational Dequantization and Coupling Layers together to build our full normalizing flow on MNIST images. We apply 8 coupling layers in the main flow, and 4 for variational dequantization if applied. We apply a checkerboard mask throughout the network as with a single channel (black-white images), we cannot apply channel mask. The overall architecture is visualized below.\n\n\n\n\ndef create_simple_flow(use_vardeq=True):\n    flow_layers = []\n    if use_vardeq:\n        vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n                                       c_in=1) for i in range(4)]\n        flow_layers += [VariationalDequantization(var_flows=vardeq_layers)]\n    else:\n        flow_layers += [Dequantization()]\n\n    for i in range(8):\n        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, c_hidden=32),\n                                      mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n                                      c_in=1)]\n\n    flow_model = ImageFlow(flow_layers).to(device)\n    return flow_model\n\nFor implementing the training loop, we use the framework of PyTorch Lightning and reduce the code overhead. If interested, you can take a look at the generated tensorboard file, in particularly the graph to see an overview of flow transformations that are applied. Note that we again provide pre-trained models (see later on in the notebook) as normalizing flows are particularly expensive to train. We have also run validation and testing as this can take some time as well with the added importance sampling.\n\ndef train_flow(flow, model_name=\"MNISTFlow\"):\n    # Create a PyTorch Lightning trainer\n    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_name),\n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=200,\n                         gradient_clip_val=1.0,\n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_bpd\"),\n                                    LearningRateMonitor(\"epoch\")],\n                         check_val_every_n_epoch=5)\n    trainer.logger._log_graph = True\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n\n    train_data_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=8)\n    result = None\n\n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, model_name + \".ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(\"Found pretrained model, loading...\")\n        ckpt = torch.load(pretrained_filename, map_location=device)\n        flow.load_state_dict(ckpt['state_dict'])\n        result = ckpt.get(\"result\", None)\n    else:\n        print(\"Start training\", model_name)\n        trainer.fit(flow, train_data_loader, val_loader)\n\n    # Test best model on validation and test set if no result has been found\n    # Testing can be expensive due to the importance sampling.\n    if result is None:\n        val_result = trainer.test(flow, val_loader, verbose=False)\n        start_time = time.time()\n        test_result = trainer.test(flow, test_loader, verbose=False)\n        duration = time.time() - start_time\n        result = {\"test\": test_result, \"val\": val_result, \"time\": duration / len(test_loader) / flow.import_samples}\n\n    return flow, result"
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#multi-scale-architecture",
    "href": "labs/08Examples/NF_homework.html#multi-scale-architecture",
    "title": "Digital Twins for Physical Systems",
    "section": "Multi-scale architecture",
    "text": "Multi-scale architecture\nOne disadvantage of normalizing flows is that they operate on the exact same dimensions as the input. If the input is high-dimensional, so is the latent space, which requires larger computational cost to learn suitable transformations. However, particularly in the image domain, many pixels contain less information in the sense that we could remove them without loosing the semantical information of the image.\nBased on this intuition, deep normalizing flows on images commonly apply a multi-scale architecture [1]. After the first \\(N\\) flow transformations, we split off half of the latent dimensions and directly evaluate them on the prior. The other half is run through \\(N\\) more flow transformations, and depending on the size of the input, we split it again in half or stop overall at this position. The two operations involved in this setup is Squeeze and Split which we will review more closely and implement below.\n\nSqueeze and Split\nWhen we want to remove half of the pixels in an image, we have the problem of deciding which variables to cut, and how to rearrange the image. Thus, the squeezing operation is commonly used before split, which divides the image into subsquares of shape \\(2\\times 2\\times C\\), and reshapes them into \\(1\\times 1\\times 4C\\) blocks. Effectively, we reduce the height and width of the image by a factor of 2 while scaling the number of channels by 4. Afterwards, we can perform the split operation over channels without the need of rearranging the pixels. The smaller scale also makes the overall architecture more efficient. Visually, the squeeze operation should transform the input as follows:\n\n\n\nThe input of \\(4\\times 4\\times 1\\) is scaled to \\(2\\times 2\\times 4\\) following the idea of grouping the pixels in \\(2\\times 2\\times 1\\) subsquares. Next, let’s try to implement this layer:\n\nclass SqueezeFlow(nn.Module):\n\n    def forward(self, z, ldj, reverse=False):\n        B, C, H, W = z.shape\n        if not reverse:\n            # Forward direction: H x W x C =&gt; H/2 x W/2 x 4C\n            z = z.reshape(B, C, H//2, 2, W//2, 2)\n            z = z.permute(0, 1, 3, 5, 2, 4)\n            z = z.reshape(B, 4*C, H//2, W//2)\n        else:\n            # Reverse direction: H/2 x W/2 x 4C =&gt; H x W x C\n            z = z.reshape(B, C//4, 2, 2, H, W)\n            z = z.permute(0, 1, 4, 2, 5, 3)\n            z = z.reshape(B, C//4, H*2, W*2)\n        return z, ldj\n\nBefore moving on, we can verify our implementation by comparing our output with the example figure above:\n\nsq_flow = SqueezeFlow()\nrand_img = torch.arange(1,17).view(1, 1, 4, 4)\nprint(\"Image (before)\\n\", rand_img)\nforward_img, _ = sq_flow(rand_img, ldj=None, reverse=False)\nprint(\"\\nImage (forward)\\n\", forward_img.permute(0,2,3,1)) # Permute for readability\nreconst_img, _ = sq_flow(forward_img, ldj=None, reverse=True)\nprint(\"\\nImage (reverse)\\n\", reconst_img)\n\nImage (before)\n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [13, 14, 15, 16]]]])\n\nImage (forward)\n tensor([[[[ 1,  2,  5,  6],\n          [ 3,  4,  7,  8]],\n\n         [[ 9, 10, 13, 14],\n          [11, 12, 15, 16]]]])\n\nImage (reverse)\n tensor([[[[ 1,  2,  3,  4],\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [13, 14, 15, 16]]]])\n\n\nThe split operation divides the input into two parts, and evaluates one part directly on the prior. So that our flow operation fits to the implementation of the previous layers, we will return the prior probability of the first part as the log determinant jacobian of the layer. It has the same effect as if we would combine all variable splits at the end of the flow, and evaluate them together on the prior.\n\nclass SplitFlow(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n\n    def forward(self, z, ldj, reverse=False):\n        if not reverse:\n            z, z_split = z.chunk(2, dim=1)\n            ldj += self.prior.log_prob(z_split).sum(dim=[1,2,3])\n        else:\n            z_split = self.prior.sample(sample_shape=z.shape).to(device)\n            z = torch.cat([z, z_split], dim=1)\n            ldj -= self.prior.log_prob(z_split).sum(dim=[1,2,3])\n        return z, ldj\n\n\n\nBuilding a multi-scale flow\nAfter defining the squeeze and split operation, we are finally able to build our own multi-scale flow. Deep normalizing flows such as Glow and Flow++ [2,3] often apply a split operation directly after squeezing. However, with shallow flows, we need to be more thoughtful about where to place the split operation as we need at least a minimum amount of transformations on each variable. Our setup is inspired by the original RealNVP architecture [1] which is shallower than other, more recent state-of-the-art architectures.\nHence, for the MNIST dataset, we will apply the first squeeze operation after two coupling layers, but don’t apply a split operation yet. Because we have only used two coupling layers and each the variable has been only transformed once, a split operation would be too early. We apply two more coupling layers before finally applying a split flow and squeeze again. The last four coupling layers operate on a scale of \\(7\\times 7\\times 8\\). The full flow architecture is shown below.\n\n\n\nNote that while the feature maps inside the coupling layers reduce with the height and width of the input, the increased number of channels is not directly considered. To counteract this, we increase the hidden dimensions for the coupling layers on the squeezed input. The dimensions are often scaled by 2 as this approximately increases the computation cost by 4 canceling with the squeezing operation. However, we will choose the hidden dimensionalities \\(32, 48, 64\\) for the three scales respectively to keep the number of parameters reasonable and show the efficiency of multi-scale architectures.\n\ndef create_multiscale_flow():\n    flow_layers = []\n\n    vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n                                   mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n                                   c_in=1) for i in range(4)]\n    flow_layers += [VariationalDequantization(vardeq_layers)]\n\n    flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, c_hidden=32),\n                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n                                  c_in=1) for i in range(2)]\n    flow_layers += [SqueezeFlow()]\n    for i in range(2):\n        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=4, c_hidden=48),\n                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n                                      c_in=4)]\n    flow_layers += [SplitFlow(),\n                    SqueezeFlow()]\n\n    # Insert your code here!!!\n    # Complete the last scale ccording to provided description(HINT: c_in should be 8)\n    #\n    #\n    #\n    #\n    #-------------------------\n\n    flow_model = ImageFlow(flow_layers).to(device)\n    return flow_model\n\nWe can show the difference in number of parameters below:\n\ndef print_num_params(model):\n    num_params = sum([np.prod(p.shape) for p in model.parameters()])\n    print(\"Number of parameters: {:,}\".format(num_params))\n\nprint_num_params(create_simple_flow(use_vardeq=False))\nprint_num_params(create_simple_flow(use_vardeq=True))\nprint_num_params(create_multiscale_flow())\n\nNumber of parameters: 556,312\nNumber of parameters: 628,388\nNumber of parameters: 534,122\n\n\nAlthough the multi-scale flow has almost 3 times the parameters of the single scale flow, it is not necessarily more computationally expensive than its counterpart. We will compare the runtime in the following experiments as well."
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#analysing-the-flows",
    "href": "labs/08Examples/NF_homework.html#analysing-the-flows",
    "title": "Digital Twins for Physical Systems",
    "section": "Analysing the flows",
    "text": "Analysing the flows\nIn the last part of the notebook, we will train all the models we have implemented above, and try to analyze the effect of the multi-scale architecture and variational dequantization.\n\nTraining flow variants\nBefore we can analyse the flow models, we need to train them first. We provide pre-trained models that contain the validation and test performance, and run-time information. As flow models are computationally expensive, we advice you to rely on those pretrained models for a first run through the notebook.\n\nflow_dict = {\"simple\": {}, \"vardeq\": {}, \"multiscale\": {}}\nflow_dict[\"simple\"][\"model\"], flow_dict[\"simple\"][\"result\"] = train_flow(create_simple_flow(use_vardeq=False), model_name=\"MNISTFlow_simple\")\nflow_dict[\"vardeq\"][\"model\"], flow_dict[\"vardeq\"][\"result\"] = train_flow(create_simple_flow(use_vardeq=True), model_name=\"MNISTFlow_vardeq\")\nflow_dict[\"multiscale\"][\"model\"], flow_dict[\"multiscale\"][\"result\"] = train_flow(create_multiscale_flow(), model_name=\"MNISTFlow_multiscale\")\n\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nWARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tutorial11/MNISTFlow_simple/lightning_logs\n\n\nStart training MNISTFlow_simple\n\n\nNameError: name 'nll' is not defined\n\n\n\nflow_dict[\"simple\"][\"result\"]\n\n{'test': [{'test_bpd': 1.07839834690094}],\n 'val': [{'test_bpd': 1.0798484086990356}],\n 'time': 0.019570968523147,\n 'samp_time': 0.017716965675354003}\n\n\n\n\nDensity modeling and sampling\nFirstly, we can compare the models on their quantitative results. The following table shows all important statistics. The inference time specifies the time needed to determine the probability for a batch of 64 images for each model, and the sampling time the duration it took to sample a batch of 64 images.\n\n%%html\n&lt;!-- Some HTML code to increase font size in the following table --&gt;\n&lt;style&gt;\nth {font-size: 120%;}\ntd {font-size: 120%;}\n&lt;/style&gt;\n\n\n\n\n\n\nimport tabulate\nfrom IPython.display import display, HTML\n\ntable = [[key,\n          \"%4.3f bpd\" % flow_dict[key][\"result\"][\"val\"][0][\"test_bpd\"],\n          \"%4.3f bpd\" % flow_dict[key][\"result\"][\"test\"][0][\"test_bpd\"],\n          \"%2.0f ms\" % (1000 * flow_dict[key][\"result\"][\"time\"]),\n          \"%2.0f ms\" % (1000 * flow_dict[key][\"result\"].get(\"samp_time\", 0)),\n          \"{:,}\".format(sum([np.prod(p.shape) for p in flow_dict[key][\"model\"].parameters()]))]\n         for key in flow_dict]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Model\", \"Validation Bpd\", \"Test Bpd\", \"Inference time\", \"Sampling time\", \"Num Parameters\"])))\n\n\n\n\nModel\nValidation Bpd\nTest Bpd\nInference time\nSampling time\nNum Parameters\n\n\n\n\nsimple\n1.080 bpd\n1.078 bpd\n20 ms\n18 ms\n556,312\n\n\nvardeq\n1.045 bpd\n1.043 bpd\n26 ms\n18 ms\n628,388\n\n\nmultiscale\n1.022 bpd\n1.020 bpd\n23 ms\n15 ms\n1,711,818\n\n\n\n\n\nQuestion1: Does variational dequantization improves bits per dimensions.Why?\nQuestion2: Compare the sample quality of variational and standard dequantization?\n\npl.seed_everything(44)\nsamples = flow_dict[\"vardeq\"][\"model\"].sample(img_shape=[16,1,28,28])\nshow_imgs(samples.cpu())\n\nINFO:lightning_fabric.utilities.seed:Seed set to 44\n\n\n\n\n\n\n\n\n\n\npl.seed_everything(42)\nsamples = flow_dict[\"multiscale\"][\"model\"].sample(img_shape=[16,8,7,7])\nshow_imgs(samples.cpu())\n\nINFO:lightning_fabric.utilities.seed:Seed set to 42\n\n\n\n\n\n\n\n\n\nFrom the few samples, we can see a clear difference between the simple and the multi-scale model. The single-scale model has only learned local, small correlations while the multi-scale model was able to learn full, global relations that form digits. This show-cases another benefit of the multi-scale model. In contrast to VAEs, the outputs are sharp as normalizing flows can naturally model complex, multi-modal distributions while VAEs have the independent decoder output noise. Nevertheless, the samples from this flow are far from perfect as not all samples show true digits.\n\n\nInterpolation in latent space\nAnother popular test for the smoothness of the latent space of generative models is to interpolate between two training examples. As normalizing flows are strictly invertible, we can guarantee that any image is represented in the latent space. We again compare the variational dequantization model with the multi-scale model below.\n\n@torch.no_grad()\ndef interpolate(model, img1, img2, num_steps=8):\n    \"\"\"\n    Inputs:\n        model - object of ImageFlow class that represents the (trained) flow model\n        img1, img2 - Image tensors of shape [1, 28, 28]. Images between which should be interpolated.\n        num_steps - Number of interpolation steps. 8 interpolation steps mean 6 intermediate pictures besides img1 and img2\n    \"\"\"\n    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n    z, _ = model.encode(imgs)\n    alpha = torch.linspace(0, 1, steps=num_steps, device=z.device).view(-1, 1, 1, 1)\n    interpolations = z[0:1] * alpha + z[1:2] * (1 - alpha)\n    interp_imgs = model.sample(interpolations.shape[:1] + imgs.shape[1:], z_init=interpolations)\n    show_imgs(interp_imgs, row_size=8)\n\nexmp_imgs, _ = next(iter(train_loader))\n\n\npl.seed_everything(42)\nfor i in range(2):\n    interpolate(flow_dict[\"vardeq\"][\"model\"], exmp_imgs[2*i], exmp_imgs[2*i+1])\n\nINFO:lightning_fabric.utilities.seed:Seed set to 42\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npl.seed_everything(42)\nfor i in range(2):\n    interpolate(flow_dict[\"multiscale\"][\"model\"], exmp_imgs[2*i], exmp_imgs[2*i+1])\n\nINFO:lightning_fabric.utilities.seed:Seed set to 42\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion3: Compare two interploation examples.\n\n\nVisualization of latents in different levels of multi-scale\nIn the following we will focus more on the multi-scale flow. We want to analyse what information is being stored in the variables split at early layers, and what information for the final variables. For this, we sample 8 images where each of them share the same final latent variables, but differ in the other part of the latent variables. Below we visualize three examples of this:\n\npl.seed_everything(44)\nfor _ in range(3):\n    z_init = flow_dict[\"multiscale\"][\"model\"].prior.sample(sample_shape=[1,8,7,7])\n    z_init = z_init.expand(8, -1, -1, -1)\n    samples = flow_dict[\"multiscale\"][\"model\"].sample(img_shape=z_init.shape, z_init=z_init)\n    show_imgs(samples.cpu())\n\nINFO:lightning_fabric.utilities.seed:Seed set to 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the early split variables indeed have a smaller effect on the image. Still, small differences can be spot when we look carefully at the borders of the digits. For instance, in the middle, the top part of the 3 has different thicknesses for different samples although all of them represent the same coarse structure. This shows that the flow indeed learns to separate the higher-level information in the final variables, while the early split ones contain local noise patterns.\n\n\nVisualizing Dequantization\nAs a final part of this notebook, we will look at the effect of variational dequantization. We have motivated variational dequantization by the issue of sharp edges/boarders being difficult to model, and a flow would rather prefer smooth, prior-like distributions. To check how what noise distribution \\(q(u|x)\\) the flows in the variational dequantization module have learned, we can plot a histogram of output values from the dequantization and variational dequantization module.\n\ndef visualize_dequant_distribution(model : ImageFlow, imgs : torch.Tensor, title:str=None):\n    \"\"\"\n    Inputs:\n        model - The flow of which we want to visualize the dequantization distribution\n        imgs - Example training images of which we want to visualize the dequantization distribution\n    \"\"\"\n    imgs = imgs.to(device)\n    ldj = torch.zeros(imgs.shape[0], dtype=torch.float32).to(device)\n    with torch.no_grad():\n        dequant_vals = []\n        for _ in tqdm(range(8), leave=False):\n            d, _ = model.flows[0](imgs, ldj, reverse=False)\n            dequant_vals.append(d)\n        dequant_vals = torch.cat(dequant_vals, dim=0)\n    dequant_vals = dequant_vals.view(-1).cpu().numpy()\n    sns.set()\n    plt.figure(figsize=(10,3))\n    plt.hist(dequant_vals, bins=256, color=to_rgb(\"C0\")+(0.5,), edgecolor=\"C0\", density=True)\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n\nsample_imgs, _ = next(iter(train_loader))\n\n\nvisualize_dequant_distribution(flow_dict[\"simple\"][\"model\"], sample_imgs, title=\"Dequantization\")\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_dequant_distribution(flow_dict[\"vardeq\"][\"model\"], sample_imgs, title=\"Variational dequantization\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4: Compare two distributions. Which one is smoother?"
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#conclusion",
    "href": "labs/08Examples/NF_homework.html#conclusion",
    "title": "Digital Twins for Physical Systems",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, we have seen how to implement our own normalizing flow, and what difficulties arise if we want to apply them on images. Dequantization is a crucial step in mapping the discrete images into continuous space to prevent underisable delta-peak solutions. While dequantization creates hypercubes with hard border, variational dequantization allows us to fit a flow much better on the data. This allows us to obtain a lower bits per dimension score, while not affecting the sampling speed. The most common flow element, the coupling layer, is simple to implement, and yet effective. Furthermore, multi-scale architectures help to capture the global image context while allowing us to efficiently scale up the flow. Normalizing flows are an interesting alternative to VAEs as they allow an exact likelihood estimate in continuous space, and we have the guarantee that every possible input \\(x\\) has a corresponding latent vector \\(z\\). However, even beyond continuous inputs and images, flows can be applied and allow us to exploit the data structure in latent space, as e.g. on graphs for the task of molecule generation [6]. Recent advances in Neural ODEs allow a flow with infinite number of layers, called Continuous Normalizing Flows, whose potential is yet to fully explore. Overall, normalizing flows are an exciting research area which will continue over the next couple of years.\nAdvanced Topics in Normalizing Flows - 1x1 convolution"
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#introduction",
    "href": "labs/08Examples/NF_homework.html#introduction",
    "title": "Digital Twins for Physical Systems",
    "section": "Introduction",
    "text": "Introduction\nThe Glow, a flow-based generative model extends the previous invertible generative models, NICE and RealNVP, and simplifies the architecture by replacing the reverse permutation operation on the channel ordering with Invertible 1x1 Convolutions. Glow is famous for being the one of the first flow-based models that works on high resolution images and enables manipulation in latent space. Let’s have a look at the interactive demonstration from OpenAI.\n\n\n\nGlow consists of a series of steps of flow. Each step of flow comprises Actnorm followed by an Invertible 1×1 Convolution, and finally a Coupling Layer.\n\n\n\nActnorm performs an affine transformation with a scale and bias parameter per channel, similar to that of batch normalization, but works on mini-batch size 1. The statistics (mean and std), however, are only calculated once to initialize the scale and bias parameters.\nInvertible 1×1 Convolution with equal number of input and output channels is a generalization of any permutation of the channel ordering. Recall the operation between layers of the RealNVP flow, the ordering of channels is switched so that all the data dimensions have a chance to be mixed. 1x1 convolution is proposed to replace this fixed permutation with a learned invertible operation.\nCoupling Layer is a powerful reversible transformation where the forward function, the reverse function and the logdeterminant are computationally efficient. The design is the same as in RealNVP.\n\n\n\nIn this tutorial, we will be focusing on the implementation of invertible 1x1 convolution layer."
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#invertible-1x1-convolution",
    "href": "labs/08Examples/NF_homework.html#invertible-1x1-convolution",
    "title": "Digital Twins for Physical Systems",
    "section": "Invertible 1x1 convolution",
    "text": "Invertible 1x1 convolution\nGiven an input of shape \\(H\\times W\\times C\\) applied with a 1x1 convolution with \\(C\\) filters, meaning the output tensor shape is also going to be \\(H\\times W\\times C\\). Thus, each layer has a set of weights \\(W\\) with \\(C\\times C\\) values. The forward operation acts just like a typical convolution, while the inverse operation can be computed by simply applying a convolution with \\(W^{-1}\\) weights.\n\n\n\nEnough descriptions! Now let’s take a look at the code.\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass InvConv2d(nn.Module):\n    def __init__(self, in_channel):\n        super().__init__()\n\n        weight = torch.randn(in_channel, in_channel)\n        # use the Q matrix from QR decomposition as the initial weight to make sure it's invertible\n        q, _ = torch.qr(weight)\n        weight = q.unsqueeze(2).unsqueeze(3)\n        self.weight = nn.Parameter(weight)\n\n    def forward(self, input, logdet, reverse=False):\n        _, _, height, width = input.shape\n\n        # You can also use torch.slogdet(self.weight)[1] to summarize the operations below\\n\",\n        dlogdet = (\n            height * width * torch.log(torch.abs(torch.det(self.weight.squeeze())))\n        )\n\n        if not reverse:\n            out = F.conv2d(input, self.weight)\n            logdet = logdet + dlogdet\n\n        else:\n            out = F.conv2d(input, self.weight.squeeze().inverse().unsqueeze(2).unsqueeze(3))\n            logdet = logdet - dlogdet\n\n        return out, logdet\n\nNote that to calculate the determinant of \\(W\\) could be computationally expensive, thus there’s also an implementation which utilizes LU decomposition to speed up, as suggested in the Glow paper.\nThe idea is to parameterizing \\(W\\) directly in its LU decomposition:\n\\[\nW = PL(U + \\text{diag}(s)),\n\\]\nwhere \\(P\\) is a permutation matrix, \\(L\\) is a lower triangular matrix with ones on the diagonal, \\(U\\) is an upper triangular matrix with zeros on the diagonal, and \\(s\\) is a vector.\nThe log-determinant is then simply:\n\\[\n\\log | \\det(W)| = \\sum \\left(\\log |s|\\right)\n\\]\nPlease check out the link above for the implementation.\n\nA small pitfall\nAs you might notice, there’s an inverse operation for the weight \\(W\\) involved when the 1x1 convolution is forwarding reversely. As a result, an error can occur when the weight \\(W\\) is not invertible, even though it seldom happens.\nTo our best knowledge, there’s no elegant solution to address this, but an easy way to workaround: If this happens unfortunately during the training, one can try to restart from the recent checkpoint.\n\n\nA complete flow block\nNow we have the Invertible 1x1 Convolution. Together with the aforementioned Actnorm and Coupling Layer, we are ready to try out the power of the Glow by plugging the block into the model we had in the NFs tutorial!\n\nclass ActNorm(nn.Module):\n    def __init__(self, in_channel):\n        super().__init__()\n\n        self.loc = nn.Parameter(torch.zeros(1, in_channel, 1, 1))\n        self.log_scale = nn.Parameter(torch.zeros(1, in_channel, 1, 1))\n        self.register_buffer(\"initialized\", torch.tensor(0, dtype=torch.uint8))\n\n    def initialize(self, input):\n\n        with torch.no_grad():\n            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)\n\n            # Insert your code here!!!\n            # Implement mean and std\n            # (HINT: Flatten, unsqueeze 3 times and permute channels. Do it for both std and mean)\n            #\n            #\n            #\n            #\n            #\n            #\n            #\n\n            #\n            #\n            #\n            #\n            #\n            #\n            #\n            #-------------------------\n            self.loc.data.copy_(-mean)\n            self.log_scale.data.copy_(-std.clamp_(min=1e-6).log())\n\n    def forward(self, input, logdet, reverse=False):\n        _, _, height, width = input.shape\n\n        if self.initialized.item() == 0:\n            self.initialize(input)\n            self.initialized.fill_(1)\n\n        dlogdet = height * width * torch.sum(self.log_scale)\n\n        if not reverse:\n            logdet += dlogdet\n            return self.log_scale.exp() * (input + self.loc), logdet\n\n        else:\n            dlogdet *= -1\n            logdet += dlogdet\n            return input / self.log_scale.exp() - self.loc, logdet"
  },
  {
    "objectID": "labs/08Examples/NF_homework.html#conclusion-1",
    "href": "labs/08Examples/NF_homework.html#conclusion-1",
    "title": "Digital Twins for Physical Systems",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve learned an advanced flow-based layer from the Glow model, an Invertible 1x1 convolution, which is adapted from the typical 1x1 convolution layers.\n\nReferences\n\nGlow: Generative Flow with Invertible 1x1 Convolutions\nGlow: Better Reversible Generative Models\nhttps://github.com/rosinality/glow-pytorch\nMaterials from NTU Speech Lab"
  },
  {
    "objectID": "labs/w3-lab02-Diff-Prog.html",
    "href": "labs/w3-lab02-Diff-Prog.html",
    "title": "Differentiable Programming",
    "section": "",
    "text": "The notebooks can be downloaded from here\n\nIntro Differentiable Programming - diff_prog.ipynb\nAutograd tutorial - autograd_tut.ipynb\n\nPlease rerun both these labs. Take careful note of how to define a custom gradient. This will become relevant in the course when we compose learned layers with PDE solvers. The PDE solver gradients can be calculated with brute force with AD or as described in the custom gradient section you can define a custom routine to calculate the gradient i.e efficiently with the adjoint state method.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/w5-lab-1dkalman.html",
    "href": "labs/w5-lab-1dkalman.html",
    "title": "1-D Kalman Filter",
    "section": "",
    "text": "You will need to download the folder “kf_book” from here Kalman-and-Bayesian-Filters-in-Python if using Google collab please upload that folder as a .zip to your collab workspace then run the first lines in the notebook to unzip it.\nPlease follow instructions in the notebook and please send results to TA in .pdf format.\nPlease attempt to first solve the exercise by yourself then feel welcome to look at the answers in 04-One-Dimensional-Kalman-Filters\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html",
    "href": "labs/02Examples/ad/autograd_tut.html",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "References:\n\nR. Grosse’ NN and ML course: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/\nBackpropagation notes from Stanford’s CS231n: http://cs231n.github.io/optimization-2/\nAutograd Github Repository (contains a tutorial and examples): https://github.com/HIPS/autograd\n\n\n\n\nSymbolic differentiation: automatic manipulation of mathematical expressions to get derivatives\n\nTakes a math expression and returns a math expression: \\(f(x) = x^2 \\rightarrow \\frac{df(x)}{dx} = 2x\\)\nUsed in Mathematica, Maple, Sympy, etc.\n\nNumeric differentiation: Approximating derivatives by finite differences: \\[\n\\frac{\\partial}{\\partial x_i} f(x_1, \\dots, x_N) = \\lim_{h \\to 0} \\frac{f(x_1, \\dots, x_i + h, \\dots, x_N) - f(x_1, \\dots, x_i - h, \\dots, x_N)}{2h}\n\\]\nAutomatic differentiation: Takes code that computes a function and returns code that computes the derivative of that function.\n\nReverse Mode AD: A method to get exact derivatives efficiently, by storing information as you go forward that you can reuse as you go backwards\n“The goal isn’t to obtain closed-form solutions, but to be able to wirte a program that efficiently computes the derivatives.”\nAutograd, Torch Autograd\n\n\n\n\n\nIn machine learning, we have functions that have large fan-in, e.g. a neural net can have millions of parameters, that all squeeze down to one scalar that tells you how well it predicts something. eg. cats…\n\n\n\nCreate a “tape” data structure that tracks the operations performed in computing a function\nOverload primitives to:\n\nAdd themselves to the tape when called\nCompute gradients with respect to their local inputs\n\nForward pass computes the function, and adds operations to the tape\nReverse pass accumulates the local gradients using the chain rule\nThis is efficient for graphs with large fan-in, like most loss functions in ML\n\n\n\n\n\n\nAutograd is a Python package for automatic differentiation.\nTo install Autograd: pip install autograd\nThere are a lot of great examples provided with the source code\n\n\n\nFrom the Autograd Github repository:\n\nAutograd can automatically differentiate native Python and Numpy code.\nIt can handle a large subset of Python’s features, including loops, conditional statements (if/else), recursion and closures\nIt can also compute higher-order derivatives\nIt uses reverse-mode differentiation (a.k.a. backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments.\n\n\n\n\n\n\nimport autograd.numpy as np # Import thinly-wrapped numpy\nfrom autograd import grad   # Basicallly the only function you need\n\n\n# Define a function as usual, using Python and Numpy\ndef tanh(x):\n    y = np.exp(-x)\n    return (1.0 - y) / (1.0 + y)\n\n# Create a *function* that computes the gradient of tanh\ngrad_tanh = grad(tanh)\n\n# Evaluate the gradient at x = 1.0\nprint(grad_tanh(1.0))\n\n# Compare to numeric gradient computed using finite differences\nprint((tanh(1.0001) - tanh(0.9999)) / 0.0002)\n\n0.39322386648296376\n0.39322386636453377\n\n\n\n\n\nIn this example, we will see how a complicated computation can be written as a composition of simpler functions, and how this provides a scalable strategy for computing gradients using the chain rule.\nWe want to write a function to compute the gradient of the sigmoid function: \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\] We can write \\(\\sigma(x)\\) as a composition of several elementary functions, as \\(\\sigma(x) = s(c(b(a(x))))\\), where\n\\[\n\\begin{align}\na(x) &= -x \\\\\nb(a) & = e^a \\\\\nc(b) & = 1 + b \\\\\ns(c) = \\frac{1}{c}.\n\\end{align}\n\\]\nHere, we have “staged” the computation such that it contains several intermediate variables, each of which are basic expressions for which we can easily compute the local gradients.\nThe computation graph for this expression is\n\\[ x \\longrightarrow a \\longrightarrow b \\longrightarrow c \\longrightarrow  s.\\]\nThe input to this function is \\(x\\), and the output is represented by node \\(s\\). We want to compute the gradient of \\(s\\) with respect to \\(x\\), \\[\\frac{\\partial s}{\\partial x}.\\] In order to make use of our intermediate computations, we just use the chain rule, \\[\n\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x},\n\\] where we clearly observe the backward propagation of the gradients, from \\(s\\) to \\(a.\\)\n\nGiven a vector-to-scalar function, \\(\\mathbb{R}^D \\to \\mathbb{R}\\), composed of a set of primitive functions \\(\\mathbb{R}^M \\to \\mathbb{R}^N\\) (for various \\(M\\), \\(N\\)), the gradient of the composition is given by the product of the gradients of the primitive functions, according to the chain rule. But the chain rule doesn’t prescribe the order in which to multiply the gradients. From the perspective of computational complexity, the order makes all the difference.\n\n\ndef grad_sigmoid_manual(x):\n    \"\"\"Implements the gradient of the logistic sigmoid function \n    $\\sigma(x) = 1 / (1 + e^{-x})$ using staged computation\n    \"\"\"\n    # Forward pass, keeping track of intermediate values for use in the \n    # backward pass\n    a = -x         # -x in denominator\n    b = np.exp(a)  # e^{-x} in denominator\n    c = 1 + b      # 1 + e^{-x} in denominator\n    s = 1.0 / c    # Final result: 1.0 / (1 + e^{-x})\n    \n    # Backward pass (differentiate basic functions)\n    dsdc = (-1.0 / (c**2))\n    dsdb = dsdc * 1\n    dsda = dsdb * np.exp(a)\n    dsdx = dsda * (-1)\n    \n    return dsdx\n\n\ndef sigmoid(x):\n    y = 1.0 / (1.0 + np.exp(-x))\n    return y\n\n# Instead of writing grad_sigmoid_manual manually, we can use \n# Autograd's grad function:\ngrad_sigmoid_automatic = grad(sigmoid)\n\n# Compare the results of manual and automatic gradient functions:\nprint(grad_sigmoid_automatic(2.0))\nprint(grad_sigmoid_manual(2.0))\n\n0.1049935854035065\n0.1049935854035065\n\n\n\n\n\nThere are several functions that compute gradients, which have different signatures\n\ngrad(fun, argnum=0)\n\nReturns a function which computes the gradient of fun with respect to positional argument number argnum. The returned function takes the same arguments as fun, but returns the gradient instead. The function fun should be scalar-valued. The gradient has the same type as the argument.\n\ngrad_named(fun, argname)\n\nTakes gradients with respect to a named argument.\n\nmultigrad(fun, argnums=[0])\n\nTakes gradients wrt multiple arguments simultaneously.\n\nmultigrad_dict(fun)\n\nTakes gradients with respect to all arguments simultaneously, and returns a dict mapping argname to gradval\n\n\n\n\n\nThe implementation of Autograd is simple, readable, and extensible!\nOne thing you can do is define custom gradients for your own functions. There are several reasons you might want to do this, including:\n\nSpeed: You may know a faster way to compute the gradient for a specific function.\nNumerical Stability\nWhen your code depends on external library calls\n\nThe @primitive decorator wraps a function so that its gradient can be specified manually and its invocation can be recorded.\n\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nfrom autograd import grad\nfrom autograd.extend import primitive, defvjp\n\n# From the Autograd examples:\n# @primitive tells autograd not to look inside this function, but instead\n# to treat it as a black box, whose gradient might be specified later.\n@primitive\ndef logsumexp(x):\n    \"\"\"Numerically stable log(sum(exp(x)))\"\"\"\n    max_x = np.max(x)\n    return max_x + np.log(np.sum(np.exp(x - max_x)))\n\n# Next, we write a function that specifies the gradient with a closure.\ndef make_grad_logsumexp(ans, x):\n    # If you want to be able to take higher-order derivatives, then all the\n    # code inside this function must be itself differentiable by autograd.\n    def gradient_product(g):\n        return np.full(x.shape, g) * np.exp(x - np.full(x.shape, ans))\n    return gradient_product\n\n# Now we tell autograd that logsumexmp has a gradient-making function.\ndefvjp(logsumexp, make_grad_logsumexp)\n\n\n# Now we can use logsumexp() inside a larger function that we want to differentiate.\ndef example_func(y):\n    z = y**2\n    lse = logsumexp(z)\n    return np.sum(lse)\n\ngrad_of_example = grad(example_func)\nprint(\"Gradient: \", grad_of_example(npr.randn(10)))\n\n# Check the gradients numerically, just to be safe.\n# Fails if a mismatch occurs\nfrom autograd.test_util import check_grads\ncheck_grads(example_func, modes=['rev'], order=2)(npr.randn(10))\n\nGradient:  [ 0.00445388 -0.06760237 -0.0030408  -0.00398812  0.13980291 -0.65753857\n -0.57149671 -0.0969613  -0.27697817  1.59207253]"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#recall-approaches-for-computing-derivatives",
    "href": "labs/02Examples/ad/autograd_tut.html#recall-approaches-for-computing-derivatives",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "Symbolic differentiation: automatic manipulation of mathematical expressions to get derivatives\n\nTakes a math expression and returns a math expression: \\(f(x) = x^2 \\rightarrow \\frac{df(x)}{dx} = 2x\\)\nUsed in Mathematica, Maple, Sympy, etc.\n\nNumeric differentiation: Approximating derivatives by finite differences: \\[\n\\frac{\\partial}{\\partial x_i} f(x_1, \\dots, x_N) = \\lim_{h \\to 0} \\frac{f(x_1, \\dots, x_i + h, \\dots, x_N) - f(x_1, \\dots, x_i - h, \\dots, x_N)}{2h}\n\\]\nAutomatic differentiation: Takes code that computes a function and returns code that computes the derivative of that function.\n\nReverse Mode AD: A method to get exact derivatives efficiently, by storing information as you go forward that you can reuse as you go backwards\n“The goal isn’t to obtain closed-form solutions, but to be able to wirte a program that efficiently computes the derivatives.”\nAutograd, Torch Autograd"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#reverse-mode-automatic-differentiation",
    "href": "labs/02Examples/ad/autograd_tut.html#reverse-mode-automatic-differentiation",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "In machine learning, we have functions that have large fan-in, e.g. a neural net can have millions of parameters, that all squeeze down to one scalar that tells you how well it predicts something. eg. cats…\n\n\n\nCreate a “tape” data structure that tracks the operations performed in computing a function\nOverload primitives to:\n\nAdd themselves to the tape when called\nCompute gradients with respect to their local inputs\n\nForward pass computes the function, and adds operations to the tape\nReverse pass accumulates the local gradients using the chain rule\nThis is efficient for graphs with large fan-in, like most loss functions in ML"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#autograd",
    "href": "labs/02Examples/ad/autograd_tut.html#autograd",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "Autograd is a Python package for automatic differentiation.\nTo install Autograd: pip install autograd\nThere are a lot of great examples provided with the source code\n\n\n\nFrom the Autograd Github repository:\n\nAutograd can automatically differentiate native Python and Numpy code.\nIt can handle a large subset of Python’s features, including loops, conditional statements (if/else), recursion and closures\nIt can also compute higher-order derivatives\nIt uses reverse-mode differentiation (a.k.a. backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments."
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#autograd-basic-usage",
    "href": "labs/02Examples/ad/autograd_tut.html#autograd-basic-usage",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "import autograd.numpy as np # Import thinly-wrapped numpy\nfrom autograd import grad   # Basicallly the only function you need\n\n\n# Define a function as usual, using Python and Numpy\ndef tanh(x):\n    y = np.exp(-x)\n    return (1.0 - y) / (1.0 + y)\n\n# Create a *function* that computes the gradient of tanh\ngrad_tanh = grad(tanh)\n\n# Evaluate the gradient at x = 1.0\nprint(grad_tanh(1.0))\n\n# Compare to numeric gradient computed using finite differences\nprint((tanh(1.0001) - tanh(0.9999)) / 0.0002)\n\n0.39322386648296376\n0.39322386636453377"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#autograd-vs-manual-gradients-via-staged-computation",
    "href": "labs/02Examples/ad/autograd_tut.html#autograd-vs-manual-gradients-via-staged-computation",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "In this example, we will see how a complicated computation can be written as a composition of simpler functions, and how this provides a scalable strategy for computing gradients using the chain rule.\nWe want to write a function to compute the gradient of the sigmoid function: \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\] We can write \\(\\sigma(x)\\) as a composition of several elementary functions, as \\(\\sigma(x) = s(c(b(a(x))))\\), where\n\\[\n\\begin{align}\na(x) &= -x \\\\\nb(a) & = e^a \\\\\nc(b) & = 1 + b \\\\\ns(c) = \\frac{1}{c}.\n\\end{align}\n\\]\nHere, we have “staged” the computation such that it contains several intermediate variables, each of which are basic expressions for which we can easily compute the local gradients.\nThe computation graph for this expression is\n\\[ x \\longrightarrow a \\longrightarrow b \\longrightarrow c \\longrightarrow  s.\\]\nThe input to this function is \\(x\\), and the output is represented by node \\(s\\). We want to compute the gradient of \\(s\\) with respect to \\(x\\), \\[\\frac{\\partial s}{\\partial x}.\\] In order to make use of our intermediate computations, we just use the chain rule, \\[\n\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x},\n\\] where we clearly observe the backward propagation of the gradients, from \\(s\\) to \\(a.\\)\n\nGiven a vector-to-scalar function, \\(\\mathbb{R}^D \\to \\mathbb{R}\\), composed of a set of primitive functions \\(\\mathbb{R}^M \\to \\mathbb{R}^N\\) (for various \\(M\\), \\(N\\)), the gradient of the composition is given by the product of the gradients of the primitive functions, according to the chain rule. But the chain rule doesn’t prescribe the order in which to multiply the gradients. From the perspective of computational complexity, the order makes all the difference.\n\n\ndef grad_sigmoid_manual(x):\n    \"\"\"Implements the gradient of the logistic sigmoid function \n    $\\sigma(x) = 1 / (1 + e^{-x})$ using staged computation\n    \"\"\"\n    # Forward pass, keeping track of intermediate values for use in the \n    # backward pass\n    a = -x         # -x in denominator\n    b = np.exp(a)  # e^{-x} in denominator\n    c = 1 + b      # 1 + e^{-x} in denominator\n    s = 1.0 / c    # Final result: 1.0 / (1 + e^{-x})\n    \n    # Backward pass (differentiate basic functions)\n    dsdc = (-1.0 / (c**2))\n    dsdb = dsdc * 1\n    dsda = dsdb * np.exp(a)\n    dsdx = dsda * (-1)\n    \n    return dsdx\n\n\ndef sigmoid(x):\n    y = 1.0 / (1.0 + np.exp(-x))\n    return y\n\n# Instead of writing grad_sigmoid_manual manually, we can use \n# Autograd's grad function:\ngrad_sigmoid_automatic = grad(sigmoid)\n\n# Compare the results of manual and automatic gradient functions:\nprint(grad_sigmoid_automatic(2.0))\nprint(grad_sigmoid_manual(2.0))\n\n0.1049935854035065\n0.1049935854035065"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#gradient-functions",
    "href": "labs/02Examples/ad/autograd_tut.html#gradient-functions",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "There are several functions that compute gradients, which have different signatures\n\ngrad(fun, argnum=0)\n\nReturns a function which computes the gradient of fun with respect to positional argument number argnum. The returned function takes the same arguments as fun, but returns the gradient instead. The function fun should be scalar-valued. The gradient has the same type as the argument.\n\ngrad_named(fun, argname)\n\nTakes gradients with respect to a named argument.\n\nmultigrad(fun, argnums=[0])\n\nTakes gradients wrt multiple arguments simultaneously.\n\nmultigrad_dict(fun)\n\nTakes gradients with respect to all arguments simultaneously, and returns a dict mapping argname to gradval"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#modularity-implementing-custom-gradients",
    "href": "labs/02Examples/ad/autograd_tut.html#modularity-implementing-custom-gradients",
    "title": "Autograd Tutorial",
    "section": "",
    "text": "The implementation of Autograd is simple, readable, and extensible!\nOne thing you can do is define custom gradients for your own functions. There are several reasons you might want to do this, including:\n\nSpeed: You may know a faster way to compute the gradient for a specific function.\nNumerical Stability\nWhen your code depends on external library calls\n\nThe @primitive decorator wraps a function so that its gradient can be specified manually and its invocation can be recorded.\n\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nfrom autograd import grad\nfrom autograd.extend import primitive, defvjp\n\n# From the Autograd examples:\n# @primitive tells autograd not to look inside this function, but instead\n# to treat it as a black box, whose gradient might be specified later.\n@primitive\ndef logsumexp(x):\n    \"\"\"Numerically stable log(sum(exp(x)))\"\"\"\n    max_x = np.max(x)\n    return max_x + np.log(np.sum(np.exp(x - max_x)))\n\n# Next, we write a function that specifies the gradient with a closure.\ndef make_grad_logsumexp(ans, x):\n    # If you want to be able to take higher-order derivatives, then all the\n    # code inside this function must be itself differentiable by autograd.\n    def gradient_product(g):\n        return np.full(x.shape, g) * np.exp(x - np.full(x.shape, ans))\n    return gradient_product\n\n# Now we tell autograd that logsumexmp has a gradient-making function.\ndefvjp(logsumexp, make_grad_logsumexp)\n\n\n# Now we can use logsumexp() inside a larger function that we want to differentiate.\ndef example_func(y):\n    z = y**2\n    lse = logsumexp(z)\n    return np.sum(lse)\n\ngrad_of_example = grad(example_func)\nprint(\"Gradient: \", grad_of_example(npr.randn(10)))\n\n# Check the gradients numerically, just to be safe.\n# Fails if a mismatch occurs\nfrom autograd.test_util import check_grads\ncheck_grads(example_func, modes=['rev'], order=2)(npr.randn(10))\n\nGradient:  [ 0.00445388 -0.06760237 -0.0030408  -0.00398812  0.13980291 -0.65753857\n -0.57149671 -0.0969613  -0.27697817  1.59207253]"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#linear-regression",
    "href": "labs/02Examples/ad/autograd_tut.html#linear-regression",
    "title": "Autograd Tutorial",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nReview\nWe are given a set of data points \\(\\{ (x_1, t_1), (x_2, t_2), \\dots, (x_N, t_N) \\}\\), where each point \\((x_i, t_i)\\) consists of an input value \\(x_i\\) and a target value \\(t_i\\).\nThe model we use is: \\[\ny_i = wx_i + b\n\\]\nWe want each predicted value \\(y_i\\) to be close to the ground truth value \\(t_i\\). In linear regression, we use squared error to quantify the disagreement between \\(y_i\\) and \\(t_i\\). The loss function for a single example is: \\[\n\\mathcal{L}(y_i,t_i) = \\frac{1}{2} (y_i - t_i)^2\n\\]\nThe cost function is the loss averaged over all the training examples: \\[\n\\mathcal{E}(w,b) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(y_i, t_i) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left(wx_i + b - t_i \\right)^2\n\\]\n\nimport autograd.numpy as np # Import wrapped NumPy from Autograd\nimport autograd.numpy.random as npr # For convenient access to numpy.random\nfrom autograd import grad # To compute gradients\n\nimport matplotlib.pyplot as plt # For plotting\n\n%matplotlib inline"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#generate-synthetic-data",
    "href": "labs/02Examples/ad/autograd_tut.html#generate-synthetic-data",
    "title": "Autograd Tutorial",
    "section": "Generate Synthetic Data",
    "text": "Generate Synthetic Data\nWe generate a synthetic dataset \\(\\{ (x_i, t_i) \\}\\) by first taking the \\(x_i\\) to be linearly spaced in the range \\([0, 10]\\) and generating the corresponding value of \\(t_i\\) using the following equation (where \\(w = 4\\) and \\(b=10\\)): \\[\nt_i = 4 x_i + 10 + \\epsilon\n\\]\nHere, \\(\\epsilon \\sim \\mathcal{N}(0, 2),\\) that is, \\(\\epsilon\\) is drawn from a Gaussian distribution with mean 0 and variance 2. This introduces some random fluctuation in the data, to mimic real data that has an underlying regularity, but for which individual observations are corrupted by random noise.\n\n# In our synthetic data, we have w = 4 and b = 10\nN = 100 # Number of training data points\nx = np.linspace(0, 10, N)\nt = 4 * x + 10 + npr.normal(0, 2, x.shape[0])\nplt.plot(x, t, 'r.')\n\n\n\n\n\n\n\n\n\n# Initialize random parameters\nw = npr.normal(0, 1)\nb = npr.normal(0, 1)\nparams = { 'w': w, 'b': b } # One option: aggregate parameters in a dictionary\n\ndef cost(params):\n    y = params['w'] * x + params['b']\n    return (1 / N) * np.sum(0.5 * np.square(y - t))\n\n# Find the gradient of the cost function using Autograd\ngrad_cost = grad(cost) \n\nnum_epochs = 1000  # Number of epochs of training\nalpha = 0.01       # Learning rate\n\nfor i in range(num_epochs):\n    # Evaluate the gradient of the current parameters stored in params\n    cost_params = grad_cost(params)\n    \n    # Update parameters w and b\n    params['w'] = params['w'] - alpha * cost_params['w']\n    params['b'] = params['b'] - alpha * cost_params['b']\n\nprint(params)\n\n{'w': 4.084961144270687, 'b': 9.264915086528749}\n\n\n\n# Plot the training data, together with the line defined by y = wx + b,\n# where w and b are our final learned parameters\nplt.plot(x, t, 'r.')\nplt.plot([0, 10], [params['b'], params['w'] * 10 + params['b']], 'b-')\nplt.show()"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#linear-regression-with-a-feature-mapping",
    "href": "labs/02Examples/ad/autograd_tut.html#linear-regression-with-a-feature-mapping",
    "title": "Autograd Tutorial",
    "section": "Linear Regression with a Feature Mapping",
    "text": "Linear Regression with a Feature Mapping\nIn this example we will fit a polynomial using linear regression with a polynomial feature mapping. The target function is\n\\[\nt = x^4 - 10 x^2 + 10 x + \\epsilon,\n\\]\nwhere \\(\\epsilon \\sim \\mathcal{N}(0, 4).\\)\nThis is an example of a generalized linear model, in which we perform a fixed nonlinear transformation of the inputs \\(\\mathbf{x} = (x_1, x_2, \\dots, x_D)\\), and the model is still linear in the parameters. We can define a set of feature mappings (also called feature functions or basis functions) \\(\\phi\\) to implement the fixed transformations.\nIn this case, we have \\(x \\in \\mathbb{R}\\), and we define the feature mapping: \\[\n\\mathbf{\\phi}(x) = \\begin{pmatrix}\\phi_1(x) \\\\ \\phi_2(x) \\\\ \\phi_3(x) \\\\ \\phi_4(x) \\end{pmatrix} = \\begin{pmatrix}1\\\\x\\\\x^2\\\\x^3\\end{pmatrix}\n\\]\n\n# Generate synthetic data\nN = 100 # Number of data points\nx = np.linspace(-3, 3, N) # Generate N values linearly-spaced between -3 and 3\nt = x ** 4 - 10 * x ** 2 + 10 * x + npr.normal(0, 4, x.shape[0]) # Generate corresponding targets\nplt.plot(x, t, 'r.') # Plot data points\n\n\n\n\n\n\n\n\n\nM = 4 # Degree of polynomial to fit to the data (this is a hyperparameter)\nfeature_matrix = np.array([[item ** i for i in range(M+1)] for item in x]) # Construct a feature matrix \nW = npr.randn(feature_matrix.shape[-1])\n\ndef cost(W):\n    y = np.dot(feature_matrix, W)\n    return (1.0 / N) * np.sum(0.5 * np.square(y - t))\n\n# Compute the gradient of the cost function using Autograd\ncost_grad = grad(cost)\n\nnum_epochs = 10000\nlearning_rate = 0.001\n\n# Manually implement gradient descent\nfor i in range(num_epochs):\n    W = W - learning_rate * cost_grad(W)\n\n# Print the final learned parameters.\nprint(W)\n\n[  0.0545782   10.42891793 -10.10707337  -0.03126282   1.02186303]\n\n\n\n# Plot the original training data again, together with the polynomial we fit\nplt.plot(x, t, 'r.')\nplt.plot(x, np.dot(feature_matrix, W), 'b-')\nplt.show()"
  },
  {
    "objectID": "labs/02Examples/ad/autograd_tut.html#neural-net-regression",
    "href": "labs/02Examples/ad/autograd_tut.html#neural-net-regression",
    "title": "Autograd Tutorial",
    "section": "Neural Net Regression",
    "text": "Neural Net Regression\nIn this example we will implement a (nonlinear) regression model using a neural network.\nTo implement and train a neural net using Autograd, you only have to define the forward pass of the network and the loss function you wish to use; you do not need to implement the backward pass of the network. When you take the gradient of the loss function using grad, Autograd automatically computes the backward pass. It essentially executes the backpropagation algorithm implicitly.\n\nimport matplotlib.pyplot as plt\n\nimport autograd.numpy as np\nimport autograd.numpy.random as npr\nfrom autograd import grad\nfrom autograd.misc import flatten #, flatten_func\n\nfrom autograd.misc.optimizers import sgd\n\n%matplotlib inline\n\n\nAutograd Implementation of Stochastic Gradient Descent (with momentum)\ndef sgd(grad, init_params, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n    \"\"\"Stochastic gradient descent with momentum.\n    grad() must have signature grad(x, i), where i is the iteration number.\"\"\"\n    flattened_grad, unflatten, x = flatten_func(grad, init_params)\n    \n    velocity = np.zeros(len(x))\n    for i in range(num_iters):\n        g = flattened_grad(x, i)\n        if callback:\n            callback(unflatten(x), i, unflatten(g))\n        velocity = mass * velocity - (1.0 - mass) * g\n        x = x + step_size * velocity\n    return unflatten(x)\nThe next example shows how to use the sgd function.\n\n# Generate synthetic data\nx = np.linspace(-5, 5, 1000)\nt = x ** 3 - 20 * x + 10 + npr.normal(0, 4, x.shape[0])\nplt.plot(x, t, 'r.')\n\n\n\n\n\n\n\n\n\ninputs = x.reshape(x.shape[-1],1)\nW1 = npr.randn(1,4)\nb1 = npr.randn(4)\nW2 = npr.randn(4,4)\nb2 = npr.randn(4)\nW3 = npr.randn(4,1)\nb3 = npr.randn(1)\n\nparams = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3 }\n\ndef relu(x):\n    return np.maximum(0, x)\n\n#nonlinearity = np.tanh\nnonlinearity = relu\n\ndef predict(params, inputs):\n    h1 = nonlinearity(np.dot(inputs, params['W1']) + params['b1'])\n    h2 = nonlinearity(np.dot(h1, params['W2']) + params['b2'])\n    output = np.dot(h2, params['W3']) + params['b3']\n    return output\n\ndef loss(params, i):\n    output = predict(params, inputs)\n    return (1.0 / inputs.shape[0]) * np.sum(0.5 * np.square(output.reshape(output.shape[0]) - t))\n\nprint(loss(params, 0))\n\noptimized_params = sgd(grad(loss), params, step_size=0.01, num_iters=5000)\nprint(optimized_params)\nprint(loss(optimized_params, 0))\n\nfinal_y = predict(optimized_params, inputs)\nplt.plot(x, t, 'r.')\nplt.plot(x, final_y, 'b-')\nplt.show()\n\n307.27526955036535\n{'W1': array([[-2.94559997,  0.16121652, -1.30047875, -1.22974889]]), 'b1': array([-5.28166397, -0.82528464,  2.41753918,  6.14589224]), 'W2': array([[-5.05168877e-01, -2.36611718e+00, -3.27880077e+00,\n         2.75007753e+00],\n       [-1.92904027e-01, -2.37367667e-01, -4.65899580e-01,\n        -1.92235478e+00],\n       [ 4.09549644e-01, -2.22665262e+00,  1.40462107e+00,\n         2.85735187e-03],\n       [-3.72648981e+00,  2.46773804e+00,  1.86232133e+00,\n        -1.55498882e+00]]), 'b2': array([ 5.30739445,  0.42395663, -4.68675653, -2.42712697]), 'W3': array([[ 6.08166514],\n       [-3.52731677],\n       [ 4.50279179],\n       [-3.36406041]]), 'b3': array([0.11658614])}\n8.641052001438881\n\n\n\n\n\n\n\n\n\n\n# A plot of the result of this model using tanh activations\nplt.plot(x, final_y, 'b-')\nplt.show()\n\n\n\n\n\n\n\n\n\n# A plot of the result of this model using ReLU activations\nplt.plot(x, final_y, 'b-')\nplt.show()"
  },
  {
    "objectID": "labs/02Examples/pytorch/diff_prog.html",
    "href": "labs/02Examples/pytorch/diff_prog.html",
    "title": "Differentiable Programming 101",
    "section": "",
    "text": "Differentiable Programming 101\nWe study some initial examples of\n\nnumerical differentiation\nsymbolic differentiation\nautomatic differentiation\n\n\n\nNumerical Differentiation\nConsider the sine function and its derivative,\n\\[ f(x) = \\sin(x), \\quad f'(x)=\\cos (x) \\]\nevaluated at the point \\(x = 0.1.\\)\n\nimport numpy as np\nf = lambda x: np.sin(x)\nx0 = 0.1\nexact = np.cos(x0)\nprint(\"True derivative:\", exact)\nprint(\"Forward Difference\\tError\\t\\t\\tCentral Difference\\tError\\n\")\nfor i in range(10):\n    h = 1/(10**i)\n    f1 = (f(x0+h)-f(x0))/h\n    f2 = (f(x0+h)-f(x0-h))/(2*h)\n    e1 = np.abs(f1 - exact)\n    e2 = np.abs(f2 - exact)\n    print('%.5e\\t\\t%.5e\\t\\t%.5e\\t\\t%.5e'%(f1,e1,f2,e2))\n\nTrue derivative: 0.9950041652780258\nForward Difference  Error           Central Difference  Error\n\n7.91374e-01     2.03630e-01     8.37267e-01     1.57737e-01\n9.88359e-01     6.64502e-03     9.93347e-01     1.65751e-03\n9.94488e-01     5.15746e-04     9.94988e-01     1.65833e-05\n9.94954e-01     5.00825e-05     9.95004e-01     1.65834e-07\n9.94999e-01     4.99333e-06     9.95004e-01     1.65828e-09\n9.95004e-01     4.99183e-07     9.95004e-01     1.66720e-11\n9.95004e-01     4.99136e-08     9.95004e-01     2.10021e-12\n9.95004e-01     4.96341e-09     9.95004e-01     3.25943e-11\n9.95004e-01     1.06184e-10     9.95004e-01     1.06184e-10\n9.95004e-01     2.88174e-09     9.95004e-01     2.88174e-09\n\n\n\n\nSymbolic Differentiation\nThough very useful in simple cases, symbolic differentiation often leads to complex and redundant expressions. In addition, balckbox routines cannot be differentiated.\n\nfrom sympy import *\nx = symbols('x')\n#\ndiff(cos(x), x)\n\n\\(\\displaystyle - \\sin{\\left(x \\right)}\\)\n\n\n\n# a more complicated esxpression\ndef sigmoid(x):\n  return 1 / (1 + exp(-x))\n\ndiff(sigmoid(x),x)\n\n\\(\\displaystyle \\frac{e^{- x}}{\\left(1 + e^{- x}\\right)^{2}}\\)\n\n\nNote that the derivative of \\[ \\sigma(x) = \\frac{1}{1+e^{-x}}\\] can be simply written as \\[ \\frac{d\\sigma }{dx}= (1-\\sigma(x)) \\sigma(x)\\]\n\n# much more complicated\nx,w1,w2,w3,b1,b2,b3 = symbols('x w1 w2 w3 b1 b2 b3')\ny = w3*sigmoid(w2*sigmoid(w1*x + b1) + b2) + b3\ndiff(y, w1)\n\n\\(\\displaystyle \\frac{w_{2} w_{3} x e^{- b_{1} - w_{1} x} e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}}}{\\left(e^{- b_{1} - w_{1} x} + 1\\right)^{2} \\left(e^{- b_{2} - \\frac{w_{2}}{e^{- b_{1} - w_{1} x} + 1}} + 1\\right)^{2}}\\)\n\n\n\ndydw1 = diff(y, w1)\nprint(dydw1)\n\nw2*w3*x*exp(-b1 - w1*x)*exp(-b2 - w2/(exp(-b1 - w1*x) + 1))/((exp(-b1 - w1*x) + 1)**2*(exp(-b2 - w2/(exp(-b1 - w1*x) + 1)) + 1)**2)\n\n\n\n\nAutomatic Differentiation\nHere we show the simplicity and efficiency of autograd from numpy.\n\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad as egrad  # for functions that vectorize over inputs\n\n# We could use np.tanh, but let's write our own as an example.\ndef tanh(x):\n    return (1.0 - np.exp(-x))  / (1.0 + np.exp(-x))\n\nx = np.linspace(-7, 7, 200)\nplt.plot(x, tanh(x),\n         x, egrad(tanh)(x),                                # first  derivative\n         x, egrad(egrad(tanh))(x),                          # second derivative\n         x, egrad(egrad(egrad(tanh)))(x),                    # third  derivative\n         x, egrad(egrad(egrad(egrad(tanh))))(x),              # fourth derivative\n         x, egrad(egrad(egrad(egrad(egrad(tanh)))))(x),        # fifth  derivative\n         x, egrad(egrad(egrad(egrad(egrad(egrad(tanh))))))(x))  # sixth  derivative\n\nplt.axis('off')\nplt.savefig(\"tanh.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom autograd import grad\ngrad_tanh = grad(tanh)            # Obtain its gradient function\ngA = grad_tanh(1.0)               # Evaluate the gradient at x = 1.0\ngN = (tanh(1.01) - tanh(0.99)) / 0.02  # Compare to finite differences\nprint(gA, gN)\n\n0.39322386648296376 0.3932226889551027\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html",
    "href": "labs/02Examples/pytorch/pytorch_101.html",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Based on Bourkes’s https://www.learnpytorch.io/\n\n\nTensors are the fundamental building blocks of machine learning.\n\nimport torch\ntorch.__version__\n\n'1.13.1'\n\n\nStart by creating basic tensors\n\nscalar\nvector\nmatrix\ntensor\n\nPlease see official doc at https://pytorch.org/docs/stable/tensors.html\n\n# scalar\nscalar = torch.tensor(7)\nscalar\n\ntensor(7)\n\n\n\nscalar.ndim\n\n0\n\n\nTo retrieve the value, use the item method\n\nscalar.item()\n\n7\n\n\n\n# vector\nvector = torch.tensor([7, 7])\nvector\n\ntensor([7, 7])\n\n\n\nvector.ndim\n\n1\n\n\n\nvector.shape\n\ntorch.Size([2])\n\n\n\nndim gives the number of external square brackets\nshape gives the actual dimension = length\n\n\n# matrix\nMAT = torch.tensor([[7, 8], [9, 10]])\nMAT\n\ntensor([[ 7,  8],\n        [ 9, 10]])\n\n\n\nMAT.ndim\n\n2\n\n\n\nMAT.shape\n\ntorch.Size([2, 2])\n\n\n\n# tensor\nTEN = torch.tensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\nTEN\n\ntensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\n\n\n\nTEN.shape\n\ntorch.Size([1, 3, 3])\n\n\n\nTEN.ndim\n\n3\n\n\nThe dimensions of a tensor go from outer to inner. That means there’s 1 dimension of 3 by 3.\n\n\n\n\nTensors of random numbers are very common in ML. They are used evrywhere.\n\nrand_tensor = torch.rand(size=(3, 4))\nrand_tensor, rand_tensor.dtype\n\n(tensor([[0.2350, 0.7880, 0.7052, 0.5025],\n         [0.5523, 0.6008, 0.9949, 0.4443],\n         [0.5769, 0.7505, 0.1360, 0.8363]]),\n torch.float32)\n\n\n\n\n\n\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n\n(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n torch.float32)\n\n\n\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n\n(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n torch.float32)\n\n\nCreate a range of numbers in a tensor.\ntorch.arange(start, end, step)\n\nzero_to_ten = torch.arange(0,10)\nzero_to_ten\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nTo get a tensor of the same shape as another.\ntorch.zeros_like(input)\n\nten_zeros = torch.zeros_like(zero_to_ten)\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n\nMany datatypes are available, some specific for CPUs, others better for GPUs.\nDefault datatype is a float32, defined by torch.float32() or just torch.float().\nLower precision values are faster to compute with, but less acccurate…\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='cpu'))\n\n\nLet us place a tensor on the GPU (usually “cuda”, bit here it is “mps” for a Mac)\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=\"mps\", # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='mps', index=0))\n\n\nMost common issues for mismatch are:\n\nshape differences\ndatatype\ndevice issues\n\n\nfloat_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n\ntorch.float16\n\n\n\n\n\nThis is often necessary to ensure compatibility and to avoid pesky bugs.\n\n# Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n\ntensor([[0.7783, 0.1803, 0.1316, 0.2174],\n        [0.5707, 0.7213, 0.5195, 0.5730],\n        [0.6286, 0.9001, 0.8025, 0.5707]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n\n\n\nBasic operations\nMatrix multiplication\nAggregation (min, max, mean, etc.)\nReshaping, squeezing\n\n\n# Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# Multiply it by 10\ntensor * 10\n\ntensor([10, 20, 30])\n\n\n\n# Can also use torch functions\ntm = torch.mul(tensor, 10)\nta = torch.add(tensor, 10)\n\nprint(\"tm = \", tm)\nprint(\"ta = \", ta)\n\ntm =  tensor([10, 20, 30])\nta =  tensor([11, 12, 13])\n\n\n\n# Element-wise multiplication \n# (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n\ntensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n\n\n\ntensor = torch.tensor([1, 2, 3])\n# Element-wise matrix multiplication\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\n\n# Matrix multiplication\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\nBuilt-in torch.matmul() is much faster and should always be used.\n\n%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n\nCPU times: user 1.16 ms, sys: 738 µs, total: 1.89 ms\nWall time: 1.31 ms\n\n\ntensor(14)\n\n\n\n%%time\ntorch.matmul(tensor, tensor)\n\nCPU times: user 310 µs, sys: 85 µs, total: 395 µs\nWall time: 368 µs\n\n\ntensor(14)\n\n\nOf course, shapes must be compatible for matrix multiplication…\n\n# Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)\n\n\n\n# View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n\n\n\n# The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n\nOriginal shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n\n\n\n# torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\n\n\n\n\n\nNeural networks are full of matrix multiplications and dot products.\nThe torch.nn.Linear() module (that we will see in the next pytorch tutorial), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input \\(x\\) and a weights matrix \\(A.\\)\n\\[ y = x A^T + b, \\]\nwhere\n\n\\(x\\) is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other).\n\\(A\\) is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the “T”, that’s because the weights matrix gets transposed). Note: You might also often see \\(W\\) or another letter like \\(X\\) used to denote the weights matrix.\n\\(b\\) is the bias term used to slightly offset the weights and inputs.\n\\(y\\) is the output (a manipulation of the input in the hope to discover patterns in it).\n\nThis is just a linear function, of type \\(y = ax+b,\\) that can be used to draw a straight line.\n\n# Since the linear layer starts with a random weights matrix, we make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2,  # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n\nInput shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n\n\n\n\n\nNow for some aggregation functions.\n\n# Create a tensor\nx = torch.arange(0, 100, 10)\nx\n\ntensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n\n\n\nprint(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n\nMinimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n\n\n\n# alternative: use torch methods\ntorch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n\n(tensor(90), tensor(0), tensor(45.), tensor(450))\n\n\nPositional min/max functions are\n\ntorch.argmin()\ntorch.argmax()\n\n\n# Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n\nTensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n\n\nChanging datatype is possible (recasting)\n\n# Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n\ntorch.float32\n\n\n\n# Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n\ntensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)\n\n\n\n# Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n\ntensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)\n\n\n\n\n\ntorch.reshape(input, shape)\ntorch.Tensor.view(shape) to obtain a view\ntorch.stack(tensors, dim=0) to concatenate along a given direction\ntorch.squeeze(input) to remove all dimensions of value 1\ntorch.unsqueeze(input, dim) to add a dimension of value 1 at dim\ntorch.permute(input, dims) to permute to dims\n\n\n# Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))\n\n\n\n# Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\n\n# Change view (keeps same data as original but changes view)\nz = x.view(1, 7)\nz, z.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\n\n# Changing z changes x\nz[:, 0] = 5\nz, x\n\n(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))\n\n\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])\n\n\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=1) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 5., 5., 5.],\n        [2., 2., 2., 2.],\n        [3., 3., 3., 3.],\n        [4., 4., 4., 4.],\n        [5., 5., 5., 5.],\n        [6., 6., 6., 6.],\n        [7., 7., 7., 7.]])\n\n\n\nprint(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n\nPrevious tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n\n\n\n# Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n\nPrevious shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n\n\n\n\n\n\nOften we need to extract subsets of data from tensors, usually, some rows or columns.\nLet’s look at indexing.\n\n# Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n\n(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n torch.Size([1, 3, 3]))\n\n\n\n# Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n\nFirst square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n\n\n\n# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n\ntensor([[2, 5, 8]])\n\n\n\n\nWe often need to interact with numpy, especially for numerical computations.\nThe two main methods are:\n\ntorch.from_numpy(ndarray)\ntorch.Tensor.numpy()\n\n\n# NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n\n(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\n\n# many pytorch calculations require 'float32'\ntensor32 = torch.from_numpy(array).type(torch.float32)\ntensor32, tensor32.dtype\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.float32)\n\n\n\n# Tensor to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n\n(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))\n\n\n\n\n\n\nTo ensure reproducibility of computations, especially ML training, we need to set the random seed.\n\nimport torch\n#import random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n\nTensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n\n\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n\n\n\n\n\nSee also the code in pytorch_M2.ipynb.\nUsually the command sequence is:\n# Check for GPU\nimport torch\ntorch.cuda.is_available()\n# Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n# check for gpu on mac\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu'\ndevice\n\n'mps'\n\n\nTo put tensors on the GPU, just use the method tensor.to(device), or put the device option directly into the tensor initialization as seen above:\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None,  \n                               device=\"mps\",  \n                               requires_grad=False)  \n\n# Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n\ntensor([1, 2, 3]) cpu\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\nIf you need to interact with your tensors (numpy, matplotlib, etc.), then need to get them back to the CPU. Here we use the method Tensor.cpu()\n\n# If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n\nTypeError: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n\n\n\n# Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n\narray([1, 2, 3])\n\n\n\n# original is still on the GPU\ntensor_on_gpu\n\ntensor([1, 2, 3], device='mps:0')"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#tensors-are-everywhere",
    "href": "labs/02Examples/pytorch/pytorch_101.html#tensors-are-everywhere",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Tensors are the fundamental building blocks of machine learning.\n\nimport torch\ntorch.__version__\n\n'1.13.1'\n\n\nStart by creating basic tensors\n\nscalar\nvector\nmatrix\ntensor\n\nPlease see official doc at https://pytorch.org/docs/stable/tensors.html\n\n# scalar\nscalar = torch.tensor(7)\nscalar\n\ntensor(7)\n\n\n\nscalar.ndim\n\n0\n\n\nTo retrieve the value, use the item method\n\nscalar.item()\n\n7\n\n\n\n# vector\nvector = torch.tensor([7, 7])\nvector\n\ntensor([7, 7])\n\n\n\nvector.ndim\n\n1\n\n\n\nvector.shape\n\ntorch.Size([2])\n\n\n\nndim gives the number of external square brackets\nshape gives the actual dimension = length\n\n\n# matrix\nMAT = torch.tensor([[7, 8], [9, 10]])\nMAT\n\ntensor([[ 7,  8],\n        [ 9, 10]])\n\n\n\nMAT.ndim\n\n2\n\n\n\nMAT.shape\n\ntorch.Size([2, 2])\n\n\n\n# tensor\nTEN = torch.tensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\nTEN\n\ntensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\n\n\n\nTEN.shape\n\ntorch.Size([1, 3, 3])\n\n\n\nTEN.ndim\n\n3\n\n\nThe dimensions of a tensor go from outer to inner. That means there’s 1 dimension of 3 by 3."
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#random-valued-tensors",
    "href": "labs/02Examples/pytorch/pytorch_101.html#random-valued-tensors",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Tensors of random numbers are very common in ML. They are used evrywhere.\n\nrand_tensor = torch.rand(size=(3, 4))\nrand_tensor, rand_tensor.dtype\n\n(tensor([[0.2350, 0.7880, 0.7052, 0.5025],\n         [0.5523, 0.6008, 0.9949, 0.4443],\n         [0.5769, 0.7505, 0.1360, 0.8363]]),\n torch.float32)"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#other-special-tensors",
    "href": "labs/02Examples/pytorch/pytorch_101.html#other-special-tensors",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "zeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n\n(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n torch.float32)\n\n\n\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n\n(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n torch.float32)\n\n\nCreate a range of numbers in a tensor.\ntorch.arange(start, end, step)\n\nzero_to_ten = torch.arange(0,10)\nzero_to_ten\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nTo get a tensor of the same shape as another.\ntorch.zeros_like(input)\n\nten_zeros = torch.zeros_like(zero_to_ten)\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n\nMany datatypes are available, some specific for CPUs, others better for GPUs.\nDefault datatype is a float32, defined by torch.float32() or just torch.float().\nLower precision values are faster to compute with, but less acccurate…\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='cpu'))\n\n\nLet us place a tensor on the GPU (usually “cuda”, bit here it is “mps” for a Mac)\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=\"mps\", # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='mps', index=0))\n\n\nMost common issues for mismatch are:\n\nshape differences\ndatatype\ndevice issues\n\n\nfloat_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n\ntorch.float16\n\n\n\n\n\nThis is often necessary to ensure compatibility and to avoid pesky bugs.\n\n# Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n\ntensor([[0.7783, 0.1803, 0.1316, 0.2174],\n        [0.5707, 0.7213, 0.5195, 0.5730],\n        [0.6286, 0.9001, 0.8025, 0.5707]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n\n\n\nBasic operations\nMatrix multiplication\nAggregation (min, max, mean, etc.)\nReshaping, squeezing\n\n\n# Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# Multiply it by 10\ntensor * 10\n\ntensor([10, 20, 30])\n\n\n\n# Can also use torch functions\ntm = torch.mul(tensor, 10)\nta = torch.add(tensor, 10)\n\nprint(\"tm = \", tm)\nprint(\"ta = \", ta)\n\ntm =  tensor([10, 20, 30])\nta =  tensor([11, 12, 13])\n\n\n\n# Element-wise multiplication \n# (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n\ntensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n\n\n\ntensor = torch.tensor([1, 2, 3])\n# Element-wise matrix multiplication\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\n\n# Matrix multiplication\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\nBuilt-in torch.matmul() is much faster and should always be used.\n\n%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n\nCPU times: user 1.16 ms, sys: 738 µs, total: 1.89 ms\nWall time: 1.31 ms\n\n\ntensor(14)\n\n\n\n%%time\ntorch.matmul(tensor, tensor)\n\nCPU times: user 310 µs, sys: 85 µs, total: 395 µs\nWall time: 368 µs\n\n\ntensor(14)\n\n\nOf course, shapes must be compatible for matrix multiplication…\n\n# Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)\n\n\n\n# View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n\n\n\n# The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n\nOriginal shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n\n\n\n# torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#tensor-multiplication-example-of-linear-regression",
    "href": "labs/02Examples/pytorch/pytorch_101.html#tensor-multiplication-example-of-linear-regression",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Neural networks are full of matrix multiplications and dot products.\nThe torch.nn.Linear() module (that we will see in the next pytorch tutorial), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input \\(x\\) and a weights matrix \\(A.\\)\n\\[ y = x A^T + b, \\]\nwhere\n\n\\(x\\) is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other).\n\\(A\\) is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the “T”, that’s because the weights matrix gets transposed). Note: You might also often see \\(W\\) or another letter like \\(X\\) used to denote the weights matrix.\n\\(b\\) is the bias term used to slightly offset the weights and inputs.\n\\(y\\) is the output (a manipulation of the input in the hope to discover patterns in it).\n\nThis is just a linear function, of type \\(y = ax+b,\\) that can be used to draw a straight line.\n\n# Since the linear layer starts with a random weights matrix, we make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2,  # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n\nInput shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#aggregation-functions",
    "href": "labs/02Examples/pytorch/pytorch_101.html#aggregation-functions",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Now for some aggregation functions.\n\n# Create a tensor\nx = torch.arange(0, 100, 10)\nx\n\ntensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n\n\n\nprint(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n\nMinimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n\n\n\n# alternative: use torch methods\ntorch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n\n(tensor(90), tensor(0), tensor(45.), tensor(450))\n\n\nPositional min/max functions are\n\ntorch.argmin()\ntorch.argmax()\n\n\n# Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n\nTensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n\n\nChanging datatype is possible (recasting)\n\n# Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n\ntorch.float32\n\n\n\n# Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n\ntensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)\n\n\n\n# Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n\ntensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)\n\n\n\n\n\ntorch.reshape(input, shape)\ntorch.Tensor.view(shape) to obtain a view\ntorch.stack(tensors, dim=0) to concatenate along a given direction\ntorch.squeeze(input) to remove all dimensions of value 1\ntorch.unsqueeze(input, dim) to add a dimension of value 1 at dim\ntorch.permute(input, dims) to permute to dims\n\n\n# Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))\n\n\n\n# Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\n\n# Change view (keeps same data as original but changes view)\nz = x.view(1, 7)\nz, z.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\n\n# Changing z changes x\nz[:, 0] = 5\nz, x\n\n(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))\n\n\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])\n\n\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=1) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 5., 5., 5.],\n        [2., 2., 2., 2.],\n        [3., 3., 3., 3.],\n        [4., 4., 4., 4.],\n        [5., 5., 5., 5.],\n        [6., 6., 6., 6.],\n        [7., 7., 7., 7.]])\n\n\n\nprint(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n\nPrevious tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n\n\n\n# Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n\nPrevious shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#slicing",
    "href": "labs/02Examples/pytorch/pytorch_101.html#slicing",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "Often we need to extract subsets of data from tensors, usually, some rows or columns.\nLet’s look at indexing.\n\n# Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n\n(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n torch.Size([1, 3, 3]))\n\n\n\n# Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n\nFirst square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n\n\n\n# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n\ntensor([[2, 5, 8]])\n\n\n\n\nWe often need to interact with numpy, especially for numerical computations.\nThe two main methods are:\n\ntorch.from_numpy(ndarray)\ntorch.Tensor.numpy()\n\n\n# NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n\n(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\n\n# many pytorch calculations require 'float32'\ntensor32 = torch.from_numpy(array).type(torch.float32)\ntensor32, tensor32.dtype\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.float32)\n\n\n\n# Tensor to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n\n(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#reproducibility",
    "href": "labs/02Examples/pytorch/pytorch_101.html#reproducibility",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "To ensure reproducibility of computations, especially ML training, we need to set the random seed.\n\nimport torch\n#import random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n\nTensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n\n\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])"
  },
  {
    "objectID": "labs/02Examples/pytorch/pytorch_101.html#running-computations-on-the-gpu",
    "href": "labs/02Examples/pytorch/pytorch_101.html#running-computations-on-the-gpu",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "See also the code in pytorch_M2.ipynb.\nUsually the command sequence is:\n# Check for GPU\nimport torch\ntorch.cuda.is_available()\n# Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n# check for gpu on mac\ndevice = 'mps' if torch.backends.mps.is_available() else 'cpu'\ndevice\n\n'mps'\n\n\nTo put tensors on the GPU, just use the method tensor.to(device), or put the device option directly into the tensor initialization as seen above:\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None,  \n                               device=\"mps\",  \n                               requires_grad=False)  \n\n# Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n\ntensor([1, 2, 3]) cpu\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\nIf you need to interact with your tensors (numpy, matplotlib, etc.), then need to get them back to the CPU. Here we use the method Tensor.cpu()\n\n# If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n\nTypeError: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n\n\n\n# Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n\narray([1, 2, 3])\n\n\n\n# original is still on the GPU\ntensor_on_gpu\n\ntensor([1, 2, 3], device='mps:0')"
  },
  {
    "objectID": "labs/w7-lab-particle.html",
    "href": "labs/w7-lab-particle.html",
    "title": "Particle Filter",
    "section": "",
    "text": "You will need to download the folder “kf_book” from here Kalman-and-Bayesian-Filters-in-Python if using Google collab please upload that folder as a .zip to your collab workspace then run the first lines in the notebook to unzip it.\nPlease follow instructions in the notebook and send results to TA in .pdf format. You wil need to fill in two lines of code to complete the update function.\nPlease attempt to first solve the exercise by yourself then feel welcome to look at the answers in ParticleFilter\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/06Examples/ensemble_kalman_lorenz.html",
    "href": "labs/06Examples/ensemble_kalman_lorenz.html",
    "title": "Ensemble Kalman Filter for Lorenz63 System",
    "section": "",
    "text": "We analyze the Lorenz63 system of nonlinear ODEs\n\\[\n\\begin{align}\n     \\frac{\\mathrm{d} x}{\\mathrm{d} t} &= -\\sigma(x-y), \\nonumber  \\\\\n    \\frac{\\mathrm{d} y}{\\mathrm{d} t}  &= \\rho x-y-xz,  \\label{eq:lorenz} \\\\\n    \\frac{\\mathrm{d} z}{\\mathrm{d} t}  &= xy -\\beta z,   \\nonumber\n\\end{align}\n\\]\nwhere \\(x=x(t),\\) \\(y=y(t)\\), \\(z=z(t)\\) and \\(\\sigma\\) (ratio of kinematic viscosity divided by thermal diffusivity), \\(\\rho\\) (measure of stability) and \\(\\beta\\) (related to the wave number) are parameters. Chaotic behavior is obtained when the parameters are chosen as\n\\[\n  \\sigma = 10,\\quad \\rho=28,\\quad \\beta = 8/3.\n\\]\nThis system is a simplified model for atmospheric convection and is an excellent example of the lack of predictability. It is ill-posed in the sense of Hadamard.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Lorenz63(state,*args): #Lorenz 63 model\n    sigma = args[0]\n    beta = args[1]\n    rho = args[2]\n    x, y, z = state #Unpack the state vector\n    f = np.zeros(3) #Derivatives\n    f[0] = sigma * (y - x)\n    f[1] = x * (rho - z) - y\n    f[2] = x * y - beta * z\n    return f \n\ndef RK4(rhs,state,dt,*args):\n    \n    k1 = rhs(state,*args)\n    k2 = rhs(state+k1*dt/2,*args)\n    k3 = rhs(state+k2*dt/2,*args)\n    k4 = rhs(state+k3*dt,*args)\n\n    new_state = state + (dt/6)*(k1+2*k2+2*k3+k4)\n    return new_state\n\ndef EnKF(ubi,w,ObsOp,JObsOp,R,B):\n    \n    # The analysis step for the (stochastic) ensemble Kalman filter \n    # with virtual observations\n\n    n,N = ubi.shape # n is the state dimension and N is the size of ensemble\n    m = w.shape[0] # m is the size of measurement vector\n\n    # compute the mean of forecast ensemble\n    ub = np.mean(ubi,1)    \n    # evaluate Jacobian of observation operator at ub\n    Dh = JObsOp(ub)\n    # compute Kalman gain\n    D = #FILL IN CODE\n    K = #FILL IN CODE\n        \n    \n    wi = np.zeros([m,N])\n    uai = np.zeros([n,N])\n    for i in range(N):\n        # create virtual observations\n        wi[:,i] = w + np.random.multivariate_normal(np.zeros(m), R)\n        # compute analysis ensemble\n        uai[:,i] = ubi[:,i] + K @ (wi[:,i]-ObsOp(ubi[:,i]))\n        \n    # compute the mean of analysis ensemble\n    ua = #FILL IN CODE\n    # compute analysis error covariance matrix\n    P = #FILL IN CODE\n    return uai, P\n\n\n\n\nsigma = 10.0     \nbeta = 8.0/3.0\nrho = 28.0     \ndt = 0.01\ntm = 10\nnt = int(tm/dt)\nt = np.linspace(0,tm,nt+1)\n\n\n\n\n\ndef h(u):\n    w = u\n    return w\n\ndef Dh(u):\n    n = len(u)\n    D = np.eye(n)\n    return D\n\n\nu0True = np.array([1,1,1]) # True initial conditions\nnp.random.seed(seed=1)\nsig_m = 0.15  # standard deviation for measurement noise\nR = sig_m**2*np.eye(3) #covariance matrix for measurement noise\n\ndt_m = 0.2 #time period between observations\ntm_m = 2 #maximum time for observations\nnt_m = int(tm_m/dt_m) #number of observation instants\n\nind_m = (np.linspace(int(dt_m/dt),int(tm_m/dt),nt_m)).astype(int)\nt_m = t[ind_m]\n\n#time integration\nuTrue = np.zeros([3,nt+1])\nuTrue[:,0] = u0True\nkm = 0\nw = np.zeros([3,nt_m])\nfor k in range(nt):\n    uTrue[:,k+1] = RK4(Lorenz63,uTrue[:,k],dt,sigma,beta,rho)\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        w[:,km] = h(uTrue[:,k+1]) + np.random.normal(0,sig_m,[3,])\n        km = km+1\n\n\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,uTrue[k,:], label='True', linewidth = 3)\n    ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n               label='Observation', markersize = 8, markeredgewidth = 2)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, tm_m, color='lightgray', alpha=0.4, lw=0)\n\nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax[0].set_ylabel('x(t)', labelpad=5)\nax[1].set_ylabel('y(t)', labelpad=-12)\nax[2].set_ylabel('z(t)')\nfig.subplots_adjust(hspace=0.5)\n\n\n\n\n\nu0b = np.array([2.0,3.0,4.0])\n\nsig_b= 0.1\nB = sig_b**2*np.eye(3)\nQ = 0.0*np.eye(3)\n\n#time integration\nub = np.zeros([3,nt+1])\nub[:,0] = u0b\nua = np.zeros([3,nt+1])\nua[:,0] = u0b\n\nn = 3 #state dimension\nm = 3 #measurement dimension\n\n# ensemble size \nN = 10\n#initialize ensemble\nuai = np.zeros([3,N])\nfor i in range(N):\n    uai[:,i] = u0b + np.random.multivariate_normal(np.zeros(n), B)\n\nkm = 0\nfor k in range(nt):\n    # Forecast Step\n    #background trajectory [without correction]\n    ub[:,k+1] = RK4(Lorenz63,ub[:,k],dt,sigma,beta,rho) \n    #EnKF trajectory [with correction at observation times]\n    for i in range(N): # forecast ensemble\n        uai[:,i] = RK4(Lorenz63,uai[:,i],dt,sigma,beta,rho) \\\n                 + np.random.multivariate_normal(np.zeros(n), Q)\n\n    # compute the mean of forecast ensemble\n    ua[:,k+1] = np.mean(uai,1)\n    # compute forecast error covariance matrix\n    B = (1/(N-1)) * (uai - ua[:,k+1].reshape(-1,1)) @ (uai - ua[:,k+1].reshape(-1,1)).T\n\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        # Analysis Step\n        uai,B = EnKF(uai,w[:,km],h,Dh,R,B)\n        # compute the mean of analysis ensemble\n        ua[:,k+1] = np.mean(uai,1)    \n        km = km+1\n\n\n\n\nWe plot the trajectories of \\(x,\\) \\(y\\) and \\(z\\) as a function of time \\(t.\\)\n\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,uTrue[k,:], label='True', linewidth = 3)\n    ax[k].plot(t,ub[k,:], ':', label='Background', linewidth = 3)\n    ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n               label='Observation', markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,ua[k,:], '--', label='Analysis', linewidth = 3)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, tm_m, color='lightgray', alpha=0.4, lw=0)\n\nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax[0].set_ylabel('x(t)', labelpad=5)\nax[1].set_ylabel('y(t)', labelpad=-12)\nax[2].set_ylabel('z(t)')\nfig.subplots_adjust(hspace=0.5)\n\nplt.savefig('L63_EnKF.eps', dpi = 500, bbox_inches = 'tight')"
  },
  {
    "objectID": "labs/06Examples/ensemble_kalman_lorenz.html#parameters",
    "href": "labs/06Examples/ensemble_kalman_lorenz.html#parameters",
    "title": "Ensemble Kalman Filter for Lorenz63 System",
    "section": "",
    "text": "sigma = 10.0     \nbeta = 8.0/3.0\nrho = 28.0     \ndt = 0.01\ntm = 10\nnt = int(tm/dt)\nt = np.linspace(0,tm,nt+1)"
  },
  {
    "objectID": "labs/06Examples/ensemble_kalman_lorenz.html#twin-experiment",
    "href": "labs/06Examples/ensemble_kalman_lorenz.html#twin-experiment",
    "title": "Ensemble Kalman Filter for Lorenz63 System",
    "section": "",
    "text": "def h(u):\n    w = u\n    return w\n\ndef Dh(u):\n    n = len(u)\n    D = np.eye(n)\n    return D\n\n\nu0True = np.array([1,1,1]) # True initial conditions\nnp.random.seed(seed=1)\nsig_m = 0.15  # standard deviation for measurement noise\nR = sig_m**2*np.eye(3) #covariance matrix for measurement noise\n\ndt_m = 0.2 #time period between observations\ntm_m = 2 #maximum time for observations\nnt_m = int(tm_m/dt_m) #number of observation instants\n\nind_m = (np.linspace(int(dt_m/dt),int(tm_m/dt),nt_m)).astype(int)\nt_m = t[ind_m]\n\n#time integration\nuTrue = np.zeros([3,nt+1])\nuTrue[:,0] = u0True\nkm = 0\nw = np.zeros([3,nt_m])\nfor k in range(nt):\n    uTrue[:,k+1] = RK4(Lorenz63,uTrue[:,k],dt,sigma,beta,rho)\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        w[:,km] = h(uTrue[:,k+1]) + np.random.normal(0,sig_m,[3,])\n        km = km+1\n\n\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,uTrue[k,:], label='True', linewidth = 3)\n    ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n               label='Observation', markersize = 8, markeredgewidth = 2)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, tm_m, color='lightgray', alpha=0.4, lw=0)\n\nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax[0].set_ylabel('x(t)', labelpad=5)\nax[1].set_ylabel('y(t)', labelpad=-12)\nax[2].set_ylabel('z(t)')\nfig.subplots_adjust(hspace=0.5)"
  },
  {
    "objectID": "labs/06Examples/ensemble_kalman_lorenz.html#data-assimilation",
    "href": "labs/06Examples/ensemble_kalman_lorenz.html#data-assimilation",
    "title": "Ensemble Kalman Filter for Lorenz63 System",
    "section": "",
    "text": "u0b = np.array([2.0,3.0,4.0])\n\nsig_b= 0.1\nB = sig_b**2*np.eye(3)\nQ = 0.0*np.eye(3)\n\n#time integration\nub = np.zeros([3,nt+1])\nub[:,0] = u0b\nua = np.zeros([3,nt+1])\nua[:,0] = u0b\n\nn = 3 #state dimension\nm = 3 #measurement dimension\n\n# ensemble size \nN = 10\n#initialize ensemble\nuai = np.zeros([3,N])\nfor i in range(N):\n    uai[:,i] = u0b + np.random.multivariate_normal(np.zeros(n), B)\n\nkm = 0\nfor k in range(nt):\n    # Forecast Step\n    #background trajectory [without correction]\n    ub[:,k+1] = RK4(Lorenz63,ub[:,k],dt,sigma,beta,rho) \n    #EnKF trajectory [with correction at observation times]\n    for i in range(N): # forecast ensemble\n        uai[:,i] = RK4(Lorenz63,uai[:,i],dt,sigma,beta,rho) \\\n                 + np.random.multivariate_normal(np.zeros(n), Q)\n\n    # compute the mean of forecast ensemble\n    ua[:,k+1] = np.mean(uai,1)\n    # compute forecast error covariance matrix\n    B = (1/(N-1)) * (uai - ua[:,k+1].reshape(-1,1)) @ (uai - ua[:,k+1].reshape(-1,1)).T\n\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        # Analysis Step\n        uai,B = EnKF(uai,w[:,km],h,Dh,R,B)\n        # compute the mean of analysis ensemble\n        ua[:,k+1] = np.mean(uai,1)    \n        km = km+1"
  },
  {
    "objectID": "labs/06Examples/ensemble_kalman_lorenz.html#output",
    "href": "labs/06Examples/ensemble_kalman_lorenz.html#output",
    "title": "Ensemble Kalman Filter for Lorenz63 System",
    "section": "",
    "text": "We plot the trajectories of \\(x,\\) \\(y\\) and \\(z\\) as a function of time \\(t.\\)\n\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,uTrue[k,:], label='True', linewidth = 3)\n    ax[k].plot(t,ub[k,:], ':', label='Background', linewidth = 3)\n    ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n               label='Observation', markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,ua[k,:], '--', label='Analysis', linewidth = 3)\n    ax[k].set_xlabel('t')\n    ax[k].axvspan(0, tm_m, color='lightgray', alpha=0.4, lw=0)\n\nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax[0].set_ylabel('x(t)', labelpad=5)\nax[1].set_ylabel('y(t)', labelpad=-12)\nax[2].set_ylabel('z(t)')\nfig.subplots_adjust(hspace=0.5)\n\nplt.savefig('L63_EnKF.eps', dpi = 500, bbox_inches = 'tight')"
  },
  {
    "objectID": "labs/w3-lab02.html",
    "href": "labs/w3-lab02.html",
    "title": "Lab 02: Data visualization with ggplot2!",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/w6-lab-ensemble.html",
    "href": "labs/w6-lab-ensemble.html",
    "title": "Ensemble Kalman Filter",
    "section": "",
    "text": "Please follow instructions in the notebook and send results to TA in .pdf format. You will need to complete the code for an ensemble Kalman filter by filling in the code at 4 lines.\nPlease attempt to first solve the exercise by yourself then feel welcome to look at the answers in enkf_l63\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/04Examples/threeDVar_l63.html",
    "href": "labs/04Examples/threeDVar_l63.html",
    "title": "3D-Var for Lorenz63 System",
    "section": "",
    "text": "We analyze the Lorenz63 system of nonlinear ODEs\n\\[\n\\begin{align}\n     \\frac{\\mathrm{d} x}{\\mathrm{d} t} &= -\\sigma(x-y), \\nonumber  \\\\\n    \\frac{\\mathrm{d} y}{\\mathrm{d} t}  &= \\rho x-y-xz,  \\label{eq:lorenz} \\\\\n    \\frac{\\mathrm{d} z}{\\mathrm{d} t}  &= xy -\\beta z,   \\nonumber\n\\end{align}\n\\]\nwhere \\(x=x(t),\\) \\(y=y(t)\\), \\(z=z(t)\\) and \\(\\sigma\\) (ratio of kinematic viscosity divided by thermal diffusivity), \\(\\rho\\) (measure of stability) and \\(\\beta\\) (related to the wave number) are parameters. Chaotic behavior is obtained when the parameters are chosen as\n\\[\n  \\sigma = 10,\\quad \\rho=28,\\quad \\beta = 8/3.\n\\]\nThis system is a simplified model for atmospheric convection and is an excellent example of the lack of predictability. It is ill-posed in the sense of Hadamard.\n\n\nWe set up a twin experiment with the following parameters:\n\ntrue initial condition $u_t (0) = [1,1,1]^T $ and initial guess $u (0) = [2,3,4]^T $\nobservations every \\(0.2\\) time units, for a duration of \\(2\\) units\ncomplete measurement of system state, i.e. \\(H(u)=u.\\)\nmeasurement noise is Gaussian, zero mean, equal variances \\(\\sigma = \\sigma_1 = \\sigma_2 = \\sigma_3 = 0.15\\) and hence \\(R= \\mathrm{diag} (\\sigma)\\)\nfixed background covariance matrix \\(B = \\mathrm{diag} (0.01,\\ 0.01,\\ 0.01)\\)\n\n\n\n\nBackground state values are computed at \\(t = 0.2\\) by time integration of the L63 system, starting from the initial guess.\nObservations at \\(t = 0.2\\) are assimilated to provide the analysis at $t = 0.2. $\nAfter that, background state values are computed at \\(t = 0.4\\) by time integration, starting from the analysis at \\(t = 0.2,\\) and so on.\n\nThe prediction is made up to time \\(t=10.\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from examples import *\n#from time_integrators import *\ndef Lorenz63(state,*args): #Lorenz 63 model\n    sigma = args[0]\n    beta = args[1]\n    rho = args[2]\n    x, y, z = state #Unpack the state vector\n    f = np.zeros(3) #Derivatives\n    f[0] = sigma * (y - x)\n    f[1] = x * (rho - z) - y\n    f[2] = x * y - beta * z\n    return f \n\ndef RK4(rhs,state,dt,*args):\n    \n    k1 = rhs(state,*args)\n    k2 = rhs(state+k1*dt/2,*args)\n    k3 = rhs(state+k2*dt/2,*args)\n    k4 = rhs(state+k3*dt,*args)\n\n    new_state = state + (dt/6)*(k1+2*k2+2*k3+k4)\n    return new_state\n    \ndef Lin3dvar(ub,w,H,R,B,opt):\n    \n    # The solution of the 3DVAR problem in the linear case requires \n    # the solution of a linear system of equations.\n    # Here we utilize the built-in numpy function to do this.\n    # Other schemes can be used, instead.\n    \n    if opt == 1: #model-space approach\n        Bi = np.linalg.inv(B)\n        Ri = np.linalg.inv(R)\n        A = Bi + (H.T)@Ri@H\n        b = Bi@ub + (H.T)@Ri@w\n        ua = np.linalg.solve(A,b) #solve a linear system \n    \n    elif opt == 2: #model-space incremental approach\n        \n        Bi = np.linalg.inv(B)\n        Ri = np.linalg.inv(R)\n        A = Bi + (H.T)@Ri@H\n        b = (H.T)@Ri@(w-H@ub)\n        ua = ub + np.linalg.solve(A,b) #solve a linear system \n        \n        \n    elif opt == 3: #observation-space incremental approach\n    \n        A = R + H@B@(H.T)\n        b = (w-H@ub)\n        ua = ub + B@(H.T)@np.linalg.solve(A,b) #solve a linear system\n        \n    return ua\n\n\n#Application: Lorenz 63\n# parameters\nsigma = 10.0     \nbeta = 8.0/3.0\nrho = 28.0     \ndt = 0.01\ntm = 10\nnt = int(tm/dt)\nt = np.linspace(0,tm,nt+1)\n\n############################ Twin experiment ##################################\n\nu0True = np.array([1,1,1]) # True initial conditions\nnp.random.seed(seed=1)\nsig_m= 0.15  # standard deviation for measurement noise\nR = sig_m**2*np.eye(3) #covariance matrix for measurement noise\nH = np.eye(3) #linear observation operator\n\ndt_m = 0.2 #time period between observations\ntm_m = 2 #maximum time for observations\nnt_m = int(tm_m/dt_m) #number of observation instants\n\n#t_m = np.linspace(dt_m,tm_m,nt_m) #np.where( (t&lt;=2) & (t%0.1==0) )[0]\nind_m = (np.linspace(int(dt_m/dt),int(tm_m/dt),nt_m)).astype(int)\nt_m = t[ind_m]\n\n#time integration\nuTrue = np.zeros([3,nt+1])\nuTrue[:,0] = u0True\nkm = 0\nw = np.zeros([3,nt_m])\nfor k in range(nt):\n    uTrue[:,k+1] = RK4(Lorenz63,uTrue[:,k],dt,sigma,beta,rho)\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        w[:,km] = H@uTrue[:,k+1] + np.random.normal(0,sig_m,[3,])\n        km = km+1\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1,ncols=1, figsize=(10,8))\n\nax.plot(t,uTrue[1,:], label=r'\\bf{True}', linewidth = 3)\nax.plot(t[ind_m],w[1,:], 'o', fillstyle='none', \\\n            label=r'\\bf{Observation}', markersize = 8, markeredgewidth = 2)\nax.set_xlabel(r'$t$',fontsize=22)\nax.axvspan(0, tm_m, color='y', alpha=0.4, lw=0)\n\nax.legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax.set_ylabel(r'$x(t)$', labelpad=5)\nax.set_ylabel(r'$y(t)$', labelpad=-12)\nax.set_ylabel(r'$z(t)$')\nfig.subplots_adjust(hspace=0.5)\n\n\n\n########################### Data Assimilation ################################\nu0b = np.array([2.0,3.0,4.0])\n\nsig_b= 0.1\nB = sig_b**2*np.eye(3)\n\n#time integration\nub = np.zeros([3,nt+1])\nub[:,0] = u0b\nua = np.zeros([3,nt+1])\nua[:,0] = u0b\nkm = 0\nfor k in range(nt):\n    ub[:,k+1] = RK4(Lorenz63,ub[:,k],dt,sigma,beta,rho)\n    ua[:,k+1] = RK4(Lorenz63,ua[:,k],dt,sigma,beta,rho)\n\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        ua[:,k+1] = Lin3dvar(ua[:,k+1],w[:,km],H,R,B,3)  \n        km = km+1\n\n\n\n\n\n############################### Plotting ######################################\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,uTrue[k,:], label=r'\\bf{True}', linewidth = 3)\n    ax[k].plot(t,ub[k,:], ':', label=r'\\bf{Background}', linewidth = 3)\n    ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n               label=r'\\bf{Observation}', markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,ua[k,:], '--', label=r'\\bf{Analysis}', linewidth = 3)\n    ax[k].set_xlabel(r'$t$',fontsize=22)\n    ax[k].axvspan(0, tm_m, color='y', alpha=0.4, lw=0)\n\nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax[0].set_ylabel(r'$x(t)$', labelpad=5)\nax[1].set_ylabel(r'$y(t)$', labelpad=-12)\nax[2].set_ylabel(r'$z(t)$')\nfig.subplots_adjust(hspace=0.5)"
  },
  {
    "objectID": "labs/04Examples/threeDVar_l63.html#problem-setup",
    "href": "labs/04Examples/threeDVar_l63.html#problem-setup",
    "title": "3D-Var for Lorenz63 System",
    "section": "",
    "text": "We set up a twin experiment with the following parameters:\n\ntrue initial condition $u_t (0) = [1,1,1]^T $ and initial guess $u (0) = [2,3,4]^T $\nobservations every \\(0.2\\) time units, for a duration of \\(2\\) units\ncomplete measurement of system state, i.e. \\(H(u)=u.\\)\nmeasurement noise is Gaussian, zero mean, equal variances \\(\\sigma = \\sigma_1 = \\sigma_2 = \\sigma_3 = 0.15\\) and hence \\(R= \\mathrm{diag} (\\sigma)\\)\nfixed background covariance matrix \\(B = \\mathrm{diag} (0.01,\\ 0.01,\\ 0.01)\\)\n\n\n\n\nBackground state values are computed at \\(t = 0.2\\) by time integration of the L63 system, starting from the initial guess.\nObservations at \\(t = 0.2\\) are assimilated to provide the analysis at $t = 0.2. $\nAfter that, background state values are computed at \\(t = 0.4\\) by time integration, starting from the analysis at \\(t = 0.2,\\) and so on.\n\nThe prediction is made up to time \\(t=10.\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from examples import *\n#from time_integrators import *\ndef Lorenz63(state,*args): #Lorenz 63 model\n    sigma = args[0]\n    beta = args[1]\n    rho = args[2]\n    x, y, z = state #Unpack the state vector\n    f = np.zeros(3) #Derivatives\n    f[0] = sigma * (y - x)\n    f[1] = x * (rho - z) - y\n    f[2] = x * y - beta * z\n    return f \n\ndef RK4(rhs,state,dt,*args):\n    \n    k1 = rhs(state,*args)\n    k2 = rhs(state+k1*dt/2,*args)\n    k3 = rhs(state+k2*dt/2,*args)\n    k4 = rhs(state+k3*dt,*args)\n\n    new_state = state + (dt/6)*(k1+2*k2+2*k3+k4)\n    return new_state\n    \ndef Lin3dvar(ub,w,H,R,B,opt):\n    \n    # The solution of the 3DVAR problem in the linear case requires \n    # the solution of a linear system of equations.\n    # Here we utilize the built-in numpy function to do this.\n    # Other schemes can be used, instead.\n    \n    if opt == 1: #model-space approach\n        Bi = np.linalg.inv(B)\n        Ri = np.linalg.inv(R)\n        A = Bi + (H.T)@Ri@H\n        b = Bi@ub + (H.T)@Ri@w\n        ua = np.linalg.solve(A,b) #solve a linear system \n    \n    elif opt == 2: #model-space incremental approach\n        \n        Bi = np.linalg.inv(B)\n        Ri = np.linalg.inv(R)\n        A = Bi + (H.T)@Ri@H\n        b = (H.T)@Ri@(w-H@ub)\n        ua = ub + np.linalg.solve(A,b) #solve a linear system \n        \n        \n    elif opt == 3: #observation-space incremental approach\n    \n        A = R + H@B@(H.T)\n        b = (w-H@ub)\n        ua = ub + B@(H.T)@np.linalg.solve(A,b) #solve a linear system\n        \n    return ua\n\n\n#Application: Lorenz 63\n# parameters\nsigma = 10.0     \nbeta = 8.0/3.0\nrho = 28.0     \ndt = 0.01\ntm = 10\nnt = int(tm/dt)\nt = np.linspace(0,tm,nt+1)\n\n############################ Twin experiment ##################################\n\nu0True = np.array([1,1,1]) # True initial conditions\nnp.random.seed(seed=1)\nsig_m= 0.15  # standard deviation for measurement noise\nR = sig_m**2*np.eye(3) #covariance matrix for measurement noise\nH = np.eye(3) #linear observation operator\n\ndt_m = 0.2 #time period between observations\ntm_m = 2 #maximum time for observations\nnt_m = int(tm_m/dt_m) #number of observation instants\n\n#t_m = np.linspace(dt_m,tm_m,nt_m) #np.where( (t&lt;=2) & (t%0.1==0) )[0]\nind_m = (np.linspace(int(dt_m/dt),int(tm_m/dt),nt_m)).astype(int)\nt_m = t[ind_m]\n\n#time integration\nuTrue = np.zeros([3,nt+1])\nuTrue[:,0] = u0True\nkm = 0\nw = np.zeros([3,nt_m])\nfor k in range(nt):\n    uTrue[:,k+1] = RK4(Lorenz63,uTrue[:,k],dt,sigma,beta,rho)\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        w[:,km] = H@uTrue[:,k+1] + np.random.normal(0,sig_m,[3,])\n        km = km+1"
  },
  {
    "objectID": "labs/04Examples/threeDVar_l63.html#plot-the-ground-truth-and-the-observations.-note-what-you-observe.-why-might-this-be-a-difficult-problem",
    "href": "labs/04Examples/threeDVar_l63.html#plot-the-ground-truth-and-the-observations.-note-what-you-observe.-why-might-this-be-a-difficult-problem",
    "title": "3D-Var for Lorenz63 System",
    "section": "",
    "text": "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(10,8))\n\nax.plot(t,uTrue[1,:], label=r'\\bf{True}', linewidth = 3)\nax.plot(t[ind_m],w[1,:], 'o', fillstyle='none', \\\n            label=r'\\bf{Observation}', markersize = 8, markeredgewidth = 2)\nax.set_xlabel(r'$t$',fontsize=22)\nax.axvspan(0, tm_m, color='y', alpha=0.4, lw=0)\n\nax.legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax.set_ylabel(r'$x(t)$', labelpad=5)\nax.set_ylabel(r'$y(t)$', labelpad=-12)\nax.set_ylabel(r'$z(t)$')\nfig.subplots_adjust(hspace=0.5)\n\n\n\n########################### Data Assimilation ################################\nu0b = np.array([2.0,3.0,4.0])\n\nsig_b= 0.1\nB = sig_b**2*np.eye(3)\n\n#time integration\nub = np.zeros([3,nt+1])\nub[:,0] = u0b\nua = np.zeros([3,nt+1])\nua[:,0] = u0b\nkm = 0\nfor k in range(nt):\n    ub[:,k+1] = RK4(Lorenz63,ub[:,k],dt,sigma,beta,rho)\n    ua[:,k+1] = RK4(Lorenz63,ua[:,k],dt,sigma,beta,rho)\n\n    if (km&lt;nt_m) and (k+1==ind_m[km]):\n        ua[:,k+1] = Lin3dvar(ua[:,k+1],w[:,km],H,R,B,3)  \n        km = km+1"
  },
  {
    "objectID": "labs/04Examples/threeDVar_l63.html#plot-results-and-write-some-conclusions",
    "href": "labs/04Examples/threeDVar_l63.html#plot-results-and-write-some-conclusions",
    "title": "3D-Var for Lorenz63 System",
    "section": "",
    "text": "############################### Plotting ######################################\nfig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,8))\nax = ax.flat\n\nfor k in range(3):\n    ax[k].plot(t,uTrue[k,:], label=r'\\bf{True}', linewidth = 3)\n    ax[k].plot(t,ub[k,:], ':', label=r'\\bf{Background}', linewidth = 3)\n    ax[k].plot(t[ind_m],w[k,:], 'o', fillstyle='none', \\\n               label=r'\\bf{Observation}', markersize = 8, markeredgewidth = 2)\n    ax[k].plot(t,ua[k,:], '--', label=r'\\bf{Analysis}', linewidth = 3)\n    ax[k].set_xlabel(r'$t$',fontsize=22)\n    ax[k].axvspan(0, tm_m, color='y', alpha=0.4, lw=0)\n\nax[0].legend(loc=\"center\", bbox_to_anchor=(0.5,1.25),ncol =4,fontsize=15)\n\nax[0].set_ylabel(r'$x(t)$', labelpad=5)\nax[1].set_ylabel(r'$y(t)$', labelpad=-12)\nax[2].set_ylabel(r'$z(t)$')\nfig.subplots_adjust(hspace=0.5)"
  },
  {
    "objectID": "labs/w4-lab02-3DVar.html",
    "href": "labs/w4-lab02-3DVar.html",
    "title": "3DVar",
    "section": "",
    "text": "Please follow instructions in the notebook threeDVar_l63.ipynb and please send results to TA in .pdf format.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "labs/w6-lab-enf.html",
    "href": "labs/w6-lab-enf.html",
    "title": "Digital Twins for Physical Systems",
    "section": "",
    "text": "ReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 08",
    "section": "",
    "text": "Topic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nConditional Normalizing Flows I\n\n\nNeural Density Estimation\n\n\nMon, Mar 04\n\n\n\n\nConditional Normalizing Flows II\n\n\nNeural Density Estimation\n\n\nWed, Mar 06\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weeks/week-08.html#remote-classes",
    "href": "weeks/week-08.html#remote-classes",
    "title": "Week 08",
    "section": "Remote classes",
    "text": "Remote classes\n\nLecture on Teams Monday March 4 5:00—6:15 PM Recording\nLecture on Teams Wednesday March 6 5:00—6:15 PM"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 04",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nVariational Data Assimilation\n\n\nData Assimilation\n\n\nMon, Jan 29\n\n\n\n\nStatistical Inverse Problems\n\n\nBayesian Inference\n\n\nWed, Jan 31\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\n3DVar\n\n\nFri, Feb 02\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 9 — A toolbox for digital twins section until 9.4\n\n\nChapter 8 — A toolbox for digital twins section 8.8\n\n\nChapter 2 — A toolbox for digital twins section 2.5\n\n\nChapter 11 — A toolbox for digital twins until section 11.4, and 11.5\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 07",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nSimulation-based Inference\n\n\nUncertainty Quantification\n\n\nThu, Feb 22\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nParticle Filter\n\n\nThu, Feb 22\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 16 — Understanding Deep Learning\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 02",
    "section": "",
    "text": "Lectures\n\n\n\n\n\nTopic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nBasics\n\n\nData Assimilation & Inverse Problems\n\n\nWed, Jan 10\n\n\n\n\n\nNo matching items\n\n\n\n\nAssignments\n\n\n\n\n\nAssignment\n\n\nTitle\n\n\nDue\n\n\n\n\n\n\nLab\n\n\nLab 01: Hello R!\n\n\nFri, Sep 16\n\n\n\n\nLab\n\n\nPyTorch intro\n\n\nFri, Jan 19\n\n\n\n\n\nNo matching items\n\n\n\n\nReadings\n\n\n\n\n\n\nChapter 8 — A toolbox for digital twins section 8.7\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 01",
    "section": "",
    "text": "Topic\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nWelcome to Digital Twins for Physical Systems\n\n\nIntroduction\n\n\nMon, Jan 08\n\n\n\n\nBasics\n\n\nData Assimilation & Inverse Problems\n\n\nWed, Jan 10\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weeks/week-01.html#footnotes",
    "href": "weeks/week-01.html#footnotes",
    "title": "Week 01",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is a lot of material there that you may want to read through. Use material presented in class as a guide what is important.↩︎"
  },
  {
    "objectID": "hw/w2-hw01.html",
    "href": "hw/w2-hw01.html",
    "title": "HW 01: Pet Names",
    "section": "",
    "text": "Add instructions for assignment.\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "goals.html",
    "href": "goals.html",
    "title": "Goal",
    "section": "",
    "text": "Goal\nThe overall goal of this course is to bring you to where the current literature is regarding the use of Digital Twins to\n\nmonitor physical systems from indirect measurements\nassess uncertainty\ncontrol the system\n\nThe course will start with introducing topics from traditional Data Assimilation (DA) and Bayesian inference and will make it through to the latest developments in Differential Programming (DP), Simulation-Based Inference (SBI), recursive Bayesian Inference (RBI), and learned RBI through the use of Generative AI.\n\n\nCourse outline\n\nIntroduction\n\nwelcome\noverview Digital Twins\n\nInverse Problems\n\nill-posedness\nTikhonov regularization\nGeneral Formulation\nDiscrepancy principle\nCross-validation\n\nBasic Data Assimilation\n\nintroduction\nadjoint state method\nvariational data assimilation\n\nStatistical Inverse Problems\ndifferential programming\n\nreverse-mode = adjoint state\n\nAdvanced Data Assimilation\nNeural Density Estimation\n\ngenerative Networks\nNormalizing Flows\nconditional Normalizing Flows\n\nSimulation-based inference\n\nintroduction scientific ML\nBayesian inference\n\nSurrogate Modeling\n\nFourier Neural Operators FNOs\n\nLearned Data Assimilation\n\n\n\nTopics\n\n\nLearning goals\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "Goals"
    ]
  },
  {
    "objectID": "slides/week-08/11.html",
    "href": "slides/week-08/11.html",
    "title": "Digital Twins for Physical Systems",
    "section": "",
    "text": "ReuseCC BY 4.0"
  },
  {
    "objectID": "slides/week-01/02-basics.html#text-book",
    "href": "slides/week-01/02-basics.html#text-book",
    "title": "Basics",
    "section": "Text book",
    "text": "Text book"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section",
    "href": "slides/week-01/02-basics.html#section",
    "title": "Basics",
    "section": "",
    "text": "INTRODUCTION"
  },
  {
    "objectID": "slides/week-01/02-basics.html#what-is-data-assimilation",
    "href": "slides/week-01/02-basics.html#what-is-data-assimilation",
    "title": "Basics",
    "section": "What is data assimilation",
    "text": "What is data assimilation\nSimplest view: a method of combining observations with model output.\nWhy do we need data assimilation? Why not just use the observations? (cf. Regression)\nWe want to predict the future!\n\nFor that we need models.\nBut when models are not constrained periodically by reality, they are of little value.\nTherefore, it is necessary to fit the model state as closely as possible to the observations, before a prediction is made."
  },
  {
    "objectID": "slides/week-01/02-basics.html#data-assimilation-methods",
    "href": "slides/week-01/02-basics.html#data-assimilation-methods",
    "title": "Basics",
    "section": "Data assimilation methods",
    "text": "Data assimilation methods\nThere are two major classes of methods:\n\nVariational methods where we explicitly minimize a cost function using optimization methods.\nStatistical methods where we compute the best linear unbiased estimate (BLUE) by algebraic computations using the Kalman filter.\n\nThey provide the same result in the linear case, which is the only context where their optimality can be rigorously proved.\nThey both have difficulties in dealing with non-linearities and large problems.\nThe error statistics that are required by both, are in general poorly known."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-approaches",
    "href": "slides/week-01/02-basics.html#introduction-approaches",
    "title": "Basics",
    "section": "Introduction: approaches",
    "text": "Introduction: approaches\nDA is an approach for solving a specific class of inverse, or parameter estimation problems, where the parameter we seek is the initial condition.\nAssimilation problems can be approached from many directions (depending on your background/preferences):\n\ncontrol theory;\nvariational calculus;\nstatistical estimation theory;\nprobability theory,\nstochastic differential equations.\n\nNewer approaches (discussed later): nudging methods, reduced methods, ensemble methods and hybrid methods that combine variational and statistical approaches, Machine/Deep Learning based approaches."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-approaches-1",
    "href": "slides/week-01/02-basics.html#introduction-approaches-1",
    "title": "Basics",
    "section": "Introduction: approaches",
    "text": "Introduction: approaches\n\nNavigation: important application of the Kalman filter.\nRemote sensing: satellite data.\nGeophysics: seismic exploration, geophysical prospecting, earthquake prediction.\nAir and noise pollution, source estimation\nWeather forecasting.\nClimatology. Global warming.\nEpidemiology.\nForest fire evolution.\nFinance."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-nonlinearity",
    "href": "slides/week-01/02-basics.html#introduction-nonlinearity",
    "title": "Basics",
    "section": "Introduction: nonlinearity",
    "text": "Introduction: nonlinearity\nThe problems of data assimilation (in particular) and inverse problems in general arise from:\n\nThe nonlinear dynamics of the physical model equations.\nThe nonlinearity of the inverse problem."
  },
  {
    "objectID": "slides/week-01/02-basics.html#introduction-iterative-process",
    "href": "slides/week-01/02-basics.html#introduction-iterative-process",
    "title": "Basics",
    "section": "Introduction: iterative process",
    "text": "Introduction: iterative process\n\nClosely related to\n\nthe inference cycle\nmachine learning…"
  },
  {
    "objectID": "slides/week-01/02-basics.html#motivational-example-digital-twin-for-geological-co2-storage",
    "href": "slides/week-01/02-basics.html#motivational-example-digital-twin-for-geological-co2-storage",
    "title": "Basics",
    "section": "Motivational Example — Digital Twin for Geological CO2 storage",
    "text": "Motivational Example — Digital Twin for Geological CO2 storage\n\n\nPlume development & associated time-lapse seismic images\n\n\n\n\nRecovery of the CO2 plume\n\n\nSee for SLIM website for our research on Geological CO2 Storage."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-1",
    "href": "slides/week-01/02-basics.html#section-1",
    "title": "Basics",
    "section": "",
    "text": "FORWARD AND INVERSE PROBLEMS"
  },
  {
    "objectID": "slides/week-01/02-basics.html#inverse-problems",
    "href": "slides/week-01/02-basics.html#inverse-problems",
    "title": "Basics",
    "section": "Inverse problems",
    "text": "Inverse problems\n\n\n\n\nfrom Asch (2022)\n\n\nIngredients of an inverse problem: the physical reality (top) and the direct mathematical model (bottom). The inverse problem uses the difference between the model- predicted observations, u (calculated at the receiver array points \\(x_r\\)), and the real observations measured on the array, in order to find the unknown model parameters, m, or the source s (or both)."
  },
  {
    "objectID": "slides/week-01/02-basics.html#forward-and-inverse-problems",
    "href": "slides/week-01/02-basics.html#forward-and-inverse-problems",
    "title": "Basics",
    "section": "Forward and inverse problems",
    "text": "Forward and inverse problems\n\n\n\n\n\n\n\nConsider a parameter-dependent dynamical system, \\[\\frac{d\\mathbf{z}}{dt}=g(t,\\mathbf{z};\\boldsymbol{\\theta}),\\qquad \\mathbf{z}(t_{0})=\\mathbf{z}_{0},\\] with \\(g\\) known, \\(\\mathbf{z}_0\\) the initial condition, \\(\\boldsymbol{\\theta}\\in\\Theta\\) parameters of the system, \\(\\mathbf{z}(t)\\in\\mathbb{R}^{k}\\) the system’s state."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-2",
    "href": "slides/week-01/02-basics.html#section-2",
    "title": "Basics",
    "section": "",
    "text": "Forward problem: Given \\(\\boldsymbol{\\theta},\\) \\(\\mathbf{z}_{0},\\)find \\(\\mathbf{z}(t)\\) for \\(t\\ge t_{0}.\\)\nInverse problem: Given \\(\\mathbf{z}(t)\\) for \\(t\\ge t_{0},\\) find \\(\\boldsymbol{\\theta}\\in\\Theta.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#observations",
    "href": "slides/week-01/02-basics.html#observations",
    "title": "Basics",
    "section": "Observations",
    "text": "Observations\nObservation equation: \\[f(t,\\boldsymbol{\\theta})=\\mathcal{H}\\mathbf{z}(t,\\boldsymbol{\\theta}),\\] where \\(\\mathcal{H}\\) is the observation operator — to account for the fact that observations are never completely known (in space-time).\nUsually we have a finite number of discrete (space-time) observations \\[\\left\\{\\widetilde{y}_{j}\\right\\} _{j=1}^{n},\\] where \\[\\widetilde{y}_{j}\\approx f(t_{j},\\boldsymbol{\\theta}).\\]"
  },
  {
    "objectID": "slides/week-01/02-basics.html#noise-free-and-noise-data",
    "href": "slides/week-01/02-basics.html#noise-free-and-noise-data",
    "title": "Basics",
    "section": "Noise-free and noise data",
    "text": "Noise-free and noise data\nNoise-free: \\[\\widetilde{y}_{j}=f(t_{j},\\boldsymbol{\\theta})\\]\nNoisy Data: \\[\\widetilde{y}_{j}=f(t_{j},\\boldsymbol{\\theta})+\\varepsilon_{j},\\] where \\(\\varepsilon_{j}\\) is error and requires that we introduce variability/uncertainty into the modeling and analysis."
  },
  {
    "objectID": "slides/week-01/02-basics.html#well-posedness",
    "href": "slides/week-01/02-basics.html#well-posedness",
    "title": "Basics",
    "section": "Well-posedness",
    "text": "Well-posedness\n\nExistence\nUniqueness\nContinuous dependence of solutions on observations.\n\n✓ The existence and uniqueness together are also known as “identifiability”.\n✓ The continuous dependence is related to the “stability” of the inverse problem."
  },
  {
    "objectID": "slides/week-01/02-basics.html#well-posednessmath",
    "href": "slides/week-01/02-basics.html#well-posednessmath",
    "title": "Basics",
    "section": "Well-posedness—math",
    "text": "Well-posedness—math\n\nDefinition 2 Let \\(X\\) and \\(Y\\) be two normed spaces and let \\(K\\::\\:X\\rightarrow Y\\) be a linear or nonlinear map between the two. The problem of finding \\(x\\) given \\(y\\) such that \\[Kx=y\\] is well-posed if the following three properties hold:\n\n\nWP1\n\nExistence—for every \\(y\\in Y\\) there is (at least) one solution \\(x\\in X\\) such that \\(Kx=y.\\)\n\nWP2\n\nUniqueness—for every \\(y\\in Y\\) there is at most one \\(x\\in X\\) such that \\(Kx=y.\\)\n\nWP3\n\nStability— the solution \\(x\\) depends continuously on the data \\(y\\) in that for every sequence \\(\\left\\{ x_{n}\\right\\} \\subset X\\) with \\(Kx_{n}\\rightarrow Kx\\) as \\(n\\rightarrow\\infty,\\) we have that \\(x_{n}\\rightarrow x\\) as \\(n\\rightarrow\\infty.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-3",
    "href": "slides/week-01/02-basics.html#section-3",
    "title": "Basics",
    "section": "",
    "text": "This concept of ill-posedness will help us to understand and distinguish between direct and inverse models.\nIt will provide us with basic comprehension of the methods and algorithms that will be used to solve inverse problems.\nFinally, it will assist us in the analysis of “what went wrong?” when we attempt to solve the inverse problems."
  },
  {
    "objectID": "slides/week-01/02-basics.html#ill-posedness-of-inverse-problems",
    "href": "slides/week-01/02-basics.html#ill-posedness-of-inverse-problems",
    "title": "Basics",
    "section": "Ill-posedness of inverse problems",
    "text": "Ill-posedness of inverse problems\nMany inverse problems are ill-posed!\nSimplest case: one observation \\(\\widetilde{y}\\) for \\(f(\\theta)\\) and we need to find the pre-image \\[\\theta^{*}=f^{-1}(\\widetilde{y})\\] for a given \\(\\widetilde{y}.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#simplest-case",
    "href": "slides/week-01/02-basics.html#simplest-case",
    "title": "Basics",
    "section": "Simplest case",
    "text": "Simplest case\n\nNon-existence: there is no \\(\\theta_{3}\\) such that \\(f(\\theta_{3})=y_{3}\\)\nNon-uniqueness: \\(y_{j}=f(\\theta_{j})=f(\\widetilde{\\theta}_{j})\\) for \\(j=1,2.\\)\nLack of continuity of inverse map: \\(\\left|y_{1}-y_{2}\\right|\\) small \\(\\nRightarrow\\left|f^{-1}(y_{1})-f^{-1}(y_{2})\\right|=\\left|\\theta_{1}-\\widetilde{\\theta}_{2}\\right|\\) small."
  },
  {
    "objectID": "slides/week-01/02-basics.html#why-is-this-so-important",
    "href": "slides/week-01/02-basics.html#why-is-this-so-important",
    "title": "Basics",
    "section": "Why is this so important?",
    "text": "Why is this so important?\nCouldn’t we just apply a good least squares algorithm (for example) to find the best possible solution?\n\nDefine \\(J(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}\\) for a given \\(y_{1}\\)\nApply a standard iterative scheme, such as direct search or gradient-based minimization, to obtain a solution\nNewton’s method: \\[\\theta^{k+1}=\\theta^{k}-\\left[J'(\\theta^{k})\\right]^{-1}J(\\theta^{k})\\]\nLeads to highly unstable behavior because of ill-posedness"
  },
  {
    "objectID": "slides/week-01/02-basics.html#what-went-wrong",
    "href": "slides/week-01/02-basics.html#what-went-wrong",
    "title": "Basics",
    "section": "What went wrong",
    "text": "What went wrong\n✗ This behavior is not the fault of steepest descent algorithms.\n✗ It is a manifestation of the inherent ill-posedness of the problem.\n✗ How to fix this problem is the subject of much research over the past 50 years!!!\nMany remedies (fortunately) exist…\n\nexplicit and implicit constrained optimizations\nregularization and penalization\nmachine learning…"
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularization",
    "href": "slides/week-01/02-basics.html#tikhonov-regularization",
    "title": "Basics",
    "section": "Tikhonov regularization",
    "text": "Tikhonov regularization\nIdea: is to replace the ill-posed problem for \\(J(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}\\) by a “nearby” problem for \\[J_{\\beta}(\\theta)=\\left|y_{1}-f(\\theta)\\right|^{2}+\\beta\\left|\\theta-\\theta_{0}\\right|^{2}\\] where \\(\\beta\\) is “suitably chosen” regularization/penalization parametersee below for details.\n\nWhen it is done correctly, TR provides convexity and compactness.\nEven when done correctly, it modifies the problem and new solutions may be far from the original ones.\nIt is not trivial to regularize correctly or even to know if you have succeeded…"
  },
  {
    "objectID": "slides/week-01/02-basics.html#non-uniqueness-seismic-travel-time-tomography",
    "href": "slides/week-01/02-basics.html#non-uniqueness-seismic-travel-time-tomography",
    "title": "Basics",
    "section": "Non uniqueness: seismic travel-time tomography",
    "text": "Non uniqueness: seismic travel-time tomography\n\n\n\n\n\nA signal seismic ray passes through a 2-parameter block model.\n\nunknowns are the 2 block slownesses (inverse of seismic velocity) \\(\\left(\\Delta s_{1},\\Delta s_{2}\\right)\\)\ndata is the observed travel time of the ray, \\(\\Delta t_{1}\\)\nmodel is the linearized travel time equation, \\(\\Delta t_{1}=l_{1}\\Delta s_{1}+l_{2}\\Delta s_{2}\\) where \\(l_{j}\\) is the length of the ray in the \\(j\\)-th block.\n\nClearly we have one equation for two unknowns and hence there is no unique solution."
  },
  {
    "objectID": "slides/week-01/02-basics.html#inverse-problems-general-formulation",
    "href": "slides/week-01/02-basics.html#inverse-problems-general-formulation",
    "title": "Basics",
    "section": "Inverse Problems: General Formulation",
    "text": "Inverse Problems: General Formulation\nAll inverse problems share a common formulation.\nLet the model parameters1 be a vector (in general, a multivariate random variable), \\(\\mathbf{m},\\) and the data be \\(\\mathbf{d},\\) \\[\\begin{aligned}\n    \\mathbf{m} & =\\left(m_{1},\\ldots,m_{p}\\right)\\in\\mathcal{M},\\\\\n    \\mathbf{d} & =\\left(d_{1},\\ldots,d_{n}\\right)\\in\\mathcal{D},\n    \\end{aligned}\\]\nwhere \\(\\mathcal{M}\\) and \\(\\mathcal{D}\\) are the corresponding model parameter space and data space.\nThe mapping \\(G\\colon\\mathcal{M}\\rightarrow\\mathcal{D}\\) is defined by the direct (or forward) model\n\\[\\mathbf{d}=g(\\mathbf{m}), \\qquad(1)\\] where\nApplied mathematicians often call the equation \\(G(m)=d\\) a mathematical model and \\(m\\) the parameters. Other scientists call \\(G\\) the forward operator and \\(m\\) the model. We will adopt the more mathematical convention, where \\(m\\) will be referred to as the model parameters, \\(G\\) the model and \\(d\\) the data."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-4",
    "href": "slides/week-01/02-basics.html#section-4",
    "title": "Basics",
    "section": "",
    "text": "\\(g\\in G\\) is an operator that describes the “physical” model and can take numerous forms, such as algebraic equations, differential equations, integral equations, or linear systems.\n\nThen we can add the observations or predictions, \\(\\mathbf{y}=(y_{1},\\ldots,y_{r}),\\) corresponding to the mapping from data space into observation space, \\(H\\colon\\mathcal{D}\\rightarrow\\mathcal{Y},\\) and described by \\[\\mathbf{y}=h(\\mathbf{d})=h\\left(g(\\mathbf{m})\\right),\\] where\n\n\\(h\\in H\\) is the observation operator, usually some projection into an observable subset of \\(\\mathcal{D}.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-5",
    "href": "slides/week-01/02-basics.html#section-5",
    "title": "Basics",
    "section": "",
    "text": "Note\n\n\nIn addition, there will be some random noise in the system, usually modeled as additive noise, giving the more realistic, stochastic direct model \\[\\mathbf{d}=g(\\mathbf{m})+\\mathbf{\\epsilon},\\label{eq:stat-inv-pb} \\qquad(2)\\] where \\(\\mathbf{\\epsilon}\\) is a random vector."
  },
  {
    "objectID": "slides/week-01/02-basics.html#inverse-problemsclassification",
    "href": "slides/week-01/02-basics.html#inverse-problemsclassification",
    "title": "Basics",
    "section": "Inverse Problems—Classification",
    "text": "Inverse Problems—Classification\nWe can now classify inverse problems as:\n\ndeterministic inverse problems that solve 1 for \\(\\mathbf{m},\\)\nstatistical inverse problems that solve 2 for \\(\\mathbf{m}.\\)\n\nThe first class will be treated by linear algebra and optimization methods.\nThe latter can be treated by a Bayesian (filtering) approach, and by weighted least-squares, maximum likelihood and DA techniques\nBoth classes can be further broken down into:\n\nLinear inverse problems, where 1 or 2 are linear equations. These include linear systems that are often the result of discretizing (partial) differential equations and integral equations.\nNonlinear inverse problems where the algebraic or differential operators are nonlinear."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-6",
    "href": "slides/week-01/02-basics.html#section-6",
    "title": "Basics",
    "section": "",
    "text": "Finally, since most inverse problems cannot be solved explicitly, computational methods are indispensable for their solution, see sections 8.4 and 8.5 of Asch (2022)\nAlso note that we will be inverting here between the model and data spaces, that are usually both of high dimension and thus this model-based inversion will invariably be computationally expensive.\nThis will motivate us to employ\n\ninversion between the data and observation spaces in a purely data-driven approach, using machine learning methods\nthis aspect will be treated later during this course"
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularizationintroduction",
    "href": "slides/week-01/02-basics.html#tikhonov-regularizationintroduction",
    "title": "Basics",
    "section": "Tikhonov Regularization—Introduction",
    "text": "Tikhonov Regularization—Introduction\nTikhonov regularization (TR) is probably the most widely used method for regularizing ill-posed, discrete and continuous inverse problems.\n\n\n\n\n\n\nNote\n\n\nNote that the LASSO and ridge regression methods—are special cases of TR.\n\n\n\nThe theory is the subject of entire books...\nRecall:\n\nthe objective of TR is to reduce, or remove, ill-posedness in optimization problems by modifying the objective function.\nthe three sources of ill-posedness: non-existence, non-uniqueness and sensitivity to perturbations.\nTR, in principle, addresses and alleviates all three sources of ill-posedness and is thus a vital tool for the solution of inverse problems."
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularizationformulation",
    "href": "slides/week-01/02-basics.html#tikhonov-regularizationformulation",
    "title": "Basics",
    "section": "Tikhonov Regularization—Formulation",
    "text": "Tikhonov Regularization—Formulation\nThe most general TR objective function is \\[\\mathcal{T}_{\\alpha}(\\mathbf{m};\\mathbf{d})=\\rho\\left(G(\\mathbf{m}),\\mathbf{d}\\right)+\\alpha J(\\mathbf{m}),\\] where\n\n\\(\\rho\\) is the data discrepancy functional that quantifies the difference between the model output and the measured data;\n\\(J\\) is the regularization functional that represents some desired quality of the sought for model parameters, usually smoothness;\n\\(\\alpha\\) is the regularization parameter that needs to be tuned, and determines the relative importance of the regularization term.\n\nEach domain, each application and each context will require specific choices of these three items, and often we will have to rely either on previous experience, or on some sort of numerical experimentation (trial-and-error) to make a good choice.\nIn some cases there exist empirical algorithms, in particular for the choice of \\(\\alpha.\\)"
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularizationdiscrepancy",
    "href": "slides/week-01/02-basics.html#tikhonov-regularizationdiscrepancy",
    "title": "Basics",
    "section": "Tikhonov Regularization—Discrepancy",
    "text": "Tikhonov Regularization—Discrepancy\nThe most common discrepancy functions are:\n\nleast-squares, \\[\\rho_{\\mathrm{LS}}(\\mathbf{d}_{1},\\mathbf{d}_{2})=\\frac{1}{2}\\Vert\\mathbf{d}_{1}-\\mathbf{d}_{2}\\Vert_{2}^{2},\\]\n1-norm, \\[\\rho_{1}(\\mathbf{d}_{1},\\mathbf{d}_{2})=\\vert\\mathbf{d}_{1}-\\mathbf{d}_{2}\\vert,\\]\nKullback-Leibler distance, \\[\\rho_{\\mathrm{KL}}(d_{1},d_{2})=\\left\\langle d_{1},\\log(d_{1}/d_{2})\\right\\rangle ,\\] where \\(d_{1}\\) and \\(d_{2}\\) are considered here as probability density functions. This discrepancy is valid in the Bayesian context."
  },
  {
    "objectID": "slides/week-01/02-basics.html#tikhonov-regularization-1",
    "href": "slides/week-01/02-basics.html#tikhonov-regularization-1",
    "title": "Basics",
    "section": "Tikhonov Regularization",
    "text": "Tikhonov Regularization\nThe most common regularization functionals are derivatives of order one or two.\n\nGradient smoothing: \\[J_{1}(\\mathbf{m})=\\frac{1}{2}\\Vert\\nabla\\mathbf{m}\\Vert_{2}^{2},\\] where \\(\\nabla\\) is the gradient operator of first-order derivatives of the elements of \\(\\mathbf{m}\\) with respect to each of the independent variables.\nLaplacian smoothing: \\[J_{2}(\\mathbf{m})=\\frac{1}{2}\\Vert\\nabla^{2}\\mathbf{m}\\Vert_{2}^{2},\\] where \\(\\nabla^{2}=\\nabla\\cdot\\nabla\\) is the Laplacian operator defined as the sum of all second-order derivatives of \\(\\mathbf{m}\\) with respect to each of the independent variables."
  },
  {
    "objectID": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter",
    "href": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter",
    "title": "Basics",
    "section": "TR—Computing the Regularization Parameter",
    "text": "TR—Computing the Regularization Parameter\nOnce the data discrepancy and regularization functionals have been chosen, we need to tune the regularization parameter, \\(\\alpha.\\)\nWe have here, similarly to the bias-variance trade-off of ML Lectures, a competition between the discrepancy error and the magnitude of the regularization term.\nWe need to choose, the best compromise between the two.\nWe will briefly present three frequently used approaches:\n\nL-curve method.\nDiscrepancy principle.\nCross-validation and LOOCV."
  },
  {
    "objectID": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter-1",
    "href": "slides/week-01/02-basics.html#trcomputing-the-regularization-parameter-1",
    "title": "Basics",
    "section": "TR—Computing the Regularization Parameter",
    "text": "TR—Computing the Regularization Parameter\n\n\n\n\n\n\nFigure 1\n\n\n\nThe L-curve criterion is an empirical method for picking a value of \\(\\alpha.\\)\n\nsince \\(e_{m}(\\alpha)=\\Vert\\mathbf{m}\\Vert_{2}\\) is a strictly decreasing function of \\(\\alpha\\) and \\(e_{d}(\\alpha)=\\Vert G\\mathbf{m}-\\mathbf{d}\\Vert_{2}\\) is a strictly increasing one,\nwe plot \\(\\log e_{m}\\) against \\(\\log e_{d}\\) we will always obtain an L-shaped curve that has an “elbow” at the optimal value of \\(\\alpha=\\alpha_{L},\\) or at least at a good approximation of this optimal value see Figure 1."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-7",
    "href": "slides/week-01/02-basics.html#section-7",
    "title": "Basics",
    "section": "",
    "text": "This trade-off curve gives us a visual recipe for choosing the regularization parameter, reminiscent of the bias-variance trade off\nThe range of values of \\(\\alpha\\) for which one should plot the curve has to be determined by either trial-and-error, previous experience, or a balancing of the two terms in the TR functional.\n\nThe discrepancy principle\n\nchoose the value of \\(\\alpha=\\alpha_{D}\\) such that the residual error (first term) is equal to an a priori bound, \\(\\delta,\\) that we would like to attain.\non the L-curve, this corresponds to the intersection with the vertical line at this bound, as shown in Figure 1.\na good approximation for the bound is to put \\(\\delta=\\sigma\\sqrt{n},\\) where \\(\\sigma^{2}\\) is the variance and \\(n\\) the number of observations.1 This can be thought of as the noise level of the data.\n\nThis is strictly valid under the hypothesis of i.i.d. Gaussian noise."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-8",
    "href": "slides/week-01/02-basics.html#section-8",
    "title": "Basics",
    "section": "",
    "text": "the discrepancy principle is also related to regularization by the truncated singular value decomposition (TSVD), in which case the truncation level implicitly defines the regularization parameter.\n\nCross-validation, as we explained in ML Lectures, is a way of using the observations themselves to estimate a parameter.\n\nWe then employ the classical approach of either LOOCV or \\(k\\)-fold cross validation, and choose the value of \\(\\alpha\\) that minimizes the RSS (Residual Sum of Squares) of the test sets.\nIn order to reduce the computational cost, a generalized cross validation (GCV) method can be used."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-9",
    "href": "slides/week-01/02-basics.html#section-9",
    "title": "Basics",
    "section": "",
    "text": "DATA ASSIMILATION"
  },
  {
    "objectID": "slides/week-01/02-basics.html#definitions-and-notation",
    "href": "slides/week-01/02-basics.html#definitions-and-notation",
    "title": "Basics",
    "section": "Definitions and notation",
    "text": "Definitions and notation\nAnalysis is the process of approximating the true state of a physical system at a given time\nAnalysis is based on:\n\nobservational data,\na model of the physical system,\nbackground information on initial and boundary conditions.\n\nAn analysis that combines time-distributed observations and a dynamic model is called data assimilation."
  },
  {
    "objectID": "slides/week-01/02-basics.html#standard-notation",
    "href": "slides/week-01/02-basics.html#standard-notation",
    "title": "Basics",
    "section": "Standard notation",
    "text": "Standard notation\nA discrete model for the evolution of a physical (atmospheric, oceanic, etc.) system from time \\(t_{k}\\) to time \\(t_{k+1}\\) is described by a dynamic, state equation\n\\[\\mathbf{x}^{f}(t_{k+1})=M_{k+1}\\left[\\mathbf{x}^{f}(t_{k})\\right], \\qquad(3)\\]\n\n\\(\\mathbf{x}\\) is the model’s state vector of dimension \\(n,\\)\n\\(M\\) is the corresponding dynamics operator (finite difference or finite element discretization), which can be time dependent.\n\nThe error covariance matrix associated with \\(\\mathbf{x}\\) is given by \\(\\mathbf{P}\\) since the true state will differ from the simulated state (Equation 3) by random or systematic errors.\nObservations, or measurements, at time \\(t_{k}\\) are defined by"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-10",
    "href": "slides/week-01/02-basics.html#section-10",
    "title": "Basics",
    "section": "",
    "text": "\\[\\mathbf{y}_{k}^{\\mathrm{o}}=H_{k}\\left[\\mathbf{x}^{t}(t_{k})\\right]+\\varepsilon_{k}^{\\mathrm{o}},\\]\n\n\\(H\\) is an observation operator that can be time dependent\n\\(\\varepsilon_k^{\\mathrm{o}}\\) is a white noise process zero mean and covariance matrix \\(\\mathbf{R}\\) (instrument errors and representation errors due to the discretization)\nobservation vector \\(\\mathbf{y}_{k}^{\\mathrm{o}}=\\mathbf{y}^{\\mathrm{o}}(t_{k})\\) has dimension \\(p_{k}\\) (usually \\(p_{k}\\ll n.\\) )\n\nSubscripts are used to denote the discrete time index, the corresponding spatial indices or the vector with respect to which an error covariance matrix is defined.\nSuperscripts refer to the nature of the vectors/matrices\n\n“a” for analysis, “b” for background (or ‘initial/first guess’),\n“f” for forecast, “o” for observation, and\n“t” for the (unknown) true state.\n\nAn analysis that combines time-distributed observations and a dynamic model is called data assimilation."
  },
  {
    "objectID": "slides/week-01/02-basics.html#standard-notationcontinuous-system",
    "href": "slides/week-01/02-basics.html#standard-notationcontinuous-system",
    "title": "Basics",
    "section": "Standard notation—continuous system",
    "text": "Standard notation—continuous system\nNow let us introduce the continuous system. In fact, continuous time simplifies both the notation and the theoretical analysis of the problem. For a finite-dimensional system of ordinary differential equations, the sate and observation equations become \\[\\dot{\\mathbf{x}}^{\\mathrm{f}}=\\mathcal{M}(\\mathbf{x}^{\\mathrm{f}},t)%\\mathbf{\\dot{\\xf}}=\\mathcal{M}(\\xf,t)\\] and \\[\\mathbf{y}^{\\mathrm{o}}(t)=\\mathcal{H}(\\mathbf{x}^{\\mathrm{t}},t)+\\boldsymbol{\\epsilon},\\] where \\(\\dot{\\left(\\,\\right)}=\\mathrm{d}/\\mathrm{d}t,\\) \\(\\mathcal{M}\\) and \\(\\mathcal{H}\\) are nonlinear operators in continuous time for the model and the observation respectively.\nThis implies that \\(\\mathbf{x},\\) \\(\\mathbf{y},\\) and \\(\\boldsymbol{\\epsilon}\\) are also continuous-in-time functions.\n\nFor PDEs, where there is in addition a dependence on space, attention must be paid to the function spaces, especially when performing variational analysis."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-11",
    "href": "slides/week-01/02-basics.html#section-11",
    "title": "Basics",
    "section": "",
    "text": "With a PDE model, the field (state) variable is commonly denoted by \\(\\boldsymbol{u}(\\mathbf{x},t),\\) where \\(\\mathbf{x}\\) represents the space variables (no longer the state variable as above!), and the model dynamics is now a nonlinear partial differential operator, \\[\\mathcal{M}=\\mathcal{M}\\left[\\partial_{\\mathbf{x}}^{\\alpha},\\boldsymbol{u}(\\mathbf{x},t),\\mathbf{x},t\\right]\\] with \\(\\partial_{\\mathbf{x}}^{\\alpha}\\) denoting the partial derivatives with respect to the space variables of order up to \\(\\left|\\alpha\\right|\\le m\\) where \\(m\\) is usually equal to two and in general varies between one and four."
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-12",
    "href": "slides/week-01/02-basics.html#section-12",
    "title": "Basics",
    "section": "",
    "text": "CONCLUSIONS"
  },
  {
    "objectID": "slides/week-01/02-basics.html#section-13",
    "href": "slides/week-01/02-basics.html#section-13",
    "title": "Basics",
    "section": "",
    "text": "Data assimilation requires not only the observations and a background, but also knowledge of:\n\nerror statistics(background, observation, model, etc.)\nphysics (forecast model, model relating observed to retrieved variables, etc.).\n\nThe challenge of data assimilation is in combining our stochastic knowledge with our physical knowledge."
  },
  {
    "objectID": "slides/week-01/02-basics.html#open-source-software",
    "href": "slides/week-01/02-basics.html#open-source-software",
    "title": "Basics",
    "section": "Open-source software",
    "text": "Open-source software\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen.\nDataAssim.jl\nEnKF.jl"
  },
  {
    "objectID": "slides/week-01/02-basics.html#references",
    "href": "slides/week-01/02-basics.html#references",
    "title": "Basics",
    "section": "References",
    "text": "References\n\nK. Law, A. Stuart, K. Zygalakis. Data Assimilation. A Mathematical Introduction. Springer, 2015.\nG. Evensen. Data assimilation, The Ensemble Kalman Filter, 2nd ed., Springer, 2009.\nA. Tarantola. Inverse problem theory and methods for model parameter estimation. SIAM. 2005.\nO. Talagrand. Assimilation of observations, an introduction. J. Meteorological Soc. Japan, 75, 191, 1997.\nF.X. Le Dimet, O. Talagrand. Variational algorithms for analysis and assimilation of meteorological observations: theoretical aspects. Tellus, 38(2), 97, 1986.\nJ.-L. Lions. Exact controllability, stabilization and perturbations for distributed systems. SIAM Rev., 30(1):1, 1988.\nJ. Nocedal, S.J. Wright. Numerical Optimization. Springer, 2006.\nF. Tröltzsch. Optimal Control of Partial Differential Equations. AMS, 2010.\n\n\n\n\nAsch, Mark. 2022. A Toolbox for Digital Twins: From Model-Based to Data-Driven. SIAM."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#credits",
    "href": "slides/week-07/09a-DL-uncertainty.html#credits",
    "title": "Simulation-based Inference",
    "section": "Credits",
    "text": "Credits\nSlides are adapted from lectures developed by Gilles Louppe at University of Liege, Belgium under the BSD 3-Clause License\n\nlectures 10, 11, and 12 of INFO8010 - Deep Learning\nand of lecture materials of SSI 2023"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#today",
    "href": "slides/week-07/09a-DL-uncertainty.html#today",
    "title": "Simulation-based Inference",
    "section": "Today",
    "text": "Today\nHow to model uncertainty in deep learning?\n\nUncertainty\nAleatoric uncertainty\nEpistemic uncertainty"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section",
    "href": "slides/week-07/09a-DL-uncertainty.html#section",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "“Every time a scientific paper presents a bit of data, it’s accompanied by an error bar — a quiet but insistent reminder that no knowledge is complete or perfect. It’s a calibration of how much we trust what we think we know.”\nCarl Sagan"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#uncertainty",
    "href": "slides/week-07/09a-DL-uncertainty.html#uncertainty",
    "title": "Simulation-based Inference",
    "section": "Uncertainty",
    "text": "Uncertainty\nUncertainty1 refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown.\nUncertainty arises in partially observable or stochastic environments, as well as due to ignorance, indolence, or both.\nWhy is uncertainty important?\nAccounting for uncertainty leads to optimal decisions. Not accounting for uncertainty leads to suboptimal, wrong, or even catastrophic decisions.\n[Credits: Wikipedia, 2023.]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#aleatoric-uncertainty",
    "href": "slides/week-07/09a-DL-uncertainty.html#aleatoric-uncertainty",
    "title": "Simulation-based Inference",
    "section": "Aleatoric uncertainty",
    "text": "Aleatoric uncertainty\nAleatoric uncertainty captures noise inherent in the observations. For example, sensor noise or motion noise result in uncertainty.\nThis uncertainty cannot be reduced with more data. However, aleatoric uncertainty could be reduced with better measurements.\nAleatoric uncertainty can further be categorized into:\n\nHomoscedastic uncertainty, which relates to the uncertainty that a particular task might cause. It stays constant for different inputs.\nHeteroscedastic uncertainty, which depends on the inputs to the model, with some inputs potentially having more noisy outputs than others."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#neural-density-estimation",
    "href": "slides/week-07/09a-DL-uncertainty.html#neural-density-estimation",
    "title": "Simulation-based Inference",
    "section": "Neural density estimation",
    "text": "Neural density estimation\nConsider training data \\((\\mathbf{x}, y) \\sim p(\\mathbf{x}, y)\\), with\n\n\\(\\mathbf{x} \\in \\mathbb{R}^p\\),\n\\(y \\in \\mathbb{R}\\).\n\nWe do not wish to learn a function \\(\\hat{y} = f(\\mathbf{x})\\), which would only produce point estimates.\nInstead we want to learn the full conditional density \\[p(y|\\mathbf{x}).\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#nn-with-gaussian-output-layer",
    "href": "slides/week-07/09a-DL-uncertainty.html#nn-with-gaussian-output-layer",
    "title": "Simulation-based Inference",
    "section": "NN with Gaussian output layer",
    "text": "NN with Gaussian output layer\nWe can model aleatoric uncertainty in the output by modeling the conditional distribution as a Gaussian distribution, \\[p(y|\\mathbf{x}) = \\mathcal{N}(y; \\mu(\\mathbf{x}), \\sigma^2(\\mathbf{x})),\\] where \\(\\mu(\\mathbf{x})\\) and \\(\\sigma^2(\\mathbf{x})\\) are parametric functions to be learned, such as neural networks.\n\n\n\n\n\n\nNote\n\n\nThe Gaussian distribution is a modeling choice. Other parametric distributions can be used."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-2",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-2",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Case 1: Homoscedastic aleatoric uncertainty"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-3",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-3",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "We have,\n\\[\\begin{aligned}\n&\\arg \\max_{\\theta,\\sigma^2} p(\\mathbf{d}|\\theta,\\sigma^2) \\\\\\\\\n&= \\arg \\max_{\\theta,\\sigma^2} \\prod_{\\mathbf{x}_i, y_i \\in \\mathbf{d}} p(y_i|\\mathbf{x}_i, \\theta,\\sigma^2) \\\\\\\\\n&= \\arg \\max_{\\theta,\\sigma^2} \\prod_{\\mathbf{x}_i, y_i \\in \\mathbf{d}} \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(y_i-\\mu(\\mathbf{x}_i))^2}{2\\sigma^2}\\right) \\\\\\\\\n&= \\arg \\min_{\\theta,\\sigma^2} \\sum_{\\mathbf{x}_i, y_i \\in \\mathbf{d}}  \\frac{(y_i-\\mu(\\mathbf{x}_i))^2}{2\\sigma^2} + \\log(\\sigma) + C\n\\end{aligned}\\]\nWhat if \\(\\sigma^2\\) was fixed?"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-4",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-4",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Case 2: heteroscedastic aleatoric uncertainty"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-5",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-5",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Same as for the homoscedastic case, except that that \\(\\sigma^2\\) is now a function of \\(\\mathbf{x}_i\\): \\[\\begin{aligned}\n&\\arg \\max_{\\theta} p(\\mathbf{d}|\\theta) \\\\\\\\\n&= \\arg \\max_{\\theta} \\prod_{\\mathbf{x}_i, y_i \\in \\mathbf{d}} p(y_i|\\mathbf{x}_i, \\theta) \\\\\\\\\n&= \\arg \\max_{\\theta} \\prod_{\\mathbf{x}_i, y_i \\in \\mathbf{d}} \\frac{1}{\\sqrt{2\\pi} \\sigma(\\mathbf{x}_i)} \\exp\\left(-\\frac{(y_i-\\mu(\\mathbf{x}_i))^2}{2\\sigma^2(\\mathbf{x}_i)}\\right) \\\\\\\\\n&= \\arg \\min_{\\theta} \\sum_{\\mathbf{x}_i, y_i \\in \\mathbf{d}}  \\frac{(y_i-\\mu(\\mathbf{x}_i))^2}{2\\sigma^2(\\mathbf{x}_i)} + \\log(\\sigma(\\mathbf{x}_i)) + C\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#gaussian-mixture-model",
    "href": "slides/week-07/09a-DL-uncertainty.html#gaussian-mixture-model",
    "title": "Simulation-based Inference",
    "section": "Gaussian mixture model",
    "text": "Gaussian mixture model\nModelling \\(p(y|\\mathbf{x})\\) as a unimodal (Gaussian) distribution can be inadequate since the conditional distribution may be multimodal.\nA Gaussian mixture model (GMM) defines instead \\(p(y|\\mathbf{x})\\) as a mixture of \\(K\\) Gaussian components, \\[p(y|\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(y;\\mu_k, \\sigma_k^2),\\] where \\(0 \\leq \\pi_k \\leq 1\\) for all \\(k\\) and \\(\\sum_{k=1}^K \\pi_k = 1\\).\n\nGaussian mixture"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#mixture-network",
    "href": "slides/week-07/09a-DL-uncertainty.html#mixture-network",
    "title": "Simulation-based Inference",
    "section": "Mixture Network",
    "text": "Mixture Network\nA mixture density network (MDN) is a neural network implementation of the Gaussian mixture model.\n\n\n\n\nMDN"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#example",
    "href": "slides/week-07/09a-DL-uncertainty.html#example",
    "title": "Simulation-based Inference",
    "section": "Example",
    "text": "Example\nLet us consider training data generated randomly as \\[y_i = \\mathbf{x}_i + 0.3\\sin(4\\pi \\mathbf{x}_i) + \\epsilon_i\\] with \\(\\epsilon_i \\sim \\mathcal{N}\\)."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#mixture-density-networks",
    "href": "slides/week-07/09a-DL-uncertainty.html#mixture-density-networks",
    "title": "Simulation-based Inference",
    "section": "Mixture Density Networks",
    "text": "Mixture Density Networks\n\n\n\n\nCredits: David Ha, Mixture Density Networks, 2015.\n\n\n\nThe data can be fit with a 2-layer network producing point estimates for \\(y\\) (demo)."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#multi-modality-issue",
    "href": "slides/week-07/09a-DL-uncertainty.html#multi-modality-issue",
    "title": "Simulation-based Inference",
    "section": "Multi modality issue",
    "text": "Multi modality issue\n\n\n\n\nCredits: David Ha, Mixture Density Networks, 2015.\n\n\nIf we flip \\(\\mathbf{x}_i\\) and \\(y_i\\), the network faces issues since for each input, there are multiple outputs that can work. It produces an average of the correct values (demo)."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#mixture-density-network",
    "href": "slides/week-07/09a-DL-uncertainty.html#mixture-density-network",
    "title": "Simulation-based Inference",
    "section": "Mixture Density Network",
    "text": "Mixture Density Network\n\n\n\n\nCredits: David Ha, Mixture Density Networks, 2015.\n\n\nA mixture density network models the data correctly, as it predicts for each input a distribution for the output, rather than a point estimate (demo)."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#normalizing-flows",
    "href": "slides/week-07/09a-DL-uncertainty.html#normalizing-flows",
    "title": "Simulation-based Inference",
    "section": "Normalizing flows",
    "text": "Normalizing flows\n\nAssume \\(p(\\mathbf{z})\\) is a uniformly distributed unit cube in \\(\\mathbb{R}^3\\) and \\(\\mathbf{x} = f(\\mathbf{z}) = 2\\mathbf{z}\\).\nSince the total probability mass must be conserved, \\[p(\\mathbf{x}=f(\\mathbf{z})) = p(\\mathbf{z})\\frac{V_\\mathbf{z}}{V_\\mathbf{x}}=p(\\mathbf{z}) \\frac{1}{8},\\quad\\text{where}\\frac{1}{8} = \\left| \\det \\left( \\begin{matrix}\n2 & 0 & 0 \\\\\\\\\n0 & 2 & 0 \\\\\\\\\n0 & 0 & 2\n\\end{matrix} \\right)\\right|^{-1}\\] represents the inverse determinant of the linear transformation \\(f\\)."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-6",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-6",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "What if \\(f\\) is non-linear?\n\nImage credits: Simon J.D. Prince, Understanding Deep Learning, 2023."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#change-of-variables-theorem",
    "href": "slides/week-07/09a-DL-uncertainty.html#change-of-variables-theorem",
    "title": "Simulation-based Inference",
    "section": "Change of variables theorem",
    "text": "Change of variables theorem\nIf \\(f\\) is non-linear,\n\nthe Jacobian \\(J_f(\\mathbf{z})\\) of \\(\\mathbf{x} = f(\\mathbf{z})\\) represents the infinitesimal linear transformation in the neighborhood of \\(\\mathbf{z}\\);\nif the function is a bijective map, then the mass must be conserved locally.\n\nTherefore, the local change of density yields \\[p(\\mathbf{x}=f(\\mathbf{z})) = p(\\mathbf{z})\\left| \\det J_f(\\mathbf{z}) \\right|^{-1}.\\]\nSimilarly, for \\(g = f^{-1}\\), we have \\[p(\\mathbf{x})=p(\\mathbf{z}=g(\\mathbf{x}))\\left| \\det J_g(\\mathbf{x}) \\right|.\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-7",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-7",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "What about the inverse \\(f^{-1}\\)?\n\nImage credits: Simon J.D. Prince, Understanding Deep Learning, 2023."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#normalizing-flows-1",
    "href": "slides/week-07/09a-DL-uncertainty.html#normalizing-flows-1",
    "title": "Simulation-based Inference",
    "section": "Normalizing flows",
    "text": "Normalizing flows\nA normalizing flow is a change of variable \\(f\\) that transforms a base distribution \\(p(\\mathbf{z})\\) into \\(p(\\mathbf{x})\\) by a series of invertible transformations.\n\nImage credits: Lilian Weng, 2018"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-8",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-8",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Formally,\n\n\\(f\\) is a composition \\(f=f_K \\circ ... \\circ f_1\\), where each \\(f_k\\) is an invertible neural transformation;\n\\(g_k = f^{-1}_k\\);\n\\(\\mathbf{z}_k = f_k(\\mathbf{z}_{k-1})\\), with \\(\\mathbf{z}_0 = \\mathbf{z}\\) and \\(\\mathbf{z}_K = \\mathbf{x}\\);\n\\(p(\\mathbf{z}_k) = p(\\mathbf{z}_{k-1} = g_k(\\mathbf{z}_k)) \\left| \\det J_{g_k}(\\mathbf{z}_k) \\right|\\).\n\n\nImage credits: Simon J.D. Prince, Understanding Deep Learning, 2023."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#example-coupling-layers",
    "href": "slides/week-07/09a-DL-uncertainty.html#example-coupling-layers",
    "title": "Simulation-based Inference",
    "section": "Example: coupling layers",
    "text": "Example: coupling layers\nAssume \\(\\mathbf{z} = (\\mathbf{z}_a, \\mathbf{z}_b)\\) and \\(\\mathbf{x} = (\\mathbf{x}_a, \\mathbf{x}_b)\\). Then,\n\nForward mapping \\(\\mathbf{x} = f(\\mathbf{z})\\): \\[\\mathbf{x}_a = \\mathbf{z}_a, \\quad \\mathbf{x}_b = \\mathbf{z}_b \\odot \\exp(s(\\mathbf{z}_a)) + t(\\mathbf{z}_a),\\]\nInverse mapping \\(\\mathbf{z} = g(\\mathbf{x})\\): \\[\\mathbf{z}_a = \\mathbf{x}_a, \\quad \\mathbf{z}_b = (\\mathbf{x}_b - t(\\mathbf{x}_a)) \\odot \\exp(-s(\\mathbf{x}_a)),\\]\n\nwhere \\(s\\) and \\(t\\) are arbitrary neural networks."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-9",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-9",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "For \\(\\mathbf{x} = (\\mathbf{x}_a, \\mathbf{x}_b)\\), the log-likelihood is \\[\\begin{aligned}\\log p(\\mathbf{x}) &= \\log p(\\mathbf{z} = g(\\mathbf{x})) \\left| \\det J_g(\\mathbf{x}) \\right|\\end{aligned}\\] where the Jacobian \\(J_g(\\mathbf{x}) = \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\) is a lower triangular matrix \\[\\left( \\begin{matrix}\n\\mathbf{I} & 0 \\\\\\\\\n\\frac{\\partial \\mathbf{z}_b}{\\partial \\mathbf{x}_a} & \\text{diag}(\\exp(-s(\\mathbf{x}_a))) \\end{matrix} \\right),\\] such that \\(\\left| \\det J_g(\\mathbf{x}) \\right| = \\prod_i \\exp(-s(\\mathbf{x}_a))_i = \\exp(-\\sum_i s(\\mathbf{x}_a)_i)\\).\nTherefore, the log-likelihood is \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z} = g(\\mathbf{x})) -\\sum_i s(\\mathbf{x}_a)_i.\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-10",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-10",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Image credits: Wehenkel and Louppe, 2019."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-11",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-11",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Epistemic uncertainty accounts for uncertainty in the model or in its parameters. It captures our ignorance about which model can best explain the collected data.\nIt can be explained away given enough data1.\nCredits: Kendall and Gal, What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?, 2017."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#bayesian-neural-networks",
    "href": "slides/week-07/09a-DL-uncertainty.html#bayesian-neural-networks",
    "title": "Simulation-based Inference",
    "section": "Bayesian neural networks",
    "text": "Bayesian neural networks\nTo capture epistemic uncertainty in a neural network, we model our ignorance with a prior distribution \\(p(\\mathbf{\\omega})\\) over its weights.\nThen we invoke Bayes for making predictions."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-12",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-12",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "The prior predictive distribution at \\(\\mathbf{x}\\) is given by integrating over all possible weight configurations, \\[p(y|\\mathbf{x}) = \\int p(y|\\mathbf{x}, \\mathbf{\\omega}) p(\\mathbf{\\omega}) d\\mathbf{\\omega}.\\]\nGiven training data \\(\\mathbf{d}=\\\\{(\\mathbf{x}_1, y_1), ..., (\\mathbf{x}_N, y_N)\\\\}\\) a Bayesian update results in the posterior \\[p(\\mathbf{\\omega}|\\mathbf{d}) = \\frac{p(\\mathbf{d}|\\mathbf{\\omega})p(\\mathbf{\\omega})}{p(\\mathbf{d})}\\] where the likelihood \\(p(\\mathbf{d}|\\omega) = \\prod_i p(y_i | \\mathbf{x}_i, \\omega).\\)\nThe posterior predictive distribution is then given by \\[p(y|\\mathbf{x},\\mathbf{d}) = \\int p(y|\\mathbf{x}, \\mathbf{\\omega}) p(\\mathbf{\\omega}|\\mathbf{d}) d\\mathbf{\\omega}.\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-13",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-13",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Bayesian neural networks are easy to formulate, but notoriously difficult to perform inference in.\n\\(p(\\mathbf{d})\\) is intractable to evaluate, which results in the posterior \\(p(\\mathbf{\\omega}|\\mathbf{d})\\) not being tractable either.\nTherefore, we must rely on approximations."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#variational-inference",
    "href": "slides/week-07/09a-DL-uncertainty.html#variational-inference",
    "title": "Simulation-based Inference",
    "section": "Variational inference",
    "text": "Variational inference\nVariational inference can be used for building an approximation \\(q(\\mathbf{\\omega};\\nu)\\) of the posterior \\(p(\\mathbf{\\omega}|\\mathbf{d})\\).\nWe can show that minimizing \\[\\text{KL}(q(\\mathbf{\\omega};\\nu) || p(\\mathbf{\\omega}|\\mathbf{d}))\\] with respect to the variational parameters \\(\\nu\\), is identical to maximizing the evidence lower bound objective (ELBO)\n\\[\\text{ELBO}(\\nu) = \\overbrace{\\mathbb{E}_{q(\\mathbf{\\omega};\\nu)} \\left[\\log p(\\mathbf{d}| \\mathbf{\\omega})\\right]}^{\\text{reconstruction error}} - \\overbrace{\\text{KL}(q(\\mathbf{\\omega};\\nu) || p(\\mathbf{\\omega}))}^{\\text{regularization}}.\\]\n\n\n\n\n\n\nNote\n\n\nThe most common measure of distance between probability distributions \\(p(x)\\) and \\(q(x)\\) is the Kullback-Leibler or KL divergence and is defined as:\n\\[\nD_{KL}[p(x)\\Vert q(x)]=\\int p(x)\\log \\begin{bmatrix}\\frac{p(x)}{q(x)}\\end{bmatrix}\\mathrm{d}x.\n\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-14",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-14",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "The integral in the ELBO is not tractable for almost all \\(q\\), but it can be maximized with stochastic gradient ascent:\n\nSample \\(\\hat{\\omega} \\sim q(\\mathbf{\\omega};\\nu)\\).\nDo one step of maximization with respect to \\(\\nu\\) on \\[\\hat{L}(\\nu) = \\log p(\\mathbf{d}|\\hat{\\omega}) - \\log\\frac{q(\\hat{\\omega};\\nu)}{p(\\hat{\\omega})} \\]\n\nIn the context of Bayesian neural networks, this procedure is also known as Bayes by backprop (Blundell et al, 2015)."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#training-vae",
    "href": "slides/week-07/09a-DL-uncertainty.html#training-vae",
    "title": "Simulation-based Inference",
    "section": "Training VAE",
    "text": "Training VAE"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#scientific-simulators",
    "href": "slides/week-07/09a-DL-uncertainty.html#scientific-simulators",
    "title": "Simulation-based Inference",
    "section": "Scientific simulators",
    "text": "Scientific simulators"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-17",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-17",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "unconditioned\\[\\theta, z, x \\sim p(\\theta, z, x)\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-18",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-18",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "conditioned\\[\\theta, z \\sim p(\\theta, z | x)\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-19",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-19",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Video\nGalton"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-20",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-20",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Credit: Brehmer"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#conditional-simulators",
    "href": "slides/week-07/09a-DL-uncertainty.html#conditional-simulators",
    "title": "Simulation-based Inference",
    "section": "Conditional simulators",
    "text": "Conditional simulators\nA conditional simulator prescribes a way to sample from the likelihood \\(p(\\mathbf{x}|\\vartheta)\\), where \\(\\vartheta\\) is a set of conditioning variables or parameters.\n\nCredits: Siddharth Mishra-Sharma, 2023."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-25",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-25",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "\\[p(x|\\theta) = \\underbrace{\\iiint}_{\\text{yikes!}} p(z_p|\\theta) p(z_s|z_p) p(z_d|z_s) p(x|z_d) dz_p dz_s dz_d\\]\nThat’s bad!"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#bayesian-inference",
    "href": "slides/week-07/09a-DL-uncertainty.html#bayesian-inference",
    "title": "Simulation-based Inference",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nStart with\n\na simulator that can generate \\(N\\) samples \\(x_i \\sim p(x_i|\\theta_i)\\),\na prior model \\(p(\\theta)\\),\nobserved data \\(x_\\text{obs} \\sim p(x_\\text{obs} | \\theta_\\text{true})\\).\n\nThen, estimate the posterior\n\\[p(\\theta|x_\\text{obs}) = \\frac{p(x_\\text{obs} | \\theta)p(\\theta)}{p(x_\\text{obs})}.\\]"
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-27",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-27",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Credits: Cranmer, Brehmer and Louppe, 2020."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#approximate-bayesian-computation-abc",
    "href": "slides/week-07/09a-DL-uncertainty.html#approximate-bayesian-computation-abc",
    "title": "Simulation-based Inference",
    "section": "Approximate Bayesian Computation (ABC)",
    "text": "Approximate Bayesian Computation (ABC)\n\nCredits: Johann Brehmer.Issues:\n\nHow to choose \\(x'\\)? \\(\\epsilon\\)? \\(||\\cdot||\\)?\nNo tractable posterior.\nNeed to run new simulations for new data or new prior."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-28",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-28",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Credits: Cranmer, Brehmer and Louppe, 2020."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-29",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-29",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Credits: Cranmer, Brehmer and Louppe, 2020."
  },
  {
    "objectID": "slides/week-07/09a-DL-uncertainty.html#section-30",
    "href": "slides/week-07/09a-DL-uncertainty.html#section-30",
    "title": "Simulation-based Inference",
    "section": "",
    "text": "Produce samples \\[\\mathbf{x} \\sim p(\\mathbf{x} | \\vartheta)\\]\n\nEvaluate densities \\[p(\\mathbf{x}|\\vartheta)\\] \\[p(\\vartheta | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\vartheta) p(\\vartheta)}{p(\\mathbf{x})}\\]\n\nEncode complex priors \\[p(\\mathbf{x})\\]\n\n\n\n\n\n\n\n\nCredits: Siddharth Mishra-Sharma, 2023."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#credits",
    "href": "slides/week-09/09b-DL-generative.html#credits",
    "title": "Deep Learning",
    "section": "Credits",
    "text": "Credits\nSlides are adapted from lectures developed by Gilles Louppe at University of Liege, Belgium under the BSD 3-Clause License\n\nlectures 10, 11, and 12 of INFO8010 - Deep Learning\nand of lecture materials of SSI 2023"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#generative-models",
    "href": "slides/week-09/09b-DL-generative.html#generative-models",
    "title": "Deep Learning",
    "section": "Generative models",
    "text": "Generative models\nA (deep) generative model is a probabilistic model \\(p_\\theta\\) that can be used as a simulator of the data.\nFormally, a generative model defines a probability distribution \\(p_\\theta(\\mathbf{x})\\) over the data \\(\\mathbf{x} \\in \\mathcal{X}\\), parameterized by \\(\\theta\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section",
    "href": "slides/week-09/09b-DL-generative.html#section",
    "title": "Deep Learning",
    "section": "",
    "text": "Credits: Karsten et al, 2022; Siddharth Mishra-Sharma, 2023."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-1",
    "href": "slides/week-09/09b-DL-generative.html#section-1",
    "title": "Deep Learning",
    "section": "",
    "text": "Disentangled latent space: Manipulating each dimension of \\(z\\) should correspond to changing an interpretable property of the data. For example, in a model of language, it might change the topic, tense, or verbosity.\nEfficient likelihood computation: If the model is probabilistic, we would like to be able to calculate the probability of new examples"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#latent-variable-model",
    "href": "slides/week-09/09b-DL-generative.html#latent-variable-model",
    "title": "Deep Learning",
    "section": "Latent variable model",
    "text": "Latent variable model\n\n\n\n\n\n\n\nConsider for now a prescribed latent variable model that relates a set of observable variables \\(\\mathbf{x} \\in \\mathcal{X}\\) to a set of unobserved variables \\(\\mathbf{z} \\in \\mathcal{Z}\\).\nThe probabilistic model defines a joint probability distribution \\(p_\\theta(\\mathbf{x}, \\mathbf{z})\\), which decomposes as \\[p_\\theta(\\mathbf{x}, \\mathbf{z}) = p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}).\\]"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-2",
    "href": "slides/week-09/09b-DL-generative.html#section-2",
    "title": "Deep Learning",
    "section": "",
    "text": "Video\n\n\nIf we interpret \\(\\mathbf{z}\\) as causal factors for the high-dimension representations \\(\\mathbf{x}\\), then sampling from \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) can be interpreted as a stochastic generating process from \\(\\mathcal{Z}\\) to \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#hot-to-fit",
    "href": "slides/week-09/09b-DL-generative.html#hot-to-fit",
    "title": "Deep Learning",
    "section": "Hot to fit?",
    "text": "Hot to fit?\n\\[\\begin{aligned}\n\\theta^{*} &= \\arg \\max_\\theta p_\\theta(\\mathbf{x}) \\\\\\\\\n&= \\arg \\max_\\theta \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\\\\\\\\\n&= \\arg \\max_\\theta \\mathbb{E}_{p(\\mathbf{z})}\\left[ p_\\theta(\\mathbf{x}|\\mathbf{z}) \\right] d\\mathbf{z}\\\\\\\\\n&\\approx \\arg \\max_\\theta \\frac{1}{N} \\sum_{i=1}^N p_\\theta(\\mathbf{x}|\\mathbf{z}_i)\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\nThe curse of dimensionality will lead to poor estimates of the expectation."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-4",
    "href": "slides/week-09/09b-DL-generative.html#section-4",
    "title": "Deep Learning",
    "section": "",
    "text": "We will directly learn a stochastic generating process \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) with a neural network.\nWe will also amortize the inference process by learning a second neural network \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) approximating the posterior, conditionally on the observed data \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-5",
    "href": "slides/week-09/09b-DL-generative.html#section-5",
    "title": "Deep Learning",
    "section": "",
    "text": "A variational auto-encoder is a deep latent variable model where:\n\nThe prior \\(p(\\mathbf{z})\\) is prescribed, and usually chosen to be Gaussian.\nThe density \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) is parameterized with a generative network \\(\\text{NN}_\\theta\\) (or decoder) that takes as input \\(\\mathbf{z}\\) and outputs parameters to the data distribution. E.g., \\[\\begin{aligned}\n\\mu, \\sigma^2 &= \\text{NN}_\\theta(\\mathbf{z}) \\\\\\\\\np_\\theta(\\mathbf{x}|\\mathbf{z}) &= \\mathcal{N}(\\mathbf{x}; \\mu, \\sigma^2\\mathbf{I})\n\\end{aligned}\\]\nThe approximate posterior \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is parameterized with an inference network \\(\\text{NN}_\\phi\\) (or encoder) that takes as input \\(\\mathbf{x}\\) and outputs parameters to the approximate posterior. E.g., \\[\\begin{aligned}\n\\mu, \\sigma^2 &= \\text{NN}_\\phi(\\mathbf{x}) \\\\\\\\\nq_\\phi(\\mathbf{z}|\\mathbf{x}) &= \\mathcal{N}(\\mathbf{z}; \\mu, \\sigma^2\\mathbf{I})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#variational-auto-encoders",
    "href": "slides/week-09/09b-DL-generative.html#variational-auto-encoders",
    "title": "Deep Learning",
    "section": "Variational auto-encoders",
    "text": "Variational auto-encoders"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-6",
    "href": "slides/week-09/09b-DL-generative.html#section-6",
    "title": "Deep Learning",
    "section": "",
    "text": "We can use variational inference to jointly optimize the generative and the inference networks parameters \\(\\theta\\) and \\(\\phi\\):\n\\[\\begin{aligned}\n\\theta^{*}, \\phi^{*} &= \\arg \\max_{\\theta,\\phi} \\mathbb{E}_{p(\\mathbf{x})} \\left[ \\text{ELBO}(\\mathbf{x};\\theta,\\phi) \\right] \\\\\\\\\n&= \\arg \\max_{\\theta,\\phi} \\mathbb{E}_{p(\\mathbf{x})}\\left[ \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [ \\log \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} ] \\right] \\\\\\\\\n&= \\arg \\max_{\\theta,\\phi} \\mathbb{E}_{p(\\mathbf{x})}\\left[ \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}\\left[ \\log p_\\theta(\\mathbf{x}|\\mathbf{z})\\right] - \\text{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z})) \\right].\n\\end{aligned}\\]\n\nGiven some generative network \\(\\theta\\), we want to put the mass of the latent variables, by adjusting \\(\\phi\\), such that they explain the observed data, while remaining close to the prior.\nGiven some inference network \\(\\phi\\), we want to put the mass of the observed variables, by adjusting \\(\\theta\\), such that they are well explained by the latent variables."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#pseudo-code",
    "href": "slides/week-09/09b-DL-generative.html#pseudo-code",
    "title": "Deep Learning",
    "section": "Pseudo code",
    "text": "Pseudo code"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-7",
    "href": "slides/week-09/09b-DL-generative.html#section-7",
    "title": "Deep Learning",
    "section": "",
    "text": "Credits: Kreis et al, 2022."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-8",
    "href": "slides/week-09/09b-DL-generative.html#section-8",
    "title": "Deep Learning",
    "section": "",
    "text": "Credits: Kreis et al, 2022.With \\(\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\), we have \\[\\begin{aligned}\n\\mathbf{x}_t &= \\sqrt{ {\\alpha}_t} \\mathbf{x}_{t-1} + \\sqrt{1-{\\alpha}_t} \\epsilon \\\\\\\\\nq(\\mathbf{x}_t | \\mathbf{x}_{t-1}) &= \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, (1-\\alpha_t)\\mathbf{I}) \\\\\\\\\nq(\\mathbf{x}_{1:T} | \\mathbf{x}_{0}) &=  \\prod_{t=1}^T q(\\mathbf{x}_t | \\mathbf{x}_{t-1})\n\\end{aligned}\\]\nsampling from \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) can be interpreted as a stochastic generating process from \\(\\mathcal{Z}\\) to \\(\\mathcal{X}\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-9",
    "href": "slides/week-09/09b-DL-generative.html#section-9",
    "title": "Deep Learning",
    "section": "",
    "text": "Credits: Simon J.D. Prince, 2023."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-10",
    "href": "slides/week-09/09b-DL-generative.html#section-10",
    "title": "Deep Learning",
    "section": "",
    "text": "Credits: Kreis et al, 2022.\n\n\n\n\\[\\begin{aligned}\np(\\mathbf{x}_{0:T}) &= p(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t)\\\\\\\\\np(\\mathbf{x}_T) &= \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, I) \\\\\\\\\np_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) &= \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\sigma^2_\\theta(\\mathbf{x}_t, t)\\mathbf{I}) \\\\\\\\\n\\mathbf{x}_{t-1} &= \\mu_\\theta(\\mathbf{x}_t, t) + \\sigma_\\theta(\\mathbf{x}_t, t) \\mathbf{z}\n\\end{aligned}\\] with \\(\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#markovian-hierarchical-vaes",
    "href": "slides/week-09/09b-DL-generative.html#markovian-hierarchical-vaes",
    "title": "Deep Learning",
    "section": "Markovian Hierarchical VAEs",
    "text": "Markovian Hierarchical VAEs"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-11",
    "href": "slides/week-09/09b-DL-generative.html#section-11",
    "title": "Deep Learning",
    "section": "",
    "text": "Similarly to VAEs, training is done by maximizing the ELBO, using a variational distribution \\(q_\\phi(\\mathbf{z}_{1:T} | \\mathbf{x})\\) over all levels of latent variables: \\[\\begin{aligned}\n\\log p_\\theta(\\mathbf{x}) &\\geq \\mathbb{E}_{q_\\phi(\\mathbf{z}_{1:T} | \\mathbf{x})}\\left[ \\log \\frac{p(\\mathbf{x},\\mathbf{z}_{1:T})}{q_\\phi(\\mathbf{z}_{1:T}|\\mathbf{x})} \\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-12",
    "href": "slides/week-09/09b-DL-generative.html#section-12",
    "title": "Deep Learning",
    "section": "",
    "text": "Diffusion models are Markovian HVAEs with the following constraints:\n\nThe latent dimension is the same as the data dimension.\nThe encoder is fixed to linear Gaussian transitions \\(q(\\mathbf{x}_t | \\mathbf{x}_{t-1})\\).\nThe hyper-parameters are set such that \\(q(\\mathbf{x}_T | \\mathbf{x}_0)\\) is a standard Gaussian."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#training",
    "href": "slides/week-09/09b-DL-generative.html#training",
    "title": "Deep Learning",
    "section": "Training",
    "text": "Training\nFor learning the parameters \\(\\theta\\) of the reverse process, we can form a variational lower bound on the log-likelihood of the data as\n\\[\\mathbb{E}_{q(\\mathbf{x}_0)}\\left[ \\log p_\\theta(\\mathbf{x}_0) \\right] \\geq \\mathbb{E}_{q(\\mathbf{x}_0)q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} | \\mathbf{x}_0)} \\right] := L\\]\nThis objective can be rewritten as \\[\\begin{aligned}\nL &= \\mathbb{E}_{q(\\mathbf{x}_0)q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}\\left[ \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} | \\mathbf{x}_0)} \\right] \\\\\\\\\n&= \\mathbb{E}_{q(\\mathbf{x}_0)} \\left[L_0 - \\sum_{t&gt;1} L_{t-1} - L_T\\right]\n\\end{aligned}\\] where"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-13",
    "href": "slides/week-09/09b-DL-generative.html#section-13",
    "title": "Deep Learning",
    "section": "",
    "text": "\\(L_0 = \\mathbb{E}_{q(\\mathbf{x}_1 | \\mathbf{x}_0)}[\\log p_\\theta(\\mathbf{x}_0 | \\mathbf{x}_1)]\\) can be interpreted as a reconstruction term. It can be approximated and optimized using a Monte Carlo estimate.\n\\(L_{t-1} = \\mathbb{E}_{q(\\mathbf{x}_t | \\mathbf{x}_0)}\\text{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) )\\) is a denoising matching term. The transition \\(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)\\) provides a learning signal for the reverse process, since it defines how to denoise the noisified input \\(\\mathbf{x}_t\\) with access to the original input \\(\\mathbf{x}_0\\).\n\\(L_T = \\text{KL}(q(\\mathbf{x}_T | \\mathbf{x}_0) || p_\\theta(\\mathbf{x}_T))\\) represents how close the distribution of the final noisified input is to the standard Gaussian. It has no trainable parameters."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-14",
    "href": "slides/week-09/09b-DL-generative.html#section-14",
    "title": "Deep Learning",
    "section": "",
    "text": "\\[\\begin{aligned}\n&\\arg \\min_\\theta L_{t-1} \\\\\\\\\n=&\\arg \\min_\\theta \\mathbb{E}_{q(\\mathbf{x}_t | \\mathbf{x}_0)} \\frac{1}{2\\sigma^2_t} \\frac{\\bar{\\alpha}_{t-1}(1-\\alpha_t)^2}{(1-\\bar{\\alpha}_t)^2} || \\hat{\\mathbf{x}}_\\theta(\\mathbf{x}_t, t) - \\mathbf{x}_0 ||_2^2\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\nInterpretation 1: Denoising. Training a diffusion model amounts to learning a neural network that predicts the original ground truth \\(\\mathbf{x}_0\\) from a noisy input \\(\\mathbf{x}_t\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-15",
    "href": "slides/week-09/09b-DL-generative.html#section-15",
    "title": "Deep Learning",
    "section": "",
    "text": "\\[\\begin{aligned}\n&\\arg \\min_\\theta L_{t-1} \\\\\\\\\n=&\\arg \\min_\\theta \\mathbb{E}_{\\mathcal{N}(\\epsilon;\\mathbf{0}, I)} \\frac{1}{2\\sigma^2_t} \\frac{(1-\\alpha_t)^2}{(1-\\bar{\\alpha}_t) \\alpha_t} || {\\epsilon}_\\theta(\\underbrace{\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon}_{\\mathbf{x}_t}, t) - \\epsilon ||_2^2 \\\\\\\\\n\\approx& \\arg \\min_\\theta \\mathbb{E}_{\\mathcal{N}(\\epsilon;\\mathbf{0}, I)} || {\\epsilon}_\\theta(\\underbrace{\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon}_{\\mathbf{x}_t}, t) - \\epsilon ||_2^2\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\nInterpretation 2: Noise prediction. Training a diffusion model amounts to learning a neural network that predicts the noise \\(\\epsilon\\) that was added to the original ground truth \\(\\mathbf{x}_0\\) to obtain the noisy \\(\\mathbf{x}_t\\)."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-16",
    "href": "slides/week-09/09b-DL-generative.html#section-16",
    "title": "Deep Learning",
    "section": "",
    "text": "\\[\\begin{aligned}\n&\\arg \\min_\\theta L_{t-1} \\\\\\\\\n=&\\arg \\min_\\theta \\mathbb{E}_{q(\\mathbf{x}_t | \\mathbf{x}_0)} \\frac{1}{2\\sigma^2_t} \\frac{(1-\\alpha_t)^2}{\\alpha_t} || s_\\theta(\\mathbf{x}_t, t) - \\nabla_{\\mathbf{x}_t}  \\log q(\\mathbf{x}_t | \\mathbf{x}_0) ||_2^2\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\nInterpretation 3: Denoising score matching. Training a diffusion model amounts to learning a neural network that predicts the score \\(\\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t | \\mathbf{x}_0)\\) of the tractable posterior.\nThe distribution \\(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)\\) is the tractable posterior distribution \\[\\begin{aligned}\nq(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0) &= \\frac{q(\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\mathbf{x}_0) q(\\mathbf{x}_{t-1} | \\mathbf{x}_0)}{q(\\mathbf{x}_t | \\mathbf{x}_0)} \\\\\\\\\n&= \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_q(\\mathbf{x}_t, \\mathbf{x}_0, t), \\sigma^2_t I)\n\\end{aligned}\\] where \\[\n\\mu_q(\\mathbf{x}_t, \\mathbf{x}_0, t) = \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1-\\alpha_t)}{1-\\bar{\\alpha}_t}\\mathbf{x}_0  \\qquad\\text{and} \\qquad\n\\sigma^2_t = \\frac{(1-\\alpha_t)(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\n\\]"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#network-architectures",
    "href": "slides/week-09/09b-DL-generative.html#network-architectures",
    "title": "Deep Learning",
    "section": "Network architectures",
    "text": "Network architectures\nDiffusion models often use U-Net architectures with ResNet blocks and self-attention layers to represent \\(\\hat{\\mathbf{x}}_\\theta(\\mathbf{x}_t, t)\\), \\(\\epsilon_\\theta(\\mathbf{x}_t, t)\\) or \\(s_\\theta(\\mathbf{x}_t, t)\\).\n\nCredits: Kreis et al, 2022."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#continuous-time-diffusion-models",
    "href": "slides/week-09/09b-DL-generative.html#continuous-time-diffusion-models",
    "title": "Deep Learning",
    "section": "Continuous-time diffusion models",
    "text": "Continuous-time diffusion models\n\nCredits: Kreis et al, 2022.With \\(\\beta_t = 1 - \\alpha_t\\), we can rewrite the forward process as \\[\\begin{aligned}\n\\mathbf{x}_t &= \\sqrt{ {\\alpha}_t} \\mathbf{x}_{t-1} + \\sqrt{1-{\\alpha}_t} \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\\\\\n&= \\sqrt{1 - {\\beta}_t} \\mathbf{x}_{t-1} + \\sqrt{ {\\beta}_t} \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\\\\\n&= \\sqrt{1 - {\\beta}(t)\\Delta_t} \\mathbf{x}_{t-1} + \\sqrt{ {\\beta}(t)\\Delta_t} \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-17",
    "href": "slides/week-09/09b-DL-generative.html#section-17",
    "title": "Deep Learning",
    "section": "",
    "text": "Credits: Song, 2021.\n\n\n\nWhen \\(\\Delta_t \\rightarrow 0\\), we can further rewrite the forward process as \\[\n\\mathbf{x}_t = \\sqrt{1 - {\\beta}(t)\\Delta_t} \\mathbf{x}_{t-1} + \\sqrt{ {\\beta}(t)\\Delta_t} \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\approx \\mathbf{x}_{t-1} - \\frac{\\beta(t)\\Delta_t}{2} \\mathbf{x}_{t-1} + \\sqrt{ {\\beta}(t)\\Delta_t} \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n.\\]\nThis last update rule corresponds to the Euler-Maruyama discretization of the stochastic differential equation (SDE) \\[\\text{d}\\mathbf{x}_t = -\\frac{1}{2}\\beta(t)\\mathbf{x}_t \\text{d}t + \\sqrt{\\beta(t)} \\text{d}\\mathbf{w}_t\\] describing the diffusion in the infinitesimal limit."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-18",
    "href": "slides/week-09/09b-DL-generative.html#section-18",
    "title": "Deep Learning",
    "section": "",
    "text": "The reverse process satisfies a reverse-time SDE that can be derived analytically from the forward-time SDE and the score of the marginal distribution \\(q(\\mathbf{x}_t)\\), as \\[\\text{d}\\mathbf{x}_t = \\left[ -\\frac{1}{2}\\beta(t)\\mathbf{x}_t - \\beta(t)\\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) \\right] \\text{d}t + \\sqrt{\\beta(t)} \\text{d}\\mathbf{w}_t.\\]\n\n\n\n\nCredits: Song, 2021."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#training-sampling-diffusion-models",
    "href": "slides/week-09/09b-DL-generative.html#training-sampling-diffusion-models",
    "title": "Deep Learning",
    "section": "Training & sampling diffusion models",
    "text": "Training & sampling diffusion models\n\nCredit: Ho et. al., 2020"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#continuous-time-normalizing-flows",
    "href": "slides/week-09/09b-DL-generative.html#continuous-time-normalizing-flows",
    "title": "Deep Learning",
    "section": "Continuous-time normalizing flows",
    "text": "Continuous-time normalizing flows\n\n\nReplace the discrete sequence of transformations with a neural ODE with reversible dynamics such that \\[\\begin{aligned}\n&\\mathbf{z}_0 \\sim p(\\mathbf{z})\\\\\\\\\n&\\frac{d\\mathbf{z}(t)}{dt} = f(\\mathbf{z}(t), t, \\theta)\\\\\\\\\n&\\mathbf{x} = \\mathbf{z}(1) = \\mathbf{z}_0 + \\int_0^1 f(\\mathbf{z}(t), t) dt.\n\\end{aligned}\\]\n\n\n\nVideo\nImage credits: Grathwohl et al, 2018\n\n\n\n\nThe instantaneous change of variable yields \\[\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}(0)) - \\int_0^1 \\text{Tr} \\left( \\frac{\\partial f(\\mathbf{z}(t), t, \\theta)}{\\partial \\mathbf{z}(t)} \\right) dt.\\]"
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#probability-flow-ode",
    "href": "slides/week-09/09b-DL-generative.html#probability-flow-ode",
    "title": "Deep Learning",
    "section": "Probability flow ODE",
    "text": "Probability flow ODE\nBack to diffusion: For any diffusion process, there exists a corresponding deterministic process \\[\\text{d}\\mathbf{x}_t = \\left[ \\mathbf{f}(t, \\mathbf{x}_t) - \\frac{1}{2} g^2(t) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t) \\right] \\text{d}t\\] whose trajectories share the same marginal densities \\(p(\\mathbf{x}_t)\\).\nTherefore, when \\(\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\\) is replaced by its approximation \\(s_\\theta(\\mathbf{x}_t, t)\\), the probability flow ODE becomes a special case of a neural ODE. In particular, it is an example of continuous-time normalizing flows!\n\nCredits: Song, 2021."
  },
  {
    "objectID": "slides/week-09/09b-DL-generative.html#section-19",
    "href": "slides/week-09/09b-DL-generative.html#section-19",
    "title": "Deep Learning",
    "section": "",
    "text": "Prince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. http://udlbook.com."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#bayesian-filters",
    "href": "slides/week-05/08-DA-Bayes.html#bayesian-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Bayesian filters",
    "text": "Bayesian filters\nWe begin by defining a probabilistic state-space, or nonlinear filtering model, of the form \\[\\begin{aligned}\n    \\mathbf{x}_{k} & \\sim p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\label{eq:filter1}\\\\\n    \\mathbf{y}_{k} & \\sim p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}),\\quad k=0,1,2,\\ldots,\\label{eq:filter2}\n    \\end{aligned} \\tag{1}\\] where\n\n\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) is the state vector at time \\(k,\\)\n\\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}\\) is the observation vector at time \\(k,\\)\nthe conditional probability, \\(p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\) represents the stochasticdynamics model, and can be a probability density or a discrete probability function, or a mixture of both,\nthe conditional probability, \\(p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}),\\) represents themeasurement model and its inherent noise."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section",
    "href": "slides/week-05/08-DA-Bayes.html#section",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "In addition, we assume that the model is Markovian, such that \\[p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{1:k-1},\\mathbf{y}_{1:k-1})=p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}),\\] and that the observations are conditionally independent of state and measurement histories, \\[p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{1:k},\\mathbf{y}_{1:k-1})=p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}).\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#example-of-gaussian-random-walk",
    "href": "slides/week-05/08-DA-Bayes.html#example-of-gaussian-random-walk",
    "title": "Bayesian Data Assimilation",
    "section": "Example of Gaussian Random Walk",
    "text": "Example of Gaussian Random Walk\nTo fix ideas and notation, we begin with a very simple, scalar case, the Gaussian random walk model. This model can then easily be generalized.\nConsider the scalar system \\[\\begin{aligned}\n    x_{k} & =x_{k-1}+w_{k-1},\\quad w_{k-1}\\sim\\mathcal{N}(0,Q),\\label{eq:GRW-x}\\\\\n    y_{k} & =x_{k}+v_{k},\\quad v_{k}\\sim\\mathcal{N}(0,R),\\label{eq:GRW-y}\n    \\end{aligned} \\tag{2}\\] where \\(x_{k}\\) is the (hidden) state and \\(y_{k}\\) is the (known) measurement.\nNoting that \\(x_{k}-x_{k-1}=w_{k-1}\\) and that \\(y_{k}-x_{k}=v_{k},\\) we can immediately rewrite this system in terms of the conditional probability densities, \\[\\begin{aligned}\n    p(x_{k}\\mid x_{k-1}) & =\\mathcal{N}\\left(x_{k}\\mid x_{k-1},Q\\right)\\\\\n     & =\\dfrac{1}{\\sqrt{2\\pi Q}}\\exp\\left[-\\frac{1}{2Q}\\left(x_{k}-x_{k-1}\\right)^{2}\\right]\n    \\end{aligned}\\] and"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-1",
    "href": "slides/week-05/08-DA-Bayes.html#section-1",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    p(y_{k}\\mid x_{k}) & =\\mathcal{N}\\left(y_{k}\\mid x_{k},R\\right)\\\\\n     & =\\dfrac{1}{\\sqrt{2\\pi R}}\\exp\\left[-\\frac{1}{2R}\\left(y_{k}-x_{k}\\right)^{2}\\right].\n    \\end{aligned}\\]\nA realization of the model is shown in Figure 1.\n\n\nFigure 1: Gaussian random walk state space model equations 2. State, \\(x_k\\) is solid blue curve, measurements, \\(y_k\\), are red circles. Fixed values of noise variance are \\(Q=1\\) and \\(R=1\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#code",
    "href": "slides/week-05/08-DA-Bayes.html#code",
    "title": "Bayesian Data Assimilation",
    "section": "Code",
    "text": "Code\n% Simulate a Gaussian random walk.\n% initialize\nrandn('state',123)\nR=1; Q=1; K=100;\n% simulate\nX_init = sqrt(Q)\\*randn(K,1);\nX = cumsum(X_init);\nW = sqrt(R)\\*randn(K,1);\nY = X + W;\n% plot\nplot(1:K,X,1:K,Y(1:K,1),'ro')\nxlabel('k'), ylabel('x_k')"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#nonlinear-filter-model",
    "href": "slides/week-05/08-DA-Bayes.html#nonlinear-filter-model",
    "title": "Bayesian Data Assimilation",
    "section": "Nonlinear Filter Model",
    "text": "Nonlinear Filter Model\nUsing the nonlinear filtering model 1 and the Markov property, we can express thejoint prior of the states, \\(\\mathbf{x}_{0:T}=\\{\\mathbf{x}_{0},\\ldots,\\mathbf{x}_{T}\\},\\) and the joint likelihood of the measurements, \\(\\mathbf{y}_{1:T}=\\{\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{T}\\},\\) as the products \\[p(\\mathbf{x}_{0:T})=p(\\mathbf{x}_{0})\\prod_{k=1}^{T}p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1})\\] and \\[p(\\mathbf{y}_{1:T}\\mid\\mathbf{x}_{0:T})=\\prod_{k=1}^{T}p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})\\] respectively."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-2",
    "href": "slides/week-05/08-DA-Bayes.html#section-2",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Then, applying Bayes’ law, we can compute the complete posterior distribution of the states as \\[p(\\mathbf{x}_{0:T}\\mid\\mathbf{y}_{1:T})=\\frac{p(\\mathbf{y}_{1:T}\\mid\\mathbf{x}_{0:T})p(\\mathbf{x}_{0:T})}{p(\\mathbf{y}_{1:T})}.\\label{eq:Bayes-post-state-eq}\\]\nBut this type of complete characterization is not feasible to compute in real-time, or near real-time, since the number of computations per time-step increases as measurements arrive.\nWhat we need is afixed number of computations per time-step.\n\nThis can be achieved by a recursive estimation that, step by step, produces the filtering distribution defined above.\nIn this light, we can now define the general Bayesian filtering problem, of which Kalman filters will be a special case."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-3",
    "href": "slides/week-05/08-DA-Bayes.html#section-3",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Bayesian filtering is the recursive computation of the marginal posterior distribution, \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\] known as the filtering distribution, of the state \\(\\mathbf{x}_{k}\\) at each time step \\(k,\\) given the measurements up to time \\(k.\\)\n\nNow, based on Bayes’ rule, we can formulate the Bayesian filtering theorem (Särkkä and Svensson 2023) .\n\nTheorem 1 (Bayesian Filter). The recursive equations, known as the Bayesian filter, for computing the filtering distribution \\(p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\) and the predicted distribution \\(p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})\\) at the time step \\(k,\\) are given by the three-stage process:\nInitialization: Define the prior distribution \\(p(\\mathbf{x}_{0}).\\)\nPrediction: Compute the predictive distribution \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})=\\int p(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1})p(\\mathbf{x}_{k-1}\\mid\\mathbf{y}_{1:k-1})\\,\\mathrm{d}\\mathbf{x}_{k-1}.\\]\nCorrection: Compute the posterior distribution by Bayes’ rule,*"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-4",
    "href": "slides/week-05/08-DA-Bayes.html#section-4",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})=\\frac{p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})}{\\int p(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k})p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})}\\,\\mathrm{d}\\mathbf{x}_{k}.\\]\nNow, if we assume that the dynamic and measurement models are linear, with i.i.d. Gaussian noise, then we obtain the closed-form solution for the Kalman filter, already derived in the Basic Course. We recall the linear, Gaussian state-space model, \\[\\begin{aligned}\n\\mathbf{x}_{k} & =\\mathbf{M}_{k-1}\\mathbf{x}_{k-1}+\\mathbf{w}_{k-1},\\label{eq:kf-1}\\\\\n\\mathbf{y}_{k} & =\\mathbf{H}_{k}\\mathbf{x}_{k}+\\mathbf{v}_{k},\\label{eq:kf-2}\n\\end{aligned} \\tag{3}\\] for the Kalman filter, where\n\n\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) is the state,\n\\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}\\) is the measurement,\n\\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1})\\) is the process noise,\n\\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k})\\) is the measurement noise,\n\\(\\mathbf{x}_{0}\\sim\\mathcal{N}(\\mathbf{m}_{0},\\mathbf{P}_{0})\\) is the Gaussian distributed initial state, with mean \\(\\mathbf{m}_{0}\\) and covariance \\(\\mathbf{P}_{0},\\)\n\\(\\mathbf{M}_{k-1}\\) is the time-dependent transition matrix of the dynamic model at time \\(k-1,\\) and\n\\(\\mathbf{H}_{k}\\) is the time-dependent measurement model matrix."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-5",
    "href": "slides/week-05/08-DA-Bayes.html#section-5",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "This model can be very elegantly rewritten in terms of conditional probabilities as \\[\\begin{aligned}\np(\\mathbf{x}_{k}\\mid\\mathbf{x}_{k-1}) & =\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{M}_{k-1}\\mathbf{x}_{k-1},\\mathbf{Q}_{k-1}\\right),\\\\\np(\\mathbf{y}_{k}\\mid\\mathbf{x}_{k}) & =\\mathcal{N}\\left(\\mathbf{y}_{k}\\mid\\mathbf{H}_{k}\\mathbf{x}_{k},\\mathbf{R}_{k}\\right).\n\\end{aligned}\\]\n\nTheorem 2 (Kalman Filter). The Bayesian filtering equations for the linear, Gaussian model Equation 3 can be explicitly computed and the resulting conditional probability distributions are Gaussian. The prediction distribution is \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k-1})=\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\hat{\\mathbf{m}}_{k},\\hat{\\mathbf{P}}_{k}\\right),\\] the filtering distribution is \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})=\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right)\\] and the smoothing distribution is \\[p(\\mathbf{y}_{k}\\mid\\mathbf{y}_{1:k-1})=\\mathcal{N}\\left(\\mathbf{y}_{k}\\mid\\mathbf{H}_{k}\\mathbf{\\hat{m}}_{k},\\mathbf{S}_{k}\\right).\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-6",
    "href": "slides/week-05/08-DA-Bayes.html#section-6",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "The parameters of these distributions can be computed by the three-stage Kalman filter loop:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)*\nPrediction: Compute the predictive distribution mean and covariance,* \\[\\begin{aligned}\n\\mathbf{\\hat{m}}_{k} & =\\mathbf{M}_{k-1}\\mathbf{m}_{k-1},\\\\\n\\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{k-1}\\mathbf{P}_{k-1}\\mathbf{M}_{k-1}^{\\mathrm{T}}+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]\nCorrection: Compute the filtering distribution mean and covariance by first defining* \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathbf{H}_{k}\\mathbf{\\hat{m}}_{k},\\quad\\textrm{the innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{k}\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{k}^{\\mathrm{T}}+\\mathbf{R}_{k},\\quad\\textrm{the measurement covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{k}^{\\mathrm{T}}\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\] then finally updating the filter mean and covariance,"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-7",
    "href": "slides/week-05/08-DA-Bayes.html#section-7",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]\n\nProof. The proof see Särkkä and Svensson (2023) is a direct application of classical results for the joint, marginal, and conditional distributions of two Gaussian random variables, \\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}.\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#kf-for-gaussian-random-walk",
    "href": "slides/week-05/08-DA-Bayes.html#kf-for-gaussian-random-walk",
    "title": "Bayesian Data Assimilation",
    "section": "KF for Gaussian Random Walk",
    "text": "KF for Gaussian Random Walk\nWe now return to the Gaussian random walk model seen above in the Example, and formulate a Kalman filter for estimating its state from noisy measurements.\n\nExample 1 Kalman Filter for Gaussian Random Walk. Suppose that we have measurements of the scalar \\(y_{k}\\) from the Gaussian random walk model \\[\\begin{aligned}\nx_{k} & =x_{k-1}+w_{k-1},\\quad w_{k-1}\\sim\\mathcal{N}(0,Q),\\label{eq:GRW-x2}\\\\\ny_{k} & =x_{k}+v_{k},\\quad v_{k}\\sim\\mathcal{N}(0,R).\\label{eq:GRW-y2}\n\\end{aligned} \\tag{4}\\] This very basic system is found in many applications where\n\n\\(x_{k}\\) represents a slowly varying quantity that we measure directly.\nprocess noise, \\(w_{k},\\) takes into account fluctuations in the state \\(x_{k}.\\)\nmeasurement noise, \\(v_{k},\\) accounts for measurement instrument errors.\n\n\nWe want to estimate the state \\(x_{k}\\) over time, taking into account the measurements \\(y_{k}.\\) That is, we would like to compute the filtering density,"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-8",
    "href": "slides/week-05/08-DA-Bayes.html#section-8",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[p({x}_{k}\\mid{y}_{1:k})=\\mathcal{N}\\left({x}_{k}\\mid{m}_{k},{P}_{k}\\right).\\] We proceed by simply writing down the three stages of the Kalman filter, noting that \\(M_{k}=1\\) and \\(H_{k}=1\\) for this model. We obtain:\nInitialization: Define the prior mean \\({m}_{0}\\) and prior covariance \\({P}_{0}.\\)\nPrediction: \\[\\begin{aligned}\n{\\hat{m}}_{k} & ={m}_{k-1},\\\\\n\\hat{{P}}_{k} & ={P}_{k-1}+{Q}.\n\\end{aligned}\\]\nCorrection: Define \\[\\begin{aligned}\n{d}_{k} & ={y}_{k}-{\\hat{m}}_{k},\\quad\\textrm{the innovation},\\\\\n{S}_{k} & =\\hat{{P}}_{k}+{R},\\quad\\textrm{the measurement covariance},\\\\\n{K}_{k} & =\\hat{{P}}_{k}{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-9",
    "href": "slides/week-05/08-DA-Bayes.html#section-9",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "then update, \\[\\begin{aligned}\n{m}_{k} & ={\\hat{m}}_{k}+K_{k}{d}_{k},\\\\\nP_{k} & =\\hat{P}_{k}-\\frac{\\hat{P}_{k}^{2}}{S_{k}}.\n\\end{aligned}\\]\nIn Figure 2, we show simulations with system noise standard deviation of \\(1\\) and measurement noise standard deviation of \\(0.5.\\) We observe that the KF tracks the random walk very efficiently.\n\n\nFigure 2: Kalman filter for tracking a Gaussian random walk state space model 4“. State, \\(x_k\\), is solid blue curve; measurements, \\(y_k\\), are red circles; Kalman filter estimate is green curve and 95% quantiles are shown. Fixed values of noise variances are \\(Q=1\\) and \\(R=0.5^2\\). Results computed by kf_gauss_state.m."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#code-1",
    "href": "slides/week-05/08-DA-Bayes.html#code-1",
    "title": "Bayesian Data Assimilation",
    "section": "Code",
    "text": "Code\n\n\n% Kalman Filter for scalar Gaussian random walk\n% Set parameters\nsig_w = 1; sig_v = 0.5;\nM = 1;\nQ = sig_w\\^2;\nH = 1;\nR = sig_v\\^2;\n% Initialize\nm0 = 0;\nP0 = 1;\n% Simulate data\nrandn('state',1234);\nsteps = 100; T = \\[1:steps\\];\nX = zeros(1,steps);\nY = zeros(1,steps);\nx = m0;\nfor k=1:steps\n  w = Q'\\*randn(1);\n  x = M\\*x + w;\n  y = H\\*x + sig_v\\*randn(1);\n  X(k) = x;\n  Y(k) = y;\nend\nplot(T,X,'-',T,Y,'.');\nlegend('Signal','Measurements');\nxlabel('{k}'); ylabel('{x}\\_k');\n% Kalman filter\nm = m0;\nP = P0;\nfor k=1:steps\n  m = M\\*m;\n  P = M\\*P\\*M' + Q;\n  d = Y(:,k) - H\\*m;\n  S = H\\*P\\*H' + R;\n  K = P\\*H'/S;\n  m = m + K\\*d;\n  P = P - K\\*S\\*K';\n  kf_m(k) = m;\n  kf_P(k) = P;\nend\n% Plot   \nclf; hold on\nfill(\\[T fliplr(T)\\],\\[kf_m+1.96\\*sqrt(kf_P) \\...\n  fliplr(kf_m-1.96\\*sqrt(kf_P))\\],1, \\...\n  'FaceColor',\\[.9 .9 .9\\],'EdgeColor',\\[.9 .9 .9\\])\nplot(T,X,'-b',T,Y,'or',T, kf_m(1,:),'-g')\nplot(T,kf_m+1.96\\*sqrt(kf_P),':r',T,kf_m-1.96\\*sqrt(kf_P),':r');\nhold off\n\n\n\n\n\n\n\nNote\n\n\n\nLine 3: by modifying these noise amplitudes, one can better understand how the KF operates.\nLines 31-32 and 34-38: the complete filter is coded in only 7 lines, exactly as prescribed by Theorem 5. This is the reason for the excellent performance of the KF, in particular in real-time systems. In higher dimensions, when the matrices become large, more attention must be paid to the numerical linear algebra routines used. The inversion of the measurement covariance matrix, \\(S,\\) in line 36, is particularly challenging and requires highly tuned decomposition methods."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-10",
    "href": "slides/week-05/08-DA-Bayes.html#section-10",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "EXTENDED KALMAN FILTERS"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#extended-kalman-filters",
    "href": "slides/week-05/08-DA-Bayes.html#extended-kalman-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Extended Kalman filters",
    "text": "Extended Kalman filters\nIn real applications, we are usually confronted with nonlinearity in the model and in the measurements.\n\nMoreover, the noise is not necessarily additive.\n\nTo deal with these nonlinearities, one possible approach is to linearize about the current mean and covariance, which produces the extended Kalman filter (EKF).\nThis filter is widely accepted as the standard for navigation and GPS systems, among others.\nRecall the nonlinear problem, \\[\\begin{aligned}\n\\mathbf{x}_{k} & = & \\mathcal{M}_{k-1}(\\mathbf{x}_{k-1})+\\mathbf{w}_{k-1},\\label{eq:state_nl}\\\\\n\\mathbf{y}_{k} & = & \\mathcal{H}_{k}(\\mathbf{x}_{k})+\\mathbf{v}_{k},\\label{eq:obs_nl}\n\\end{aligned} \\tag{5}\\] where"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-11",
    "href": "slides/week-05/08-DA-Bayes.html#section-11",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n},\\) \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m},\\) \\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1}),\\) \\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k}),\\)\nand now \\(\\mathcal{M}_{k-1}\\) and \\(\\mathcal{H}_{k}\\) are nonlinear functions of \\(\\mathbf{x}_{k-1}\\) and \\(\\mathbf{x}_{k}\\) respectively.\n\nThe EKF is then based on Gaussian approximationsof the filtering densities, \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\approx\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right),\\] where these approximations are derived from the first-order truncation of the corresponding Taylor series in terms of the statistical moments of the underlying random variables.\nLinearization in the Taylor series expansions will require evaluation of the Jacobian matrices, defined as \\[\\mathbf{M}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{M}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}\\] and"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-12",
    "href": "slides/week-05/08-DA-Bayes.html#section-12",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\mathbf{H}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}.\\]\n\nTheorem 3 The first-order extended Kalman filter with additive noise for the nonlinear system 5 can be computed by the three-stage process:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)\nPrediction: Compute the predictive distribution mean and covariance,* \\[\\begin{aligned}\n\\mathbf{\\hat{m}}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{m}_{k-1}),\\\\\n\\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{\\mathbf{x}}(\\mathbf{m}_{k-1})\\mathbf{P}_{k-1}\\mathbf{M}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]\nCorrection: Compute the filtering distribution mean and covariance by first defining* \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathcal{H}_{k}(\\mathbf{\\hat{m}}_{k}),\\quad\\textrm{the innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{\\mathbf{x}}(\\mathbf{\\hat{m}}_{k})\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})+\\mathbf{R}_{k},\\thinspace\\textrm{the measurement covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain,}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-13",
    "href": "slides/week-05/08-DA-Bayes.html#section-13",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "then finally updating the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]\n\nProof. The proof see Särkkä and Svensson (2023) is once again a direct application of classical results for the joint, marginal and conditional distributions of two Gaussian random variables, \\(\\mathbf{x}_{k}\\in\\mathbb{R}^{n}\\) and \\(\\mathbf{y}_{k}\\in\\mathbb{R}^{m}.\\) In addition, use is made of the Taylor series approximations to compute the Jacobian matrices \\(\\mathbf{M}_{\\mathbf{x}}\\) and \\(\\mathbf{H}_{\\mathbf{x}}\\) evaluated at \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k-1}\\) and \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k}\\) respectively."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#extended-kalman-filter-non-additive-noise",
    "href": "slides/week-05/08-DA-Bayes.html#extended-kalman-filter-non-additive-noise",
    "title": "Bayesian Data Assimilation",
    "section": "Extended Kalman filter — non-additive noise",
    "text": "Extended Kalman filter — non-additive noise\nFor non-additive noise, the model is now \\[\\begin{aligned}\n\\mathbf{x}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{x}_{k-1},\\mathbf{w}_{k-1}),\\label{eq:state_nl_na}\\\\\n\\mathbf{y}_{k} & =\\mathcal{H}_{k}(\\mathbf{x}_{k},\\mathbf{v}_{k}),\\label{eq:obs_nl_na}\n\\end{aligned} \\tag{6}\\] where \\(\\mathbf{w}_{k-1}\\sim\\mathcal{N}(0,\\mathbf{Q}_{k-1}),\\) and \\(\\mathbf{v}_{k}\\sim\\mathcal{N}(0,\\mathbf{R}_{k})\\) are system and measurement Gaussian noises.\nIn this case the overall three-stage scheme is the same, with necessary modifications to take into account the additional functional dependence on \\(\\mathbf{w}\\) and \\(\\mathbf{v}.\\)\n\nInitialization:\n\nDefine the prior mean \\(\\mathbf{m}_{0}\\) and prior covariance \\(\\mathbf{P}_{0}.\\)\n\nPrediction:\n\nCompute the predictive distribution mean and covariance,"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-14",
    "href": "slides/week-05/08-DA-Bayes.html#section-14",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n    \\mathbf{\\hat{m}}_{k} & =\\mathcal{M}_{k-1}(\\mathbf{m}_{k-1},\\mathbf{0}),\\\\\n    \\hat{\\mathbf{P}}_{k} & =\\mathbf{M}_{\\mathbf{x}}(\\mathbf{m}_{k-1})\\mathbf{P}_{k-1}\\mathbf{M}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})\\\\\n     & +\\mathbf{M}_{\\mathbf{w}}(\\mathbf{m}_{k-1})\\mathbf{Q}_{k-1}\\mathbf{M}_{\\mathbf{w}}^{\\mathrm{T}}(\\mathbf{m}_{k-1})+\\mathbf{Q}_{k-1}.\n    \\end{aligned}\\]\n\nCorrection:\n\nCompute the filtering distribution mean and covariance by first defining \\[\\begin{aligned}\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\mathcal{H}_{k}(\\mathbf{\\hat{m}}_{k},\\mathbf{0}),\\,\\textrm{the}\\,\\textrm{innovation},\\\\\n\\mathbf{S}_{k} & =\\mathbf{H}_{\\mathbf{x}}(\\mathbf{\\hat{m}}_{k})\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\\\\n& +\\mathbf{H}_{\\mathbf{v}}(\\mathbf{\\hat{m}}_{k})\\mathbf{R}_{k}\\mathbf{H}_{\\mathbf{v}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k}),\\,\\textrm{the}\\,\\textrm{measurement\\,covariance},\\\\\n\\mathbf{K}_{k} & =\\hat{\\mathbf{P}}_{k}\\mathbf{H}_{\\mathbf{x}}^{\\mathrm{T}}(\\mathbf{\\hat{m}}_{k})\\mathbf{S}_{k}^{-1},\\,\\textrm{the}\\,\\textrm{Kalman\\,gain,}\n\\end{aligned}\\] then finally updating the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#ekf-pros-and-cons",
    "href": "slides/week-05/08-DA-Bayes.html#ekf-pros-and-cons",
    "title": "Bayesian Data Assimilation",
    "section": "EKF — pros and cons",
    "text": "EKF — pros and cons\nPros:\n\nRelative simplicity, based on well-known linearization methods.\nMaintains the simple, elegant, and computationally efficient KF update equations.\nGood performance for such a simple method.\nAbility to treat nonlinear process and observation models.\nAbility to treat both additive and more general nonlinear Gaussian noise.\n\nCons:\n\nPerformance can suffer in presence of strong nonlinearity because of the local validity of the linear approximation (valid for small perturbations around the linear term).\nCannot deal with non-Gaussian noise, such as discrete-valued random variables."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-15",
    "href": "slides/week-05/08-DA-Bayes.html#section-15",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Requires differentiable process and measurement operators and evaluation of Jacobian matrices, which might be problematic in very high dimensions.\n\nIn spite of this, the EKF remains a solid filter and, as mentioned earlier, remains the basis of most GPS and navigation systems."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#ekf-example-nonlinear-oscillator",
    "href": "slides/week-05/08-DA-Bayes.html#ekf-example-nonlinear-oscillator",
    "title": "Bayesian Data Assimilation",
    "section": "EKF Example — nonlinear oscillator",
    "text": "EKF Example — nonlinear oscillator\nConsider the nonlinear ODE model for the oscillations of a noisy pendulum with unit mass and length \\(L,\\) \\[\\frac{\\mathrm{d}^{2}\\theta}{\\mathrm{d}t^{2}}+\\frac{g}{L}\\sin\\theta+w(t)=0\\] where\n\n\\(\\theta\\) is the angular displacement of the pendulum,\n\\(g\\) is the gravitational constant,\n\\(L\\) is the pendulum’s length, and\n\\(w(t)\\) is a white noise process.\n\nThis is rewritten in state space form, \\[\\dot{\\mathbf{x}}+\\mathcal{M}(\\mathbf{x})+\\mathbf{w}=0,\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-16",
    "href": "slides/week-05/08-DA-Bayes.html#section-16",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "where\n\\[\\begin{aligned}\n    \\mathbf{x} & =\\left[\\begin{array}{c}\n    x_{1}\\\\\n    x_{2}\n    \\end{array}\\right]=\\left[\\begin{array}{c}\n    \\theta\\\\\n    \\dot{\\theta}\n    \\end{array}\\right],\\quad\\mathcal{M}(\\mathbf{x})=\\left[\\begin{array}{c}\n    x_{2}\\\\\n    -\\dfrac{g}{L}\\sin x_{1}\n    \\end{array}\\right],\\\\\n    \\mathbf{w} & =\\left[\\begin{array}{c}\n    0\\\\\n    w(t)\n    \\end{array}\\right].\n    \\end{aligned}\\]\nSuppose that we have discrete, noisy measurements of the horizontal component of the position, \\(\\sin(\\theta).\\)\n\nThen the measurement equation is scalar, \\[y_{k}=\\sin\\theta_{k}+v_{k},\\] where \\(v_{k}\\) is a zero-mean Gaussian random variable with variance \\(R.\\)\n\nThe system is thus nonlinear in both state and measurement and the state-space system is of the general form 5."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-17",
    "href": "slides/week-05/08-DA-Bayes.html#section-17",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "A simple discretization, based on the simplest Euler’s method, produces \\[\\begin{aligned}\n    \\mathbf{x}_{k} & =\\mathcal{M}(\\mathbf{x}_{k-1})+\\mathbf{w}_{k-1}\\\\\n    {y}_{k} & =\\mathcal{H}_{k}(\\mathbf{x}_{k})+{v}_{k},\n    \\end{aligned}\\] where \\[\\begin{aligned}\n    \\mathbf{x}_{k} & =\\left[\\begin{array}{c}\n    x_{1}\\\\\n    x_{2}\n    \\end{array}\\right]_{k},\\\\\n    \\mathcal{M}(\\mathbf{x}_{k-1}) & =\\left[\\begin{array}{c}\n    x_{1}+\\Delta tx_{2}\\\\\n    x_{2}-\\Delta t\\dfrac{g}{L}\\sin x_{1}\n    \\end{array}\\right]_{k-1},\\\\\n    \\mathcal{H}(\\mathbf{x}_{k}) & =[\\sin x_{1}]_{k}.\n    \\end{aligned}\\]\nThe noise terms have distributions \\[\\mathbf{w}_{k-1}\\sim\\mathcal{N}(\\mathbf{0},Q),\\quad v_{k}\\sim\\mathcal{N}(0,R),\\] where the process covariance matrix is"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-18",
    "href": "slides/week-05/08-DA-Bayes.html#section-18",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[Q=\\left[\\begin{array}{cc}\n    q_{11} & q_{12}\\\\\n    q_{21} & q_{22}\n    \\end{array}\\right],\\] with components (see remark below the example), \\[q_{11}=q_{c}\\frac{\\Delta t^{3}}{3},\\quad q_{12}=q_{21}=q_{c}\\frac{\\Delta t^{2}}{2},\\quad q_{22}=q_{c}\\Delta t,\\] and \\(q_{c}\\) is the continuous process noise spectral density.\nFor the first-order EKFhigher orders are possiblewe will need the Jacobian matrices of \\(\\mathcal{M}(\\mathbf{x})\\) and \\(\\mathcal{H}(\\mathbf{x})\\) evaluated at \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k-1}\\) and \\(\\mathbf{x}=\\mathbf{\\hat{m}}_{k}\\) . These are easily obtained here, in an explicit form, \\[\\mathbf{M}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{M}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}=\\left[\\begin{array}{cc}\n    1 & \\Delta t\\\\\n    -\\Delta t\\dfrac{g}{L}\\cos x_{1} & 1\n    \\end{array}\\right]_{k-1},\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-19",
    "href": "slides/week-05/08-DA-Bayes.html#section-19",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\mathbf{H}_{\\mathbf{x}}=\\left[\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{x}}\\right]_{\\mathbf{x}=\\mathbf{m}}=\\left[\\begin{array}{cc}\n\\cos x_{1} & 0\\end{array}\\right]_{k}.\\]\nFor the simulations, we take:\n\n500 time steps with \\(\\Delta t=0.01.\\)\nNoise levels \\(q_{c}=0.01\\) and \\(R=0.1.\\)\nInitial angle \\(x_{1}=1.8\\) and initial angular velocity \\(x_{2}=0.\\)\nInitial diagonal state covariance of \\(0.1.\\)\n\nResults are plotted in Figure Figure 3.\n\nWe notice that despite the very noisy, nonlinear measurements, the EKF rapidly approaches the true state and then tracks it extremely well."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-20",
    "href": "slides/week-05/08-DA-Bayes.html#section-20",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Figure 3: Extended Kalman filter for tracking a noisy pendulum model, where horizontal position is measured. State, \\(x_k\\), $is solid blue curve; measurements, \\(y_k\\), are red circles; extended Kalman filter estimate is green curve. Results computed by EKfPendulum.m\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the above example, we have used a rather special form for the process noise covariance, \\(Q.\\) It cannot be computed exactly for nonlinear systems and some kind of approximations are needed.1 One way is to use an Euler-Maruyama method from SDEs, but this leads to singular dynamics where the particle smoothers will not work. Another approach, which was used here, is to first construct an approximate model and then compute the covariance using that model. In this case the approximate model was taken as \\[\\ddot{x}=w(t),\\] which is maybe overly simple, but works. Then the matrix \\(Q\\) is propagated through this simplified dynamics using an integration factor (exponential) solution and the corresponding power series expression of the transition matrix. Details of this can be found in [Grewal; Andrews].\n\n\n\nThanks to Simo Särkkä (private communication) for suggesting this explanation."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#unscented-kalman-filters",
    "href": "slides/week-05/08-DA-Bayes.html#unscented-kalman-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Unscented Kalman filters",
    "text": "Unscented Kalman filters\nThe unscented Kalman filter (UKF) was developed to overcome two shortcomings of the EKF:\n\nits difficulty to treat strong nonlinearities and\nits reliance on the computation of Jacobians.\n\nThe UKF is based on the unscented transform (UT), a method for approximating the distribution of a transformed variable, \\[\\mathbf{y}=g(\\mathbf{x}),\\] where \\(\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{m},\\mathbf{P}),\\) without linearizing the function \\(g.\\)\nThe UT is computed as follows:"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-21",
    "href": "slides/week-05/08-DA-Bayes.html#section-21",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Choose a collection of so-called\\(\\sigma\\)-pointsthat reproduce the mean and covariance of the distribution of \\(\\mathbf{x}\\).\nApply the nonlinearfunction to the \\(\\sigma\\)-points.\nEstimate themean and variance of the transformed random variable.\n\nThis is a deterministic sampling approach, as opposed to Monte Carlo, particle filters, and ensemble filters that all use randomly sampled points. Note that the first two usually require orders of magnitude more points than the UKF.\nSuppose that the random variable \\(\\mathbf{x}\\in\\mathbb{R}^{n}\\) with mean \\(\\mathbf{m}\\) and covariance \\(\\mathbf{P}.\\) Compute \\(N=2n+1\\) \\(\\sigma\\)-points and their corresponding weights \\[\\{\\mathbf{x}^{(\\pm i),},w^{(\\pm i)}\\},\\quad i=0,1,\\ldots,N\\] by the formulas"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-22",
    "href": "slides/week-05/08-DA-Bayes.html#section-22",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\mathbf{x}^{(0)} & =\\mathbf{m},\\\\\n\\mathbf{x}^{(\\pm i)} & =\\mathbf{m}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}^{(i)},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda},\\\\\nw^{(\\pm i)} & =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n,\n\\end{aligned}\\] where\n\n\\(\\mathbf{p}_{i}\\) is the \\(i\\)-th column of the square root of \\(\\mathbf{P},\\) which is the matrix \\(S\\) such that \\(SS^{\\mathrm{T}}=\\mathbf{P},\\) sometimes denoted as \\(\\mathbf{P}^{1/2},\\)\n\\(\\lambda\\) is a scaling parameter, defined as \\[\\lambda=\\alpha^{2}(n+\\kappa)-n,\\quad0&lt;\\alpha&lt;1,\\]\n\\(\\alpha\\) and \\(\\kappa\\) describe the spread of the \\(\\sigma\\)-points around the mean, with \\(\\kappa=3-n\\) usually,\n\\(\\beta\\) is used to include prior information on non-Gaussian distributions of \\(\\mathbf{x}.\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-23",
    "href": "slides/week-05/08-DA-Bayes.html#section-23",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "For the covariance matrix, the weight \\(w^{(0)}\\) is modified to \\[w^{(0)}=\\frac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right).\\] These points and weights ensure that the means and covariances are consistently captured by the UT."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#ukf-algorithm",
    "href": "slides/week-05/08-DA-Bayes.html#ukf-algorithm",
    "title": "Bayesian Data Assimilation",
    "section": "UKF — algorithm",
    "text": "UKF — algorithm\n\nTheorem 4 The UKF for the nonlinear system 5 computes a Gaussian approximation of the filtering distribution \\[p(\\mathbf{x}_{k}\\mid\\mathbf{y}_{1:k})\\approx\\mathcal{N}\\left(\\mathbf{x}_{k}\\mid\\mathbf{m}_{k},\\mathbf{P}_{k}\\right),\\] based on the UT, following the three-stage process:\nInitialization: Define the prior mean \\(\\mathbf{m}_{0},\\) prior covariance \\(\\mathbf{P}_{0}\\) and the parameters \\(\\alpha,\\) \\(\\beta,\\) \\(\\kappa.\\)\nPrediction:\nCompute the \\(\\sigma\\)-points and weights, \\[\\begin{aligned}\n\\mathbf{x}_{k-1}^{(0)} & =\\mathbf{m}_{k-1},\\\\\n\\mathbf{x}_{k-1}^{(\\pm i)} & =\\mathbf{m}_{k-1}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}_{k-1}^{(i)},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda},\\quad w^{(\\pm i)}  =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-24",
    "href": "slides/week-05/08-DA-Bayes.html#section-24",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Propagate the \\(\\sigma\\)-points through the dynamic model \\[\\tilde{\\mathbf{x}}_{k}^{(i)}=\\mathcal{M}_{k-1}\\left(\\mathbf{x}_{k-1}^{(i)}\\right).\\]\nCompute the predictive distribution mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k}^{-} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\tilde{\\mathbf{x}}_{k}^{(i)}\\\\\n\\mathbf{P}_{k}^{-} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left(\\tilde{\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)\\left(\\tilde{\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)^{\\mathrm{T}}+\\mathbf{Q}_{k-1}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-25",
    "href": "slides/week-05/08-DA-Bayes.html#section-25",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Correction:\nCompute the updated \\(\\sigma\\)-points and weights, \\[\\begin{aligned}\n\\mathbf{x}_{k}^{(0)} & =\\mathbf{m}_{k}^{-},\\\\\n\\mathbf{x}_{k}^{(\\pm i)} & =\\mathbf{m}_{k}^{-}\\pm\\sqrt{n+\\lambda}\\,\\mathbf{p}_{k}^{(i)-},\\quad i=1,2,\\ldots,n,\\\\\nw^{(0)} & =\\frac{\\lambda}{n+\\lambda}+\\left(1-\\alpha^{2}+\\beta\\right),\\\\\nw^{(\\pm i)} & =\\frac{\\lambda}{2\\left(n+\\lambda\\right)},\\quad i=1,2,\\ldots,n.\n\\end{aligned}\\]\nPropagate the updated \\(\\sigma\\)-points through the measurement model \\[\\tilde{\\mathbf{y}}_{k}^{(i)}=\\mathcal{H}_{k}\\left(\\mathbf{x}_{k}^{(i)}\\right).\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-26",
    "href": "slides/week-05/08-DA-Bayes.html#section-26",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Compute the predicted mean and innovation, predicted measurement covariance, state-measurement cross-covariance, and filter gain, \\[\\begin{aligned}\n\\boldsymbol{\\mu}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\tilde{\\mathbf{y}}_{k}^{(i)},\\quad\\textrm{the mean,}\\\\\n\\mathbf{d}_{k} & =\\mathbf{y}_{k}-\\boldsymbol{\\mu}_{k},\\quad\\textrm{the innovation,}\\\\\n\\mathbf{S}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}}+\\mathbf{R}_{k},\\textrm{ measur cov}\\\\\n\\mathbf{C}_{k} & =\\sum_{i=0}^{\\pm n}w^{(i)}\\left({\\mathbf{x}}_{k}^{(i)}-\\mathbf{m}_{k}^{-}\\right)\\left(\\tilde{\\mathbf{y}}_{k}^{(i)}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}},\\textrm{ s-m cross-cov},\\\\\n\\mathbf{K}_{k} & =\\mathbf{C}_{k}\\mathbf{S}_{k}^{-1},\\quad\\textrm{the Kalman gain.}\n\\end{aligned}\\]\nFinally, update the filter mean and covariance, \\[\\begin{aligned}\n\\mathbf{m}_{k} & =\\mathbf{\\hat{m}}_{k}+\\mathbf{K}_{k}\\mathbf{d}_{k},\\\\\n\\mathbf{P}_{k} & =\\hat{\\mathbf{P}}_{k}-\\mathbf{K}_{k}\\mathbf{S}_{k}\\mathbf{K}_{k}^{\\mathrm{T}}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-27",
    "href": "slides/week-05/08-DA-Bayes.html#section-27",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Just as was the case with the EKF, the UKF can also be applied to the non-additive noise model 6.\n\nThis is achieved by applying a non-additive version of the UT. Details can be found in (Särkkä and Svensson 2023)."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#particle-filters",
    "href": "slides/week-05/08-DA-Bayes.html#particle-filters",
    "title": "Bayesian Data Assimilation",
    "section": "Particle filters",
    "text": "Particle filters\nWhat happens if both the models are nonlinear and the pdfs are non Gaussian?\nThe Kalman filter and its extensions are no longer optimal and, more importantly, can easily fail the estimation process. Another approach must be used.\nA promising candidate is the particle filter (PF)\nThe particle filter (Doucet, Johansen, et al. 2009) (and references therein) works sequentially in the spirit of the Kalman filter, but unlike the latter, it handles an ensemble of states (the particles) whose distribution approximates the pdf of the true state.\nBayes’ rule and the marginalization formula, \\[p(x)=\\int p(x\\mid y)p(y)\\,\\mathrm{d}y,\\] are explicitly used in the estimation process.\nThe linear and Gaussian hypotheses can then be ruled out, in theory."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-28",
    "href": "slides/week-05/08-DA-Bayes.html#section-28",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "In practice though, the particle filter cannot yet be applied to very high dimensional systems (this is often referred to as “the curse of dimensionality”). Though recent work by [Friedemann, Raffin2023] has improved this by sophisticated parallel computing.\nParticle filters are methods for obtaining Monte Carlo approximationsof the solutions of the Bayesian filtering equations.\nRather than trying to compute the exact solution of the Bayesian filtering equations, the transformations of such filtering (Bayes’ rule for the analysis, model propagation for the forecast) are applied to the members of the sample.\n\nThe statistical moments are meant to be those of the targeted pdf.\nObviously this sampling strategy can only be exact in the asymptotic limit; that is, in the limit where the number of members (or particles) goes to infinity.\n\nThe most popular and simple algorithm of Monte Carlo type that solves the Bayesian filtering equations is called the bootstrap particle filter. It is computed by a three-stage process."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-29",
    "href": "slides/week-05/08-DA-Bayes.html#section-29",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Sampling\n\nWe consider a sample of particles \\(\\left\\{ \\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{M}\\right\\}\\). The related probability density function at time \\(t_{k}\\) is \\(p_{k}(\\mathbf{x}),\\) where \\[p_{k}(\\mathbf{x})\\simeq\\sum_{i=1}^{M}\\omega_{i}^{k}\\delta(\\mathbf{x}-\\mathbf{x}_{k}^{i})\\] and \\(\\delta\\) is the Dirac mass and the sum is meant to be an approximation of the exact density that the samples emulate. A positive scalar, \\(\\omega_{k}^{i},\\) weights the importance of particle \\(i\\) within the ensemble. At this stage, we assume that the weights \\(\\omega_{i}^{k}\\) are uniform and \\(\\omega_{i}^{k}=1/M\\)"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-30",
    "href": "slides/week-05/08-DA-Bayes.html#section-30",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Forecast\n\nAt the forecast step, the particles are propagated by the model without approximation, \\[p_{k+1}(\\mathbf{x})\\simeq\\sum_{i=1}^{M}\\omega_{k}^{i}\\delta(\\mathbf{x}-\\mathbf{x}_{k+1}^{i}),\\] with \\(\\mathbf{x}_{k+1}^{i}=\\mathcal{M}_{k+1}(\\mathbf{x}_{k}).\\) A stochastic noise can optionally be added to the dynamics of each particle."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-31",
    "href": "slides/week-05/08-DA-Bayes.html#section-31",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "Analysis\n\nThe analysis step of the particle filter is extremely simple and elegant. The rigorous implementation of Bayes’ rule ascribes to each particle a statistical weight that corresponds to the likelihood of the particle given the data. The weight of each particle is updated according to \\[\\omega_{k+1}^{\\mathrm{a},i}\\propto\\omega_{k+1}^{\\mathrm{f},i}p(\\mathbf{y}_{k+1}|\\mathbf{x}_{k+1}^{i})\\,.\\] It is remarkable that the analysis is carried out with only a few multiplications. It does not involve inverting any system or matrix, as opposed for instance to the Kalman filter.\n\n\nOne usually has to choose between\n\nlinear Kalman filters\nensemble Kalman filters\nnonlinear filters\nhybrid variational-filter methods."
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#section-32",
    "href": "slides/week-05/08-DA-Bayes.html#section-32",
    "title": "Bayesian Data Assimilation",
    "section": "",
    "text": "These questions are resumed in the following Table:\n\n\n\nTable 1: Decision matrix for choice of Kalman filters.\n\n\n\n\n\nEstimator\nModel type\npdf\nCPU-time\n\n\n\n\nKF\nlinear\nGaussian\nlow\n\n\nEKF\nlocally linear\nGaussian\nlow-medium\n\n\nUKF\nnonlinear\nGaussian\nmedium\n\n\nEnKF\nnonlinear\nGaussian\nmedium-high\n\n\nPF\nnonlinear\nnon-Gaussian\nhigh"
  },
  {
    "objectID": "slides/week-05/08-DA-Bayes.html#codes",
    "href": "slides/week-05/08-DA-Bayes.html#codes",
    "title": "Bayesian Data Assimilation",
    "section": "Codes",
    "text": "Codes\nVarious open-source repositories and codes are available for both academic and operational data assimilation.\n\nDARC: https://research.reading.ac.uk/met-darc/ from Reading, UK.\nDAPPER: https://github.com/nansencenter/DAPPER from Nansen, Norway.\nDART: https://dart.ucar.edu/ from NCAR, US, specialized in ensemble DA.\nOpenDA: https://www.openda.org/.\nVerdandi: http://verdandi.sourceforge.net/ from INRIA, France.\nPyDA: https://github.com/Shady-Ahmed/PyDA, a Python implementation for academic use.\nFilterpy: https://github.com/rlabbe/filterpy, dedicated to KF variants.\nEnKF; https://enkf.nersc.no/, the original Ensemble KF from Geir Evensen."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#inverse-problems",
    "href": "slides/week-03/03-adjoint.html#inverse-problems",
    "title": "Adjoint state",
    "section": "Inverse problems",
    "text": "Inverse problems\n\n\n\n\nfrom Asch (2022)\n\n\nIngredients of an inverse problem: the physical reality (top) and the direct mathematical model (bottom). The inverse problem uses the difference between the model- predicted observations, \\(u\\) (calculated at the receiver array points \\(x_r\\)), and the real observations measured on the array, in order to find the unknown model parameters, m, or the source s (or both)."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#adjoint-state-methods",
    "href": "slides/week-03/03-adjoint.html#adjoint-state-methods",
    "title": "Adjoint state",
    "section": "Adjoint-state Methods",
    "text": "Adjoint-state Methods\nA very general approach for solving inverse problems... including Machine Learning!\nVariational DA is based on an adjoint approach.\n\n\n\n\n\n\nDefinition 1 An adjoint is a general mathematical technique, based on variational calculus, that enables the computation of the gradient of an objective, or cost functional with respect to the model parameters in a very efficient manner."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#adjoint-statecontinuous-formulation",
    "href": "slides/week-03/03-adjoint.html#adjoint-statecontinuous-formulation",
    "title": "Adjoint state",
    "section": "Adjoint-state—continuous formulation",
    "text": "Adjoint-state—continuous formulation\nLet \\(\\mathbf{u}(\\mathbf{x},t)\\) be the state of a dynamical system whose behavior depends on model parameters \\(\\mathbf{m}(\\mathbf{x},t)\\) and is described by a differential operator equation \\[\\mathbf{L}(\\mathbf{u},\\mathbf{m})=\\mathbf{f},\\] where \\(\\mathbf{f}(\\mathbf{x},t)\\) represents external forces.\nDefine a cost function \\(J(\\mathbf{m})\\) as an energy functional1 or, more commonly, as a misfit functional that quantifies the error (\\(L^{2}\\)-distance2) between the observation and the model prediction \\(\\mathbf{u}(\\mathbf{x},t;\\mathbf{m}).\\) For example, \\[J(\\mathbf{m})=\\int_{0}^{T}\\int_{\\Omega}\\left(\\mathbf{u}(\\mathbf{x},t;\\mathbf{m})-\\mathbf{u}^{\\mathrm{obs}}(\\mathbf{x},t)\\right)^{2}\\,\\mathrm{d}\\mathbf{x}\\:\\mathrm{d}t,\\] where \\(\\mathbf{x}\\in\\Omega\\subset\\mathbb{R}^{n},\\,n=1,2,3,\\) and \\(0\\le t\\le T.\\)\nA functional is a generalization of a function. The functional depends on functions, whereas a function depends on variables. We then say that a functional is mapping from a space of functions into the real numbers.The \\(L^{2}\\)-space is a Hilbert space of (measurable) functions that are square-integrable (in Lebesgue sense)."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#adjoint-statecontinuous-formulation-1",
    "href": "slides/week-03/03-adjoint.html#adjoint-statecontinuous-formulation-1",
    "title": "Adjoint state",
    "section": "Adjoint-state—continuous formulation",
    "text": "Adjoint-state—continuous formulation\nOur objective is to choose the model parameters \\(\\mathbf{m}\\) as a function of the observed output \\(\\mathbf{u}^{\\mathrm{obs}},\\) such that the cost function \\(J(\\mathbf{m})\\) is minimized.\nThe minimization is most frequently performed by a gradient-based method, the simplest of which is steepest gradient, though usually some variant of a quasi-Newton approach is used, see Asch (2022), Wright (2006).\nIf we can obtain an expression for the gradient, then the minimization will be considerably facilitated.\nThis is the objective of the adjoint method that provides an explicit formula for the gradient of \\(J(\\mathbf{m}).\\)"
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#adjoint-methodsoptimization-formulation",
    "href": "slides/week-03/03-adjoint.html#adjoint-methodsoptimization-formulation",
    "title": "Adjoint state",
    "section": "Adjoint methods—optimization formulation",
    "text": "Adjoint methods—optimization formulation\nSuppose we are given a (P)DE,\n\\[F(\\mathbf{u};\\mathbf{m})=0, \\tag{1}\\]\nwhere\n\n\\(\\mathbf{u}\\) is the state vector,\n\\(\\mathbf{m}\\) is the parameter vector, and\n\\(F\\) includes the partial differential operator \\(\\mathbf{L},\\) the right-hand side (source) \\(\\mathbf{f},\\) boundary and initial conditions.\n\nNote that the components of \\(\\mathbf{m}\\) can appear as any combination of\n\ncoefficients in the equation,\nthe source,\nor as components of the boundary/initial conditions."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-1",
    "href": "slides/week-03/03-adjoint.html#section-1",
    "title": "Adjoint state",
    "section": "",
    "text": "To solve this very general parameter estimation problem, we are given a cost function \\(J(\\mathbf{u};\\mathbf{m}).\\)\nThe constrained optimization problem is then \\[\\begin{cases}\n        \\underset{\\mathbf{m}}{\\operatorname{minimize}} & J\\left(\\mathbf{u}(\\mathbf{m});\\mathbf{m}\\right)\\\\\n        \\mathrm{subject\\,to} & F(\\mathbf{u};\\mathbf{m})=0,\n        \\end{cases}\\] where \\(J\\) can depend on both \\(\\mathbf{u}\\) and on \\(\\mathbf{m}\\) explicitly in the presence of eventual regularization terms.\n\n\n\n\n\n\nNote\n\n\n\nthe constraint is a (partial) differential equation and\nthe minimization is with respect to a (vector) function.\n\n\n\n\nThis type of optimization is the subject of variational calculus, a generalization of classical calculus where differentiation is performed with respect to a variable, not a function."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-2",
    "href": "slides/week-03/03-adjoint.html#section-2",
    "title": "Adjoint state",
    "section": "",
    "text": "The gradient of \\(J\\) with respect to \\(\\mathbf{m}\\) (also known as the sensitivity) is then obtained by the chain-rule, \\[\\nabla_{\\mathbf{m}}J=\\frac{\\partial J}{\\partial\\mathbf{u}}\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{m}}+\\frac{\\partial J}{\\partial\\mathbf{m}}.\\]\n\nThe partial derivatives of \\(J\\) with respect to \\(\\mathbf{u}\\) and \\(\\mathbf{m}\\) are readily computed from the expression for \\(J,\\)\nbut the derivative of \\(\\mathbf{u}\\) with respect to \\(\\mathbf{m}\\) requires a potentially very large number of evaluations, corresponding to the product of the dimensions of \\(\\mathbf{u}\\) and \\(\\mathbf{m}\\) that can both be very large.\n\nThe adjoint method is a way to avoid calculating all of these derivatives.\nWe use the fact that if \\(F(\\mathbf{u};\\mathbf{m})=0\\) everywhere, then this implies that the total derivative of \\(F\\) with respect to \\(\\mathbf{m}\\) is equal to zero everywhere too."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-3",
    "href": "slides/week-03/03-adjoint.html#section-3",
    "title": "Adjoint state",
    "section": "",
    "text": "Differentiating the PDE 1, we can thus write \\[\\frac{\\partial F}{\\partial\\mathbf{u}}\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{m}}+\\nabla_{\\mathbf{m}}F=0.\\]\nThis can be solved for the untractable derivative of \\(\\mathbf{u}\\) with respect to \\(\\mathbf{m},\\) to give \\[\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{m}}=-\\left(\\frac{\\partial F}{\\partial\\mathbf{u}}\\right)^{-1}\\nabla_{\\mathbf{m}}F\\] assuming that the inverse of \\(\\partial F/\\partial{\\mathbf{u}}\\) exists.\nSubstituting in the expression for the gradient of \\(J,\\) we obtain \\[\\begin{aligned}\n    \\nabla_{\\mathbf{m}}J & =-\\frac{\\partial J}{\\partial\\mathbf{u}}\\left(\\frac{\\partial F}{\\partial\\mathbf{u}}\\right)^{-1}\\nabla_{\\mathbf{m}}F+\\frac{\\partial J}{\\partial\\mathbf{m}},\\\\\n     & =\\mathbf{p}\\nabla_{\\mathbf{m}}F+\\frac{\\partial J}{\\partial\\mathbf{m}}\n    \\end{aligned}\\]"
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-4",
    "href": "slides/week-03/03-adjoint.html#section-4",
    "title": "Adjoint state",
    "section": "",
    "text": "where we have conveniently defined \\(\\mathbf{p}\\) as the solution of the adjoint equation \\[\\left(\\frac{\\partial F}{\\partial\\mathbf{u}}\\right)^{\\mathrm{T}}\\mathbf{p}=-\\frac{\\partial J}{\\partial\\mathbf{u}}.\\]"
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#adjoint-methodssumming-up",
    "href": "slides/week-03/03-adjoint.html#adjoint-methodssumming-up",
    "title": "Adjoint state",
    "section": "Adjoint Methods—summing up",
    "text": "Adjoint Methods—summing up\nIn summary, we have a three-step procedure that combines a model-based approach (through the PDE) with a data-driven approach (through the cost function):\n\nSolve the adjoint equation for the adjoint state, \\(\\mathbf{p}.\\)\nUsing the adjoint state, compute the gradient of the cost function \\(J.\\)\nUsing the gradient, solve the optimization problem to estimate the parameters \\(\\mathbf{m}\\) that minimize the mismatch between model and observations.\n\n\n\n\n\n\n\nImportant\n\n\nThis key result enables us to compute the desired gradient \\(\\nabla_\\mathbf{m}J\\), without the explicit knowledge of the variation of \\(\\mathbf{u}\\)."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-5",
    "href": "slides/week-03/03-adjoint.html#section-5",
    "title": "Adjoint state",
    "section": "",
    "text": "A number of important remarks can be made.\n\nWe obtain explicit formulas — in terms of the adjoint state — for the gradient with respect to each/any model parameter. Note that this has been done in a completely general setting, without any restrictions on the operator \\({F}\\) or on the model parameters \\(\\mathbf{m}.\\)\nThe computational cost is one solution of the adjoint equation which is usually of the same order as (if not identical to) the direct equation, but with a reversal of time. Note that for nonlinear equations this may not be the case and one may require four or five times the computational effort.\nThe variation (Gâteaux derivative) of \\({F}\\) with respect to the model parameters \\(\\mathbf{m}\\) is, in general, straightforward to compute.\nWe have not explicitly considered boundary (or initial) conditions in the above, general approach. In real cases, these are potential sources of difficulties for the use of the adjoint approach — see Section 8.7.3, where the the discrete adjoint can provide a way to overcome this hurdle."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-6",
    "href": "slides/week-03/03-adjoint.html#section-6",
    "title": "Adjoint state",
    "section": "",
    "text": "For complete mathematical rigor, the above development should be performed in an appropriate Hilbert space setting that guarantees the existence of all the inner products and adjoint operators. The interested reader could consult Tröltzsch (2010).\nIn many real problems, the optimization of the misfit functional leads to multiple local minima and often to very “flat” cost functions. These are hard problems for gradient-based optimization methods. These difficulties can be (partially) overcome by a panoply of tools:\n\nRegularization terms can alleviate the non-uniqueness problem.\nRescaling the parameters and/or variables in the equations can help with the “flatness.” This technique is often employed in numerical optimization.\nHybrid algorithms, that combine stochastic and deterministic optimization (e.g., Simulated Annealing), can be used to avoid local minima.\nJudicious use of machine learning techniques and methods."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#section-7",
    "href": "slides/week-03/03-adjoint.html#section-7",
    "title": "Adjoint state",
    "section": "",
    "text": "When measurement and modeling errors can be modeled by Gaussian distributions and a background (prior) solution exists, the objective function may be generalized by including suitable covariance matrices. This is the approach employed systematically in data assimilation, see below."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#adjoint-state-methoddiscrete-systems",
    "href": "slides/week-03/03-adjoint.html#adjoint-state-methoddiscrete-systems",
    "title": "Adjoint state",
    "section": "Adjoint-state method—Discrete systems",
    "text": "Adjoint-state method—Discrete systems\nIn practice, we often work with discretized systems of equations, e.g. discretized PDEs.\nAssume that \\(\\mathbf{x}\\) depends, as usual on a parameter vector, \\(\\mathbf{m}\\), made up of \\(p\\)\n\ncontrol variables, design parameters, or decision parameters\n\nGoal: To optimize these values for a given cost function, \\(J(\\mathbf{x}, \\mathbf{m})\\), we need to\n\nevaluate \\(J(\\mathbf{x}, \\mathbf{m})\\) and compute, as for the continuous case, the gradient \\(\\mathrm{d}J/\\mathrm{d}\\mathbf{m}\\).\n\nPossible with an adjoint-state method at a cost that is independent of \\(p\\).\nInvolves inversion of linear system at \\(\\mathcal{O}(n^3)\\) operations — making it computational feasible to deal with problems of this type\n\\[A[\\mathbf{m}]\\mathbf{x}=\\mathbf{b} \\tag{2}\\]\nwhere \\(A[\\mathbf{m}]\\) represents the discretized PDE, \\(\\mathbf{m}\\) spatially varying coefficients of the PDE, and \\(\\mathbf{b}\\) a source term."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#automatic-differentiation",
    "href": "slides/week-03/03-adjoint.html#automatic-differentiation",
    "title": "Adjoint state",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nGradients w.r.t \\(\\mathbf{m}\\) of objectives that include \\(A[\\mathbf{m}]\\), e.g.\n\\[J(\\mathbf{m})=\\frac{1}{2}\\|\\mathbf{b}-PA^{-1}[\\mathbf{m}]\\mathbf{q}\\|^2_2\\]\ncan be calculated with the adjoint-state method leading to\n\nfast and scalable codes (e.g. Full-waveform inversion, see also JUDI.jl)\nbut coding can be involved especially when combined with machine learning (e.g. when \\(\\mathbf{m}\\) is parameterized by a neural network)\n\nUse automatic differentiation (AD) tools instead of analytical tools.\nTwo approaches to compute the adjoint state and the resulting gradients\n\nContinuous case — discretization of the (analytical) adjoint, denoted by AtD = adjoint then discretize;\nDiscrete case — adjoint of the discretization (the code), denoted as DtA = discretize then adjoint."
  },
  {
    "objectID": "slides/week-03/03-adjoint.html#problems-ad",
    "href": "slides/week-03/03-adjoint.html#problems-ad",
    "title": "Adjoint state",
    "section": "Problems AD",
    "text": "Problems AD\nRealistic (complex and large scale) solved with DtA using AD — provided by ML\n\nPyTorch, TensorFlow (Python), or Flux.jl/Zygote.jl (Julia)\n\n\n\n\n\n\n\nCaution\n\n\nWhile convenient, AD as part of ML tools (known as backpropagation)\n\nmay not scale\nintegrates poorly with existing CSE codes\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nSolution: Combine hand-derived gradients (via the adjoint-state method) w/ Automatic Differentiation\n\nuse ChainRules.jl to include hand-derived gradients in AD systems (e.g. Zygote.jl)\nknown as intrusive automatic differentiation (Li et al. 2020), see e.g. Louboutin et al. (2023) for Julia implementation\n\n\n\n\nWe will devote a separate lecture on AD."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#motivation",
    "href": "slides/week-04/06-Bayes-Inference.html#motivation",
    "title": "Statistical Inverse Problems",
    "section": "Motivation",
    "text": "Motivation\nSo, far we dealt with deterministic problems\n\nproduce single answer\nassume there was no noise\nno uncertainties in forward model\n\nReality is different\n\nneed to handle uncertainty\nBayesian inference provides a framework for principled handling of uncertainty\nproduces estimate for the posterior distribution—i.e, offers complete statistical description of all parameter values that are consistent with the data."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#uncertainty-quantification",
    "href": "slides/week-04/06-Bayes-Inference.html#uncertainty-quantification",
    "title": "Statistical Inverse Problems",
    "section": "Uncertainty Quantification",
    "text": "Uncertainty Quantification\nDefinition 11.1 (UQ). UQ is the composite task of assessing uncertainty in the computational estimate of a given quantity of interest (QoI).\nDefinition 11.2 (QoI). In an uncertainty quantification problem, we seek statistical information about a chosen output of interest. This statistic is know as the quantity of interest (QoI) and is usually defined by a high-dimensional integral. The QoI accounts for uncertainty in the model or process.\nWe can decompose UQ into a number of subtasks:\n\nQuantify uncertainties in model inputs by specifying probability distributions.\nPropagate input uncertainties through the model and quantify effects on the QoI.\nQuantify variability in the true physical QoI\nAggregate uncertainties arising from different sources to appraise overall uncertainty (input, model, numerical errors, inherent variability)\nCompute sensitivities of output variables with respect to input variables (forward UQ)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#account-for-uncertainty",
    "href": "slides/week-04/06-Bayes-Inference.html#account-for-uncertainty",
    "title": "Statistical Inverse Problems",
    "section": "Account for uncertainty",
    "text": "Account for uncertainty\n\nSolve the deterministic model, with random perturbations of parameters/initial conditions/forcing;\nFormulate and solve an inverse problem to seek the model parameters that minimize the difference between measurements and simulated observations. This can be achieved by variational and by statistical approaches;\nStarting from some prior knowledge (probabilities), use Bayesian analysis to compute a posterior distribution;\n\nFor UQ, we seek not just point estimates of the optimal, best-fit parameters but a complete statistical description of all parameter values that are consistent with the data;\nThe Bayesian approach does this by\n\nreformulating the inverse problem as a problem in statistical inference,\nincorporating uncertainties in the measurements, the forward model, and any prior information about the parameters."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section",
    "href": "slides/week-04/06-Bayes-Inference.html#section",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "The solution of this inverse problem is the set of so-called posterior probability densities of the parameters, describing updated uncertainty in the model parameters.\nResulting uncertainty in the model parameters can be quantified, taking into account uncertainties\n\nin the data\nin the model,\nand prior information.\n\nA major challenge is uncertainty quantification for the large-scale design and inverse problems that underlie digital twins.\nIn fact Bayes’ Law provides not only the solution to an inverse problem,\n\nbut also a complete quantification of the solution’s uncertainty.\nhowever, this can have a high computational cost because it most often relies on some kind of Monte Carlo simulation."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-approach",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-approach",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nBayesian analysis provides approach for models to learn from data\n\nCapture our prior knowledge about a model.\nRigorously assess the plausibility of candidate model classes based on system data.\nFinally, we can make robust probabilistic predictions that incorporate all uncertainties, allowing for better decision making and design.\n\nRelevant for system identification, or parameter estimation, where the inverse problems are often ill-posed and many candidate models exist to describe the behavior of a system.\n\nEstimation of uncertain parameters.\nPrediction of future events.\nTests of hypotheses.\nDecision making."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-inference",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-inference",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nIn the most general case, where we want to perform Bayesian inference for the estimation of parameters (an inverse problem!), we simply replace the probabilities by the corresponding density functions.\nThen Bayesian inference is performed in three steps:\n\nChoose a probability density \\(f(\\theta),\\) called the prior distribution, that expresses our beliefs, or prior experimental or historical knowledge, about a parameter \\(\\theta\\) before we see any data.\nChoose a statistical model \\(f(x\\mid\\theta)\\) that reflects our beliefs about \\(x\\) given \\(\\theta.\\) Notice that this is expressed as a conditional probability, called the likelihood function, and not as a joint probability function.\nAfter observing data \\(x_{1},\\ldots,x_{n},\\) update our beliefs and calculate the posterior distribution \\(f(\\theta\\mid x_{1},\\ldots,x_{n}).\\)\n\nLet us look more closely at the three components of Bayes’ Law."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-1",
    "href": "slides/week-04/06-Bayes-Inference.html#section-1",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Definition 1 Prior Distribution. For a given statistical model that depends on a parameter \\(\\theta,\\) considered as random, the distribution assigned to \\(\\theta\\) before observing the other random variables of interest is called the prior distribution. This is just the marginal distribution of the parameter.\n\n\nDefinition 2 Posterior Distribution. For a statistical inference problem, with parameter \\(\\theta\\) and random sample \\(X_{1},\\ldots,X_{n},\\) the conditional distribution of \\(\\theta\\) given \\(X_{1}=x_{1},\\) \\(\\ldots,\\) \\(X_{n}=x_{n}\\) is called the posterior distribution of \\(\\theta.\\)\n\n\nDefinition 3 Likelihood Function. Suppose that \\(X_{1},X_{2},\\ldots,X_{n}\\) have a joint density \\[f(X_{1},X_{2},\\ldots,X_{n}\\mid\\theta).\\] Given observations \\(X_{1}=x_{1},\\) \\(X_{2}=x_{2},\\) \\(\\ldots,\\) \\(X_{n}=x_{n},\\) the likelihood function of \\(\\theta\\) is \\[L(\\theta)=L(\\theta\\mid x_{1},x_{2},\\ldots,x_{n})=f(x_{1},x_{2},\\ldots,x_{n}\\mid\\theta).\\]"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-2",
    "href": "slides/week-04/06-Bayes-Inference.html#section-2",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "If the \\(X_{i}\\) are i.i.d. with density \\(f(X_{i}\\mid\\theta),\\) then the joint density is a product and \\[L(\\theta\\mid x_{1},x_{2},\\ldots,x_{n})=\\prod_{i=1}^{n}f(x_{i}\\mid\\theta).\\]\nWe point out the following properties of the likelihood:\n\nThe likelihood is not a probability density function and can take values outside the interval \\([0,1].\\)\nLikelihood is an important concept in both frequentist and Bayesian statistics.\nLikelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric model. This will be very important when we will deal with parameter estimation, and inverse problems in general.\nThe likelihood measures the support (evidence) provided by the data for each possible value of the parameter. This means that if we compute the likelihood function at two points, \\(\\theta=\\theta_{1},\\) \\(\\theta=\\theta_{2},\\) and find that \\(L(\\theta_{1}\\mid x)&gt;L(\\theta_{2}\\mid x)\\), then the sample observed is more likely to have occurred if \\(\\theta=\\theta_{1}.\\) We say that \\(\\theta_{1}\\) is a more plausible value for \\(\\theta\\) than \\(\\theta_{2}.\\)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-3",
    "href": "slides/week-04/06-Bayes-Inference.html#section-3",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "For i.i.d. random variables, the log-likelihood is usually used, since it reduces the product to a sum.\nWe now formulate the general version of Bayes’ Theorem.\n\nSuppose that \\(n\\) random variables, \\(X_{1},\\ldots,X_{n},\\) form a random sample from a distribution with density, or probability function in the case of a discrete distribution, \\(f(x\\mid\\theta).\\) Suppose also that the unknown parameter, \\(\\theta,\\) has a prior pdf \\(f(\\theta).\\) Then the posterior pdf of \\(\\theta\\) is \\[f(\\theta\\mid x)=\\frac{f(x_{1}\\mid\\theta)\\cdots f(x_{n}\\mid\\theta)f(\\theta)}{f_{n}(x)},\\label{eq:bayes}\\] where \\(f_{n}(x)\\) is the marginal joint pdf of \\(X_{1},\\ldots,X_{n}.\\)\n\nIn this theorem,\n\nthe prior, \\(f(\\theta),\\) represents the credibility of, or belief in the values of the parameters we seek, without any consideration of the data/observations;\nthe posterior, \\(f(\\theta\\mid x),\\) is the credibility of the parameters with the data taken into account;"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-4",
    "href": "slides/week-04/06-Bayes-Inference.html#section-4",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "\\(f(x\\mid\\theta),\\) considered as a function of \\(\\theta\\), is the likelihood function, which is the probability that the data/observation could be generated by the model with a given value of the parameter;\nthe denominator, called the evidence, \\(f_{n}(x),\\) is the total probability of the data taken over all the possible parameter values, also called the marginal likelihood, or the marginal, and can be considered as a normalization factor;\nthe posterior distribution is thus proportional to the product of the likelihood and the prior distribution, or, in applied terms, \\[f(\\mathrm{parameter}\\mid\\mathrm{data})\\propto f(\\mathrm{data}\\mid\\mathrm{parameter})\\,f(\\mathrm{parameter}).\\]\n\nWhat can one do with the posterior distribution thus obtained? The answer is a lot of things, in fact a complete quantification of the incertitude of the parameter’s estimation is possible. We can compute:\n\nPoint estimates by summarizing the center of the posterior. Typically, these are the posterior mean or the posterior mode."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-5",
    "href": "slides/week-04/06-Bayes-Inference.html#section-5",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Interval estimates for a given level \\(\\alpha\\).\nEstimates of the probability of an event, such as \\(\\mathrm{P}(a&lt;\\theta&lt;b)\\) or \\(P(\\theta&gt;b).\\)\nPosterior quantiles.\n\n\n\n\n\n\n\nNote\n\n\nBayesian Inference is a general framework that plays a crucial rule in solving inverse and data-assimilation problems. It also plays a major role in uncertainty quantification. These will be discussed\n\nwhen we introduce Bayesian filtering\nextensions of Kalman filtering\nsimulation-based inference that leverages the ability of deep neural networks to capture the posterior from simulations"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-estimation-and-inverse-modeling",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-estimation-and-inverse-modeling",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian Estimation and Inverse Modeling",
    "text": "Bayesian Estimation and Inverse Modeling\nBayesian methods for identification and estimation are critical to the robust understanding of a system because\n\nthey allow us to quantify all of our uncertainty about the system using a probability distribution, and\nto update this distribution with new information.\n\nBy taking the Bayesian approach, we are able to effectively\n\nCapture our prior knowledge about a model.\nRigorously assess the plausibility of candidate model classes based on system data.\nFinally, we can make robust probabilistic predictions that incorporate all uncertainties, allowing for better decision making and design."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-6",
    "href": "slides/week-04/06-Bayes-Inference.html#section-6",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Given data, \\(\\mathcal{D}\\), we want to do three things:\n\nEstimate the parameters \\(\\boldsymbol{\\theta}\\) of a model \\(\\mathcal{M}\\), not as point or optimal values, but inferring their probability distribution. This is known as parameter estimation.\nSelect the best model by comparing different parametrizations using the model posterior distribution. This is known as model selection.\nPredict from the chosen model the data for some new input values. This is known as posterior prediction.\n\nBayesian parameter estimation (BPE) is a universal approach to fitting models to data\n\ndefine a generative model for the data,\na likelihood function, and\na prior distribution over the parameters.\n\nThe posterior rarely has closed form, so we characterize it by sampling, from which we can find the best model parameters together with their uncertainties."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayes-formula",
    "href": "slides/week-04/06-Bayes-Inference.html#bayes-formula",
    "title": "Statistical Inverse Problems",
    "section": "Bayes’ formula",
    "text": "Bayes’ formula\n\n\nAccording to Theorem 2.61, Bayes’ rule take the form\n\\[p(\\boldsymbol{\\theta}\\mid \\mathcal{D})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\mathcal{D})}=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\int_{\\boldsymbol{\\theta}}p(\\mathcal{D}\\mid\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}\\]\nwhere\n\n\\(p(\\mathcal{D}\\mid\\boldsymbol{\\theta})\\) is the conditional probability known as likelihood—.i.e, physical model generating data\n\\(p(\\boldsymbol{\\theta})\\) is the prior distribution\n\\(p(\\boldsymbol{\\theta}\\mid \\mathcal{D})\\) is the posterior probability distribution.\n\n\n\n\n\nfrom Asch (2022)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-7",
    "href": "slides/week-04/06-Bayes-Inference.html#section-7",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "Likelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric model—this will be very important in all the chapters where we will deal with parameter estimation.\nBayes’ theorem can be extended to include a model, \\(\\mathcal{M}\\), for the parameters yielding\n\\[p(\\boldsymbol{\\theta}\\mid \\mathcal{D},\\mathcal{M})=\\frac{p(\\mathcal{D}\\mid\\boldsymbol{\\theta},\\mathcal{M})p(\\boldsymbol{\\theta}\\mid\\mathcal{M})}{p(\\mathcal{D}\\mid \\mathcal{M})}\\]\nor in practical terms\n\\[p(\\text{parameter}\\mid\\text{data})\\propto p(\\text{data}\\mid \\text{parameter})p(\\text{parameter})\\]\nMost probable estimator is called the maximum a posteriori (MAP) estimator — the \\(\\boldsymbol{\\theta}\\) that maximizes the posterior—i.e.,\n\\[\\boldsymbol{\\theta}_\\ast=\\mathop{\\mathrm{arg\\,max}}_{\\boldsymbol{\\theta}}\\,p(\\boldsymbol{\\theta}\\mid \\mathcal{D})\\]"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-priors",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-priors",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian priors",
    "text": "Bayesian priors\nPriors can be classified in three broad categories:\n\nUninformative priors, which are flat/diffuse and have smallest impact on posterior.\nWeakly informative priors, use some knowledge, e.g. positivity.\nInformative priors, convey maximal information\n\n\n\nPriors can be sequential. Suppose we observe \\(y_1,\\dots, y_n\\) sequentially, then we can write\n\\[p(\\theta\\mid y_1,\\dots, y_m)\\propto p(\\theta\\mid y_1,\\dots, y_{m-1})p(y_m\\mid \\theta),\\]\n\\(m=2,3.\\dots,n\\) where the posterior distribution of the parameter \\(\\theta\\) after \\(m − 1\\) measurements acts as a prior with \\(p(y_m\\mid \\theta)\\) the likelihood.\n\ncan be applied recursively\nholds when \\(\\theta\\) does not vary with time\n\n\n\n\n\nfrom Asch (2022)"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-regression",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-regression",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nModel relation between a dependent variable, \\(y\\), and observed independent variables, \\(x_1,x_2,\\dots x_p\\) that represent properties of a process.\n\\[y=f(x;\\theta)\\]\nForward/direct problem: compute \\(y\\) given \\(\\theta\\) (easy)\nInverse/indirect problem: compute \\(\\theta\\) given \\(y\\) (difficult)\nwhere\n\n\\(f\\) is an operator/equation/system\n\\(x\\) is the independent variable\n\\(\\theta\\) is the parameter/feature\n\\(y\\) is the measurement/dependent variable"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-8",
    "href": "slides/week-04/06-Bayes-Inference.html#section-8",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "We have \\[\\mathcal{D}=\\left\\{(\\mathbf{x}_i,y_1),\\,i=1,\\dots n\\right\\}\\]\nthat can be solved by\n\nclassical least squares—i.e., find parameters \\(\\theta\\) that minimize the squared error.\nmaximum likelihood where we choose the parameters that maximize the likelihood of the parameters.\nBayesian regression, where we use Bayes’ formula to compute a posterior distribution of the parameters.\n\nMethods 1 and 2 yield point estimates while 3 provides full posterior."
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#bayesian-parameter-estimation-in-state-space-models",
    "href": "slides/week-04/06-Bayes-Inference.html#bayesian-parameter-estimation-in-state-space-models",
    "title": "Statistical Inverse Problems",
    "section": "Bayesian Parameter Estimation in State Space Models",
    "text": "Bayesian Parameter Estimation in State Space Models\n\\[\n\\begin{aligned}\n\\boldsymbol{\\theta} & \\sim p(\\boldsymbol{\\theta}) \\\\\n\\mathbf{x}_0 & \\sim p\\left(\\mathbf{x}_0 \\mid \\boldsymbol{\\theta}\\right) \\\\\n\\mathbf{x}_k & \\sim p\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}, \\boldsymbol{\\theta}\\right) \\\\\n\\mathbf{y}_k & \\sim p\\left(\\mathbf{y}_k \\mid \\mathbf{x}_k, \\boldsymbol{\\theta}\\right), \\quad k=0,1,2, \\ldots\n\\end{aligned}\n\\]\nApplying Bayes’ Law of Theorem 2.61, we can compute the complete posterior distribution of the state plus parameters as \\[\np\\left(\\mathbf{x}_{0: T}, \\boldsymbol{\\theta} \\mid \\mathbf{y}_{1: T}\\right)=\\frac{p\\left(\\mathbf{y}_{1: T} \\mid \\mathbf{x}_{0: T}, \\boldsymbol{\\theta}\\right) p\\left(\\mathbf{x}_{0: T} \\mid \\boldsymbol{\\theta}\\right) p(\\boldsymbol{\\theta})}{p\\left(\\mathbf{y}_{1: T}\\right)},\n\\] where the joint prior of the states, \\(\\mathbf{x}_{0: T}=\\left\\{\\mathbf{x}_0, \\ldots, \\mathbf{x}_T\\right\\}\\), and the joint likelihood of the measurements, \\(\\mathbf{y}_{1: T}=\\left\\{\\mathbf{y}_1, \\ldots, \\mathbf{y}_T\\right\\}\\), conditioned on the parameters, are"
  },
  {
    "objectID": "slides/week-04/06-Bayes-Inference.html#section-9",
    "href": "slides/week-04/06-Bayes-Inference.html#section-9",
    "title": "Statistical Inverse Problems",
    "section": "",
    "text": "\\[\np\\left(\\mathbf{x}_{0: T} \\mid \\boldsymbol{\\theta}\\right)=p\\left(\\mathbf{x}_0 \\mid \\boldsymbol{\\theta}\\right) \\prod_{k=1}^T p\\left(\\mathbf{x}_k \\mid \\mathbf{x}_{k-1}, \\boldsymbol{\\theta}\\right)\n\\] and \\[\np\\left(\\mathbf{y}_{1: T} \\mid \\mathbf{x}_{0: T}, \\boldsymbol{\\theta}\\right)=\\prod_{k=1}^T p\\left(\\mathbf{y}_k \\mid \\mathbf{x}_k, \\boldsymbol{\\theta}\\right)\n\\] respectively. Finally, we need to marginalize over the state to obtain the predictive posterior for the parameter \\(\\boldsymbol{\\theta}\\), \\[\np\\left(\\boldsymbol{\\theta} \\mid \\mathbf{y}_{1: T}\\right)=\\int p\\left(\\mathbf{x}_{0: T}, \\boldsymbol{\\theta} \\mid \\mathbf{y}_{1: T}\\right) \\mathrm{d} \\mathbf{x}_{0: T}\n\\]\nIntegrals are difficult to compute — will seek iterative (learned) methods to capture statistics"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Spring 2024 syllabus: Digital Twins for Physical Systems — 33655 - CSE 8803 - DTP\nSyllabus updates:This syllabus is subject to change throughout the semester. We will try to keep these updates to a minimum.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be made familiar with …\n\nthe concept of Digital Twins and how they interact with their environment\nStatistical Inverse Problems and Bayesian Inference\ntechniques from Data Assimilation (DAT), Simulation-Based Inference (SBI), and Recursive Bayesian Inference (RBI), and Uncertainty Quantification [UQ]\n\nFor more on the Course outline, Topics, and Learning goals, see Goals.\nFor a motivational article see Digital Twins in the era of generative AI",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\nData assimilation: methods, algorithms, and applications\nAsch, Bocquet, and Nodet (2016), Mark\nSIAM, 2016\n\n\nA toolbox for digital twins: from model-based to data-driven\nAsch (2022), Mark\nSIAM, 2022\n\n\nNotebook: Kalman and Bayesian Filters in Python\nRoger R. Labbe\nPrivate, 2020\n\n\nUnderstanding Deep Learning\nPrince (2023)\nMIT, 2024\n\n\n\n\n\nThese electronic books are only available when online at Georgia Tech!\n\nThe Python notebook is hands on, very practical, and contains very useful exercises with answers. You are encouraged to explore and we will incorporate material from this notebook in this course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#journal-papers",
    "href": "syllabus.html#journal-papers",
    "title": "Syllabus",
    "section": "Journal Papers",
    "text": "Journal Papers\n\n\n\nA comprehensive review of digital twin—part 1: modeling and twinning enabling technologies\nThelen et al. (2022)\nSpringer, 2022\n\n\nA comprehensive review of digital twin—part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives\nThelen et al. (2023)\nSpringer, 2022\n\n\nSequential Bayesian inference for uncertain nonlinear dynamic systems: A tutorial\nTatsis, Dertimanis, and Chatzi (2022)\nArxiv, 2022",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#lectures-computational-labs",
    "href": "syllabus.html#lectures-computational-labs",
    "title": "Syllabus",
    "section": "Lectures & computational labs",
    "text": "Lectures & computational labs\nIn part the course is an adaptation of open-source Basic and Advanced courses (CSU-IMU-2023) developed by Mark Asch. Codes will be drawn from the course and from the book. During the second half of the course, we will draw from the current literature.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Georgia Tech’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated.\n\n\nDiscrimination, Harassment, and Sexual Misconduct\nTo maintain a safe learning environment that fosters the dignity, respect, and success of students, faculty, and staff, Tech prohibits discriminatory harassment, which is unwelcome verbal, nonverbal, or physical conduct directed at any person or group based upon race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity, or veteran status that has the purpose or effect of creating an objectively hostile working or academic environment.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nDisability Services are available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website.\nAnnouncements will be emailed through Sakai Announcements periodically. Please check your email regularly to ensure you have the latest announcements for the course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\n\nComputational labs\nIn labs, you will apply the concepts discussed during lectures, with a focus on the computation.\n\n\nExams\nThere will be no exams.\n\n\nFinal Project\nThe purpose of the Final Project is to apply what you’ve learned throughout the semester to analyze an interesting data-driven research question.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nComp. Lab assignments\n30%\n\n\nPresentation Journal Paper\n20%\n\n\nFinal Project & Presentation\n50%",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 4: Phase II Registration\nJan 8: Classes begin\nJan 12: Deadline for Registration/Schedule Changes\nMarch 13: Last day to withdraw\nMarch 18-22: Spring break\nApril 24: Final Instructional Class",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nPlease abide by the following as you work on assignments in this course:\n\nYou may discuss individual lab assignments with other students; however, you may not directly share (or copy) code or write up with other students. For team assignments, you may collaborate freely within your team. You may discuss the assignment with other teams; however, you may not directly share (or copy) code or write up with another team. Unauthorized sharing (or copying) of the code or write up will be considered a violation for all students involved.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n\n\n\nGuidance on the use of Generative AI\nAI-based assistance, such as ChatGPT and Copilot, can be used in the same way as collaboration with other people for the following activities: For lab assignments in understanding the lecture and textbook materials, and in developing your ideals for the project proposals. In these activities you are welcome to talk about your ideas and work with other people, both inside and outside the class, as well as with AI- based assistants. In the course, we will discuss how you can use these tools as a study tool, or as an assistant in helping to draft code or summaries.\n\nGenerating Research Ideas or Approaches:\n\nBrainstorming: You can use the AI tool as a brainstorming partner, where you exchange ideas whether the AI prompts you or you prompt the AI for ideas. Brainstorming is an iterative process that can be made more effective with the way that the queries are posted. For more samples or information, post this query: “How can I use AI to help me to brainstorm an idea?”\nSurveying Existing Approaches: Large Language Models, if trained broadly in a topic, can give a good initial overview of existing approaches or existing literature on a topic. Current research sources such as library or professional society databases are more reliable in terms of accuracy of peer-reviewed content.\nPrompt engineering is important: Practice the prompts used for Generative AI. The value of the response depends on the value of the prompt. If you provide a low quality or vague prompt, you will get vague results. Varying skill levels among users might exacerbate existing inequalities among students. For example, students for whom English is the second language might be at a disadvantage.\n\n\n\nAdvice on Usage:\n\nBe very skeptical of the results. Do not trust any outputs that you cannot evaluate yourself or trace back to original credible sources. There are many stories of generative AI giving citations of articles that do not exist (see the article in the Chronicles of Higher Education referenced below).\nBe scientific with your prompts (or queries): Prompting is not deterministic, so the same prompt at a different time may result in a different response. Small changes in the wording of the prompt may yield very different responses. Keep records, make small changes and see how it affects the outcome, etc.\nDon’t share any data or information that is confidential, proprietary, or have IP implications. Your uploaded data or ideas might be incorporated into the learning model to be available for others in your research area, prior to you having a chance to publish it. If you intend to pursue commercialization or other Intellectual Property avenues for your work, putting the information into an open AI platform may be considered as disclosure.\n\n\n\n\nLate work & extensions\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest lab assignments will be dropped to accommodate such circumstances.\n\n\nCampus Resources for Students\nLinks to Campus Resources for Students — e.g. Center for Academic Success, Communication Center, Office of Disability Services, OMED, Division of Student Life, etc. available at: https://ctl.gatech.edu/sites/default/files/documents/campus_resources_students.pdf\n\nQuick Guide on Student Mental Health and Wellness Resources:\nThe Center for Mental Health Care & Resources is here to offer confidential support and services to students in need of mental health care. During regular business hours, students who are not actively in counseling may call 404-894-2575 or walk-in to the office , 353 Ferst DR NW Atlanta GA 30313 (Flag building next to the Student Center). Any time outside of business hours, students may call 404-894-2575 and select the option to speak to the after-hours counselor.\n\n\nAdditional On Campus Options\nStudents who are experiencing an immediate life-threatening emergency on campus, can call the Georgia Tech Campus Police at 404-894-2500\n\n\nLocal and National crisis resources available 24/7:\n\nCrisis Text Line: Text HOME to 741741 https://www.crisistextline.org/\nGeorgia Crisis & Access Line: Call 1-800-715-4225\nNational Suicide Prevention Lifeline: Call 988 or 1-800-273-8255, or access text chat here https://suicidepreventionlifeline.org/chat/\n\n\n\nAdditional services offered by Satellite counselors’ (walk-in consultation hours):\nEach Satellite counselor provides weekly consultation appointments at varying times based on their schedules (see highlighted section below for my Fall ’23 hours). Anyone can access a Satellite counselor during these times, to set up a consultation email is preferable, once a message is received the Satellite counselor will reply with available dates/times and a link. Some Satellite counselors will accommodate in-person walk-ins at these times as well.\n\nWhat it is\n\nConsulting about a brief, non-emergency concern during the counselor’s posted walk-in consultation hours (similar to Let’s Talk). Consultations are approximately 15 minutes.\nProviding information to students about mental health resources on campus and how to get connected\nConsulting with faculty/staff about a student of concern\n\n\n\nWhat it isn’t\n\nNot for a walk-in counseling appointment or initial assessment\nNot for students who are seeking support in their counselor’s absence. Students who need additional/urgent support when their counselor is unavailable should visit the Center for Mental Health Care & Resources\nNot for crisis\nNot for case management\n\nTara Holdampf, MS, LPC, (she/her), Satellite Counselor’s Consultation Hours for the College of Sciences: Please email tara.holdampf@studentlife.gatech.edu for an appointment, or stop by my Office Location: MoSE 1120B. Monday-Friday 3:00 PM - 4:00 PM\n\n\nOther useful web pages to explore:\nGT Wellness Hub: https://gtwellnesshub.com (self-paced online resources for students)\n\nFree Headspace (for Students Only): https://gtwellnesshub.com/personal-guide-to-health- happiness/\nTogetherall (for Students Only): https://gtwellnesshub.com/one-on-one-personal-assistance/\nSilver cloud (for Students Only): https://gtwellnesshub.com/helpful-resources-right-at-your- fingertips/\n\nDivision of Student Engagement and Wellbeing: https://students.gatech.edu/\nHealth, Wellness & Recreation: https://students.gatech.edu/health-wellness-recreation\nTech Ends Suicide: https://mentalhealth.gatech.edu/end-suicide-initiative\nMental Health Services: https://mentalhealth.gatech.edu/about/services",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final Project",
    "section": "",
    "text": "Final Project\n\nGoal\nThe goal of the Final Project is to give students the opportunity to apply and further develop their understanding of Digital Twins to a topic close to their graduate research interests.\n\n\nRequirements\nThe Final Project consist of three parts\n\nProposal consisting of a “one pager” + “5 min pitch”\nProject presentation of 20 minutes with 5 minutes questions, presented at the end of the term\nA final report\n\n\nOne pager + 5 min pitch\nOn March 11th, everybody needs to give a 5 minute pitch on their project proposal. That same day, the “one pager” outlining the project is due by 6:00 PM. See description below. Your seminar will be evaluated with this form. Please send the slides for your presentation to the course instructor with CSE 8803 in the subject line or bring a memory stick. The 5 minutes presentations include time for questions. Since time is limited, the 5 minute limit will be strictly enforced. So please stick to the main message, the problem you want to tackle, what techniques you will be using, and why it is important.\n\n\nProposal\nConcise description of the work to be undertaken. The description should not exceed two pages and should contain\n\nTitle of the Final Project\nProblem Statement\nRelevance\nIdentification of methodology that will be used to solve the problem\n\nProposal should be submitted in PDF format or a link to an html page. The project proposals are due March 13 at 6:00 PM and presentations are scheduled for that same day during class. The presentations are five minutes each for each person.\n\n\nProject presentation at the end of the term\nEach project presentation will be allotted 20 minutes precisely with 5 minutes of questions. The seminar should reflect the current status of the project, which may be subject to change when the project is finalized.\nThe seminar will be evaluated using the following form2.\n\n\nFinal Report\nThe report needs to be written as a short journal paper. Students will be required to use the IEEE template in two-column mode. The paper should be four-to-six pages including abstract, body text, figures, and bibliography. Reports should consist of\n\nabstract\nintroduction\nproblem statement\nmethodology\nresults\nconclusions\nreferences\nappendices\n\n\n\n\nGrading (project alone)\n\n10% Proposal plus speed presentation\n30% Seminar\n60% Final Report\n\n\n\nLate policy\nLate submission is unacceptable and will lead to deductions in your grade. The deadline for submission will be communicate in class.\n\n\n\n\nCalendar for events\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "Final Project"
    ]
  }
]