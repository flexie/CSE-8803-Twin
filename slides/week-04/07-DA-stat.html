<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.528">

  <meta name="dcterms.date" content="2024-01-31">
  <title>Digital Twins for Physical Systems - Data Assimilation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="../images/3DVAR.png" data-background-opacity="0.5" data-background-size="contain" class="quarto-title-block center">
  <h1 class="title">Data Assimilation</h1>
  <p class="subtitle">statistical data assimilation</p>

<div class="quarto-title-authors">
</div>

  <p class="date">2024-01-31</p>
</section>
<section id="statistical-da-introduction" class="slide level2 center">
<h2>Statistical DA: introduction</h2>
<p>Now we will generalize the variational approach to deal with <span style="color: magenta">errors and noise</span> in</p>
<ul>
<li><p>the models,</p></li>
<li><p>the observations and</p></li>
<li><p>the initial conditions.</p></li>
</ul>
<p>The variational results could of course be derived as a special case of statistical DA, in the limit where the noise disappears.</p>
<p>Even the statistical results can be derived in a very general way, using SDEs and/or Bayesian analysis, and then specialized to the various Kalman-type filters that we will study here.</p>
<p>Practical inverse problems and data assimilation problems involve measured data.</p>
<ul>
<li>These data are inexact and are mixed with <span style="color: magenta">random noise</span>.</li>
</ul>
</section>
<section id="section" class="slide level2 center">
<h2></h2>
<ul>
<li>Only <span style="color: magenta">statistical models</span> can provide rigorous, effective means for dealing with this measurement error.</li>
</ul>
<p>We want to <span style="color: magenta">estimate a scalar quantity</span>, say the temperature or the ozone level, at a fixed point in space.</p>
<div class="hidden">

</div>
<p>Suppose we have:</p>
<ul>
<li><p>a model forecast, <span class="math inline">\(x^{\mathrm{b}}\)</span> (<span style="color: blue">background</span>, or <em>a priori</em> value)</p></li>
<li><p>and a measured value, <span class="math inline">\(x^{\mathrm{obs}}\)</span> (<span style="color: blue">observation</span>).</p></li>
</ul>
<p>The simplest possible approach is to try a <span style="color: magenta">linear combination</span> of the two, <span class="math display">\[x^{\mathrm{a}}=x^{\mathrm{b}}+w(x^{\mathrm{obs}}-x^{\mathrm{b}}),\]</span> where <span class="math inline">\(x^{\mathrm{a}}\)</span> denotes the <span style="color: red">analysis</span>that we seek and <span class="math inline">\(0\le w\le1\)</span> is a <span style="color: magenta">weight</span> factor. We subtract the (always unknown) true state <span class="math inline">\(x^{\mathrm{t}}\)</span> from both sides, <span class="math display">\[x^{\mathrm{a}}-x^{\mathrm{t}}=x^{\mathrm{b}}-x^{\mathrm{t}}+w(x^{\mathrm{obs}}-x^{\mathrm{t}}-x^{\mathrm{b}}+x^{\mathrm{t}})\]</span></p>
</section>
<section id="section-1" class="slide level2 center">
<h2></h2>
<p>and defining the three <span style="color: magenta">errors</span> (analysis, background, observation) as <span class="math display">\[e^{\mathrm{a}}=x^{\mathrm{a}}-x^{\mathrm{t}},\quad\mathrm{e^{b}}=x^{\mathrm{b}}-x^{\mathrm{t}},\quad e^{\mathrm{obs}}=x^{\mathrm{obs}}-x^{\mathrm{t}},\]</span> we obtain <span class="math display">\[e^{\mathrm{a}}=e^{\mathrm{b}}+w(e^{\mathrm{obs}}-e^{\mathrm{b}})=we^{\mathrm{obs}}+(1-w)e^{\mathrm{b}}.\]</span> If we have many realizations, we can take an <span style="color: magenta">ensemble average</span>, or expectation, denoted by <span class="math inline">\(\left\langle \cdot\right\rangle ,\)</span> <span class="math display">\[\left\langle e^{\mathrm{a}}\right\rangle =\left\langle e^{\mathrm{b}}\right\rangle +w(\left\langle e^{\mathrm{obs}}\right\rangle -\left\langle e^{\mathrm{b}}\right\rangle ).\]</span> Now if these errors are centred (have zero mean, or the estimates of the true state are <span style="color: magenta">unbiased</span>), then <span class="math display">\[\left\langle e^{\mathrm{a}}\right\rangle =0\]</span> also. So we must look at the <span style="color: magenta">variance</span> and demand that it be as small as possible. The variance is defined, using the above notation, as</p>
</section>
<section id="section-2" class="slide level2 center">
<h2></h2>
<p><span class="math display">\[\sigma^{2}=\left\langle \left(e-\left\langle e\right\rangle \right)^{2}\right\rangle .\]</span> Now, taking variances of the error equation, and using the zero-mean property, we obtain <span class="math display">\[\sigma_{\mathrm{a}}^{2}=\sigma_{\mathrm{b}}^{2}+w^{2}\left\langle \left(e^{\mathrm{obs}}-e^{\mathrm{b}}\right)^{2}\right\rangle +2w\left\langle e^{\mathrm{b}}\left(e^{\mathrm{obs}}-e^{\mathrm{b}}\right)\right\rangle .\]</span> This reduces to <span class="math display">\[\sigma_{\mathrm{a}}^{2}=\sigma_{\mathrm{b}}^{2}+w^{2}\left(\sigma_{\mathrm{o}}^{2}+\sigma_{\mathrm{b}}^{2}\right)-2w\sigma_{\mathrm{b}}^{2}\]</span> if <span class="math inline">\(e^{\mathrm{o}}\)</span> and <span class="math inline">\(e^{\mathrm{b}}\)</span> are <span style="color: magenta">uncorrelated</span>.</p>
<p>Now, to compute a <span style="color: magenta">minimum</span>, take the derivative with respect to <span class="math inline">\(w\)</span> and equate to zero, to obtain <span class="math display">\[0=2w\left(\sigma_{\mathrm{obs}}^{2}+\sigma_{\mathrm{b}}^{2}\right)-2\sigma_{\mathrm{b}}^{2},\]</span></p>
</section>
<section id="section-3" class="slide level2 center">
<h2></h2>
<p>where we have ignored all cross terms (errors are assumed independent). Finally, solving this last equation, we can write the <span style="color: red">optimal weight</span>, <span class="math display">\[w_{*}=\frac{\sigma_{\mathrm{b}}^{2}}{\sigma_{\mathrm{obs}}^{2}+\sigma_{\mathrm{b}}^{2}}=\frac{1}{1+\sigma_{\mathrm{o}}^{2}/\sigma_{\mathrm{b}}^{2}}\]</span> which depends on the <span style="color: magenta">ratio</span> of the background and the observation errors. Clearly <span class="math inline">\(0\le w_{*}\le1\)</span> and</p>
<ul>
<li><p>if the observation is perfect, <span class="math inline">\(\sigma_{\mathrm{obs}}^{2}=0\)</span> and thus <span class="math inline">\(w_{*}=1,\)</span> the maximum weight;</p></li>
<li><p>if the background is perfect, <span class="math inline">\(\sigma_{\mathrm{b}}^{2}=0\)</span> and <span class="math inline">\(w_{*}=0,\)</span> so the observation will not be taken into account.</p></li>
</ul>
<p>We can now rewrite the analysis error variance as,</p>
</section>
<section id="section-4" class="slide level2 center">
<h2></h2>
<p><span class="math display">\[\begin{aligned}
\sigma_{\mathrm{a}}^{2} &amp; =w_{*}^{2}\sigma_{\mathrm{obs}}^{2}+(1-w_{*})^{2}\sigma_{\mathrm{b}}^{2}\\
&amp; =\frac{\sigma_{\mathrm{b}}^{2}\sigma_{\mathrm{obs}}^{2}}{\sigma_{\mathrm{obs}}^{2}+\sigma_{\mathrm{b}}^{2}}\\
&amp; =(1-w_{*})\sigma_{\mathrm{b}}^{2}\\
&amp; =\frac{1}{\sigma_{\mathrm{obs}}^{-2}+\sigma_{\mathrm{b}}^{-2}},
\end{aligned}\]</span> where we suppose that <span class="math inline">\(\sigma_{\mathrm{b}}^{2},\;\sigma_{\mathrm{o}}^{2}&gt;0.\)</span> In other words, <span class="math display">\[\frac{1}{\sigma_{\mathrm{a}}^{2}}=\frac{1}{\sigma_{\mathrm{o}}^{2}}+\frac{1}{\sigma_{\mathrm{b}}^{2}}.\]</span> This is a very <span style="color: magenta">fundamental result</span>, implying that the overall <span style="color: magenta"><em>precision</em></span>, <span class="math inline">\(\tau=1/\sigma^{2},\)</span> (reciprocal of the variance) is the sum of the background and measurement precisions. Finally, the <span style="color: magenta">analysis equation</span> becomes</p>
<p><span class="math display">\[x^{\mathrm{a}}=x^{\mathrm{b}}+\frac{1}{1+\alpha}(x^{\mathrm{obs}}-x^{\mathrm{b}}),\]</span> where <span class="math inline">\(\alpha=\sigma_{\mathrm{obs}}^{2}/\sigma_{\mathrm{b}}^{2}.\)</span> This is called the <span style="color: blue">BLUE</span>- <span style="color: blue">B</span>est <span style="color: blue">L</span>inear <span style="color: blue">U</span>nbiased <span style="color: blue">E</span>stimator - because it gives an <span style="color: red">unbiased, optimal weighting for a linear combination</span> of two independent measurements.</p>
</section>
<section id="statistical-da-3-special-cases-and-conclusions" class="slide level2 center">
<h2>Statistical DA: 3 special cases and conclusions</h2>
<p>We can isolate three special cases:</p>
<ul>
<li><p>if the observation is very accurate, <span class="math inline">\(\sigma_{\mathrm{obs}}^{2}\ll\sigma_{\mathrm{b}}^{2},\)</span> <span class="math inline">\(\alpha\ll1\)</span> and thus <span style="color: magenta"><span class="math inline">\(x^{\mathrm{a}}\approx x^{\mathrm{obs}}\)</span></span></p></li>
<li><p>if the background is accurate, <span class="math inline">\(\alpha\gg1\)</span> and <span style="color: magenta"><span class="math inline">\(x^{\mathrm{a}}\approx x^{\mathrm{b}}\)</span></span></p></li>
<li><p>and finally, if observation and background varaiances are approximately equal, <span class="math inline">\(\alpha\approx1\)</span> and <span class="math inline">\(x^{\mathrm{a}}\)</span> is the <span style="color: magenta">arithmetic average</span> of <span class="math inline">\(x^{\mathrm{b}}\)</span> and <span class="math inline">\(x^{\mathrm{obs}}.\)</span></p></li>
</ul>
<p><span style="color: magenta"><strong>Conclusion</strong></span>: this simple, linear model does indeed capture the full range of possible solutions in a statistically rigorous manner, thus providing us with an “enriched” solution when compared with a non-probabilistic, scalar response such as the arithmetic average of observation and background, which would correspond to only the last of the above three special cases.</p>
</section>
<section id="section-5" class="slide level2 center">
<h2></h2>
<div class="center">
<p><span style="color: blue"><strong>KALMAN FILTERS</strong></span></p>
</div>
</section>
<section id="kalman-filters---background-and-history" class="slide level2 center">
<h2>Kalman Filters - background and history</h2>
<p>DA is concerned with dynamic systems, where (noisy) observations are acquired over time.</p>
<p><span style="color: magenta">Question</span>: Is there some statistically optimal way to combine the dynamic model and the observations?</p>
<ul>
<li><p>One <span style="color: magenta">answer</span> is provided by <span style="color: magenta">Kalman filters</span></p></li>
<li><p>They are linear models for state estimation of noisy dynamic systems.</p></li>
<li><p>They have been the <em>de facto</em> standard in many robotics and tracking/prediction applications because they are well-suited for systems where there is <span style="color: magenta">uncertainty about an observable dynamic process</span>.</p></li>
<li><p>They are also the basis of many data assimilation systems.</p></li>
<li><p>They use a paradigm of <span style="color: red">“observe, predict, correct”</span> to extract information from a noisy signal.</p></li>
</ul>
<p>The Kalman filter was invented<sup>1</sup> in 1960 by R. E. Kálmán to solve this sort of problem in a mathematically optimal way.</p>
<aside><ol class="aside-footnotes"><li id="fn1"><p>1 Apparently, following a prior invention by Stratonovich, one year earlier.</p></li></ol></aside></section>
<section id="section-6" class="slide level2 center">
<h2></h2>
<p>Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous <span style="color: magenta">variety of domains</span>.</p>
<ul>
<li><p>There are Kalman filters in aircraft and autonomous vehicles, on submarines, and, in cruise missiles.</p></li>
<li><p>Wall Street uses them to track the market.</p></li>
<li><p>They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments.</p></li>
<li><p>Chemical plants use them to control and monitor reactions.</p></li>
<li><p>They are used to perform medical imaging and to remove noise from cardiac signals.</p></li>
<li><p>Weather forecasting is based on Kalman filters.</p></li>
<li><p>They can effectively be used for modeling in <span style="color: magenta">epidemiology</span>.</p></li>
</ul>
<p>In <span style="color: magenta">summary</span>, if it involves a sensor and/or time-series data, a Kalman filter or a close relative of the Kalman filter is usually involved.</p>
</section>
<section id="kalman-filters-formulation" class="slide level2 center">
<h2>Kalman Filters — formulation</h2>
<p>Consider a dynamical system that evolves in time and we would like to<span style="color: magenta">estimate</span>a series of <em>true</em> states, <span class="math inline">\(\mathbf{x}^{\mathrm{t}}_{k}\)</span> (a sequence of random vectors) where discrete time is indexed by the letter <span class="math inline">\(k.\)</span></p>
<p>These times are those when the <span style="color: magenta">observations</span> or measurements are taken, as shown in the Figure.</p>

<img data-src="../../slides/images/mod_traject.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;1: Sequential assimilation: a computed model trajectory, observations, and their error bars.
</p></section>
<section id="section-7" class="slide level2 center">
<h2></h2>
<p>The assimilation starts with an <span style="color: magenta">unconstrained model trajectory</span> from <span class="math inline">\(t_{0},t_{1},\ldots,t_{k-1},t_{k},\ldots,t_{n}\)</span> and aims to provide an<span style="color: magenta">optimal fit</span>to the available <span style="color: magenta">observations/measurements</span> given their <span style="color: magenta">uncertainties</span> (error bars).</p>
<ul>
<li><p>For example, in current, synoptic scale weather forecasts, <span class="math inline">\(t_{k}-t_{k-1}=6\)</span> hours and is less for the convective scale.</p></li>
<li><p>In robotics, or autonomous vehicles, the time intervals are of the order of the instrumental frequency, which can be a few milliseconds.</p></li>
</ul>
</section>
<section id="kalman-filters---stochastic-model" class="slide level2 center">
<h2>Kalman Filters - stochastic model</h2>
<p>We seek to estimate the state <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> of a discrete-time dynamic process that is governed by the <span style="color: magenta">linear stochastic difference equation</span> <span id="eq-stateKF"><span class="math display">\[\mathbf{x}_{k+1}=\mathbf{M}_{k+1}\mathbf{x}_{k}+\mathbf{w}_{k} \tag{1}\]</span></span></p>
<p>with a <span style="color: magenta">measurement/observation</span> <span class="math inline">\(\mathbf{y}\in\mathbb{R}^{m},\)</span> <span id="eq-obsKF"><span class="math display">\[\mathbf{y}_{k}=\mathbf{H}_{k}\mathbf{x}_{k}+\mathbf{v}_{k}. \tag{2}\]</span></span></p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><span class="math inline">\(\mathbf{M}_{k+1}\)</span> and <span class="math inline">\(\mathbf{H}_{k}\)</span> are considered linear, here.</p></li>
<li><p>The random vectors, <span class="math inline">\(\mathbf{w}_{k}\)</span> and <span class="math inline">\(\mathbf{v}_{k},\)</span> represent the process/modeling and measurement/observation errors respectively.</p></li>
<li><p>They are assumed to be independent, white noise processes with <span style="color: magenta">Gaussian</span>/normal probability distributions, <span class="math display">\[\begin{aligned}
    \mathbf{w}_{k} &amp; \sim &amp; \mathcal{N}(0,\mathbf{Q}_{k}),\\
    \mathbf{v}_{k} &amp; \sim &amp; \mathcal{N}(0,\mathbf{R}_{k}),
    \end{aligned}\]</span> where <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(\mathbf{R}\)</span> are the <span style="color: magenta">covariance matrices</span> (supposed known) of the modeling and observation errors respectively.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="section-8" class="slide level2 center">
<h2></h2>
<p>All these assumptions about unbiased and uncorrelated errors (in time and between each other) are not limiting, since <span style="color: magenta">extensions</span> of the standard Kalman filter can be developed should any of these not be valid see next lecture.</p>
<p>We note that, for a broader mathematical view on the above system, we could formulate all of statistical DA in terms of stochastic differential equations (<span style="color: magenta">SDEs</span>).</p>
<ul>
<li>Then the theory of <span style="color: magenta">Itô</span> provides a detailed solution of the problem of optimal filtering as well as rigorous existence and uniqueness results… see <span class="citation" data-cites="law2015data sarkka2023bayesian">(<a href="#/references" role="doc-biblioref" onclick="">Law, Stuart, and Zygalakis 2015</a>; <a href="#/references" role="doc-biblioref" onclick="">Särkkä and Svensson 2023</a>)</span>.</li>
</ul>
</section>
<section id="kalman-filters-sequential-assimilation-scheme" class="slide level2 center">
<h2>Kalman Filters — sequential assimilation scheme</h2>
<p>The typical assimilation scheme is made up of two major steps:</p>
<ol type="1">
<li><p>a <span style="color: magenta">prediction/forecast</span> step, and</p></li>
<li><p>a <span style="color: magenta">correction/analysis</span> step.</p></li>
</ol>

<img data-src="../../slides/images/seq_assim.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;2: Sequential assimilation scheme for the Kalman filter. The <span class="math inline"><em>x</em></span>-axis denotes time, the <span class="math inline"><em>y</em></span>-axis denotes the values of the state and observations vectors.
</p></section>
<section id="section-9" class="slide level2 center">
<h2></h2>
<p>At time <span class="math inline">\(t_{k}\)</span> we have the result of a previous forecast, <span class="math inline">\(\mathbf{x}^{\mathrm{f}}_{k},\)</span> (the analogue of the background state <span class="math inline">\(\mathbf{x}^{\mathrm{b}}_{k}\)</span>) and the result of an ensemble of observations in <span class="math inline">\(\mathbf{y}_{k}.\)</span></p>
<p>Based on these two vectors, we perform an analysis that produces <span class="math inline">\(\mathbf{x}^{\mathrm{a}}_{k}.\)</span></p>
<p>We then use the evolution model to obtain a prediction of the state at time <span class="math inline">\(t_{k+1}.\)</span></p>
<p>The result of the forecast is denoted <span class="math inline">\(\mathbf{x}^{\mathrm{f}}_{k+1},\)</span> and becomes the background, or initial guess, for the next time-step see <a href="#/fig-KFscheme-1" class="quarto-xref">Figure&nbsp;2</a>).</p>
<p>The Kalman filter problem can be resumed as follows:</p>
<ul>
<li><p>given a prior/background estimate <span class="math inline">\(\mathbf{x}^{\mathrm{f}}\)</span> of the system state at time <span class="math inline">\(t_{k},\)</span></p></li>
<li><p>what is the best update/analysis <span class="math inline">\(\mathbf{x}^{\mathrm{a}}_{k}\)</span> based on the currently available measurements <span class="math inline">\(\mathbf{y}_{k}?\)</span></p></li>
</ul>
</section>
<section id="kalman-filters-the-filter" class="slide level2 center">
<h2>Kalman Filters — the filter</h2>
<p>The goal of the Kalman filter is:</p>
<ul>
<li><p>to compute <span style="color: magenta">an optimal <em>a posteriori</em> estimate</span> <span class="math inline">\(\mathbf{x}_{k}^{\mathrm{a}}\)</span></p></li>
<li><p>that is a <span style="color: magenta">linear combination</span> of an <em>a priori</em> estimate <span class="math inline">\(\mathbf{x}_{k}^{\mathrm{f}}\)</span> and a weighted difference between the actual measurement <span class="math inline">\(\mathbf{y}_{k}\)</span> and the measurement prediction <span class="math inline">\(\mathbf{H}_{k}\mathbf{x}^{\mathrm{f}}_{k}.\)</span></p></li>
</ul>
<p>This is none other than the <span style="color: blue">BLUE</span> that we have seen above.</p>
<p>The filter is thus of the linear, recursive form <span id="eq-kfBlue-1"><span class="math display">\[\mathbf{x}_{k}^{\mathrm{a}}=\mathbf{x}_{k}^{\mathrm{f}}+\mathbf{K}_{k}\left(\mathbf{y}_{k}-\mathbf{H}_{k}\mathbf{x}_{k}^{\mathrm{f}}\right). \tag{3}\]</span></span></p>
<p>The difference <span class="math inline">\(\mathbf{d}_{k}=\mathbf{y}_{k}-\mathbf{H}_{k}\mathbf{x}_{k}^{\mathrm{f}}\)</span> is called the <span style="color: magenta"><em>innovation</em></span> and reflects the discrepancy between the actual and the predicted measurements at time <span class="math inline">\(t_{k}.\)</span></p>
</section>
<section id="section-10" class="slide level2 center">
<h2></h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Note that, for generality, the matrices are shown with a time-dependence. When this is not the case, the subscripts <span class="math inline">\(k\)</span> can be dropped.</p>
</div>
</div>
</div>
<p>The <span style="color: magenta"><em>Kalman gain</em> matrix</span>, <span class="math inline">\(\mathbf{K},\)</span> minimizes the <em>a posteriori</em> error covariance <a href="#/section-10" class="quarto-xref">Equation&nbsp;4</a>.</p>
<ul>
<li>We define forecast (<em>a priori</em>) and analysis (<em>a posteriori</em>) estimate errors as <span class="math display">\[\begin{aligned}
    \mathbf{e}_{k}^{\mathrm{f}} &amp; = &amp; \mathbf{x}_{k}^{\mathrm{f}}-\mathbf{x}_{k}^{\mathrm{t}},\\
    \mathbf{e}_{k}^{\mathrm{a}} &amp; = &amp; \mathbf{x}_{k}^{\mathrm{a}}-\mathbf{x}_{k}^{\mathrm{t}},
    \end{aligned}\]</span> where <span class="math inline">\(\mathbf{x}_{k}^{\mathrm{t}}\)</span> is the (unknown) true state. Respective error covariance matrices are <span id="eq-apecov"><span class="math display">\[\begin{aligned}
    \mathbf{P}_{k}^{\mathrm{f}} &amp; = &amp; \mathop{\mathrm{Cov}}(\mathbf{e}_{k}^{\mathrm{f}})=\mathrm{E}\left[\mathbf{e}_{k}^{\mathrm{f}}(\mathbf{e}_{k}^{\mathrm{f}})^{\mathrm{T}}\right],\nonumber \\
    \mathbf{P}_{k}^{\mathrm{a}} &amp; = &amp; \mathop{\mathrm{Cov}}(\mathbf{e}_{k}^{\mathrm{a}})=\mathrm{E}\left[\mathbf{e}_{k}^{\mathrm{a}}(\mathbf{e}_{k}^{\mathrm{a}})^{\mathrm{T}}\right].
    \end{aligned} \tag{4}\]</span></span></li>
</ul>
<p><span style="color: magenta"><em>Optimal gain</em></span> requires a careful derivation, that is beyond our scope here (see <span class="citation" data-cites="asch2022toolbox asch2016data">(<a href="#/references" role="doc-biblioref" onclick="">Asch 2022</a>; <a href="#/references" role="doc-biblioref" onclick="">Asch, Bocquet, and Nodet 2016</a>)</span>).</p>
</section>
<section id="kalman-filters-optimal-gain" class="slide level2 center">
<h2>Kalman Filters — optimal gain</h2>
<p>The <span style="color: magenta"><em>Kalman gain</em>matrix</span>, <span class="math inline">\(\mathbf{K},\)</span> is chosen to minimize the <em>a posteriori</em> error covariance <a href="#/section-10" class="quarto-xref">Equation&nbsp;4</a>.</p>
<p>The resulting <span class="math inline">\(\mathbf{K}\)</span> that minimizes <a href="#/section-10" class="quarto-xref">Equation&nbsp;4</a> is given by <span id="eq-KFoptgain"><span class="math display">\[\mathbf{K}_{k}=\mathbf{P}_{k}^{\mathrm{f}}\mathbf{H}_{k}^{\mathrm{T}}\left(\mathbf{H}_{k}\mathbf{P}_{k}^{\mathrm{f}}\mathbf{H}_{k}^{\mathrm{T}}+\mathbf{R}_{k}\right)^{-1} \tag{5}\]</span></span> where we remark that <span class="math inline">\(\mathbf{H}\mathbf{P}_{k}^{\mathrm{f}}\mathbf{H}_{k}^{\mathrm{T}}+\mathbf{R}_{k}=\mathrm{E}\left[\mathbf{d}_{k}\mathbf{d}_{k}^{\mathrm{T}}\right]\)</span> is the covariance of the innovation.</p>
<p>Looking at this expression for <span class="math inline">\(\mathbf{K}_{k},\)</span> we see:</p>
<ul>
<li><p>when the measurement error covariance<span style="color: magenta"><span class="math inline">\(\mathbf{R}_{k}\)</span> approaches zero</span>, the gain <span class="math inline">\(\mathbf{K}_{k}\)</span> weights the innovation more heavily, since <span class="math display">\[\lim_{\mathbf{R}\rightarrow0}\mathbf{K}_{k}=\mathbf{H}_{k}^{-1}.\]</span></p></li>
<li><p>On the other hand, as the <em>a priori</em> error estimate covariance <span style="color: magenta"><span class="math inline">\(\mathbf{P}_{k}^{\mathrm{f}}\)</span> approaches zero</span>,</p></li>
</ul>
</section>
<section id="section-11" class="slide level2 center">
<h2></h2>
<ul>
<li><p>the gain <span class="math inline">\(\mathbf{K}_{k}\)</span> weights the innovation less heavily, and <span class="math display">\[\lim_{\mathbf{P}_{k}^{\mathrm{f}}\rightarrow0}\mathbf{K}_{k}=0.\]</span></p></li>
<li><p>Another way of thinking about the weighting of <span class="math inline">\(\mathbf{K}\)</span> is that as the measurement error covariance <span class="math inline">\(\mathbf{R}\)</span> approaches zero, the actual measurement <span class="math inline">\(\mathbf{y}_{k}\)</span> is “trusted” more and more, while the predicted measurement <span class="math inline">\(\mathbf{H}_{k}\mathbf{x}_{k}^{\mathrm{f}}\)</span> is trusted less and less.</p></li>
<li><p>On the other hand, as the <em>a priori</em> error estimate covariance <span class="math inline">\(\mathbf{P}_{k}^{\mathrm{f}}\)</span> approaches zero, the actual measurement <span class="math inline">\(\mathbf{y}_{k}\)</span> is trusted less and less, while the predicted measurement <span class="math inline">\(\mathbf{H}_{k}\mathbf{x}_{k}^{\mathrm{f}}\)</span> is “trusted” more and more this will be illustrated in the computational example below.</p></li>
</ul>
</section>
<section id="kalman-filters-2-step-procedure" class="slide level2 center">
<h2>Kalman Filters — 2-step procedure</h2>

<img data-src="../images/KF-flow.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;3: Kalman filter loop, showing the two phases, predict and correct, preceded by an initialization step.
</p><p>The predictor-corrector loop is illustrated in <a href="#/fig-kf_loop" class="quarto-xref">Figure&nbsp;3</a> and can be transposed, as is, into an <span style="color: magenta">operational algorithm</span>.</p>
</section>
<section id="kf-predictorforecast-step" class="slide level2 center">
<h2>KF — predictor/forecast step</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>Start from a previous analyzed state<sup>1</sup>, <span class="math inline">\(\mathbf{x}_{k}^{\mathrm{a}},\)</span> or from the initial state if <span class="math inline">\(k=0,\)</span> characterized by the Gaussian pdf <span class="math inline">\(p(\mathbf{x}_{k}^{\mathrm{a}}\mid\mathbf{y}_{1:k}^{\mathrm{o}})\)</span> of mean <span class="math inline">\(\mathbf{x}_{k}^{\mathrm{a}}\)</span> and covariance matrix <span class="math inline">\(\mathbf{P}_{k}^{a}.\)</span></p>
<p>An estimate of <span class="math inline">\(\mathbf{x}_{k+1}^{\mathrm{t}}\)</span> is given by the dynamical model which defines the forecast as <span id="eq-fcov1"><span class="math display">\[\begin{gather}
    \mathbf{x}_{k+1}^{\mathrm{f}} &amp; = &amp; \mathbf{M}_{k+1}\mathbf{x}_{k}^{\mathrm{a}},\label{eq:fstate}\\
    \mathbf{P}_{k+1}^{\mathrm{f}} &amp; = &amp; \mathbf{M}_{k+1}\mathbf{P}_{k}^{\mathrm{a}}\mathbf{M}_{k+1}^{\mathrm{T}}+\mathbf{Q}_{k+1},
    \end{gather} \tag{6}\]</span></span> where the expression for <span class="math inline">\(\mathbf{P}^{\mathrm{f}}_{k+1}\)</span> is obtained from the dynamics equation and the definition of the model noise covariance, <span class="math inline">\(\mathbf{Q}.\)</span></p>
</div><div class="column" style="width:40%;">
<p><img data-src="../images/KF-flow.png"></p>
</div>
</div>
<aside><ol class="aside-footnotes"><li id="fn2"><p>We use here the classical notation <span class="math inline">\(\mathbf{y}_{i:j}=(\mathbf{y}_{i},\mathbf{y}_{i+1},\ldots,\mathbf{y}_{j})\)</span> for <span class="math inline">\(i\le j\)</span> that denotes conditioning on all the observations in the interval.</p></li></ol></aside></section>
<section id="kf---correctoranalysis-step" class="slide level2 center">
<h2>KF - corrector/analysis step</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>At time <span class="math inline">\(t_{k+1},\)</span> the pdf <span class="math inline">\(p(\mathbf{x}_{k+1}^{\mathrm{f}}\mid\mathbf{y}_{1:k}^{\mathrm{o}})\)</span> is known, thanks to the mean <span class="math inline">\(\mathbf{x}_{k+1}^{\mathrm{f}}\)</span> and covariance matrix <span class="math inline">\(\mathbf{P}_{k+1}^{\mathrm{f}}\)</span> just calculated, as well as the assumption of a Gaussian distribution.</p>
<p>The analysis step then consists of correcting this pdf using the observation available at time <span class="math inline">\(t_{k+1}\)</span> in order to compute <span class="math inline">\(p(\mathbf{x}_{k+1}^{\mathrm{a}}\mid\mathbf{y}_{1:k+1}^{\mathrm{o}}).\)</span> This comes from the BLUE in the dynamical context and gives</p>
</div><div class="column" style="width:40%;">
<p><img data-src="../images/KF-flow.png"></p>
</div>
</div>
<p><span id="eq-acov"><span class="math display">\[\begin{gather}
    \mathbf{K}_{k+1} &amp; = &amp; \mathbf{P}_{k+1}^{\mathrm{f}}\mathbf{H}^{\mathrm{T}}\left(\mathbf{H}\mathbf{P}_{k+1}^{\mathrm{f}}\mathbf{H}^{\mathrm{T}}+\mathbf{R}_{k+1}\right)^{-1},\label{eq:aK}\\
    \mathbf{x}_{k+1}^{\mathrm{a}} &amp; = &amp; \mathbf{x}_{k+1}^{\mathrm{f}}+\mathbf{K}_{k+1}\left(\mathbf{y}_{k+1}-\mathbf{H}\mathbf{x}_{k+1}^{\mathrm{f}}\right),\label{eq:astate}\\
    \mathbf{P}_{k+1}^{\mathrm{a}} &amp; = &amp; \left(\mathbf{I}-\mathbf{K}_{k+1}\mathbf{H}\right)\mathbf{P}_{k+1}^{\mathrm{f}}.\label{eq:acov}
    \end{gather} \tag{7}\]</span></span></p>
</section>
<section id="overall-picture" class="slide level2 center">
<h2>Overall Picture</h2>
<div class="center">
<p><img data-src="../images/Sketch-of-sequential-data-assimilation-algorithms-in-the-observation-space-where-H-is.png" style="width:50.0%"></p>
</div>
<p><strong>Principle</strong>: as we move forward in time, the uncertainty of the analysis is reduced, and the forecast is improved.</p>
</section>
<section id="kf-relation-between-bayes-and-blue" class="slide level2 center">
<h2>KF — Relation Between Bayes and BLUE</h2>
<p>If we know that the <em>a priori</em> and the observation data are both Gaussian, Bayes’ rule can be readily applied to compute the <em>a posteriori</em> pdf.</p>
<ul>
<li>The <em>a posteriori</em> pdf is then Gaussian, and its parameters are given by the BLUE equations.</li>
</ul>
<p>Hence with Gaussian pdfs and a linear observation operator, there is no need to use Bayes’ rule.</p>
<ul>
<li><p>The BLUE equations can be used instead to compute the parameters of the resulting pdf.</p></li>
<li><p>Since the BLUE provides the same result as Bayes’ rule, it is the best estimator of all.</p></li>
</ul>
<p>In addition one can recognize the 3D-Var cost function.</p>
<ul>
<li>By optimizing this cost function, 3D-Var finds the MAP (maximum a posteriori) estimate of the Gaussian pdf, which is equivalent to the MV (minimum variance) estimate found by the BLUE.</li>
</ul>
</section>
<section id="section-12" class="slide level2 center">
<h2></h2>
<div class="center">
<p><strong><span style="color: blue">ENSEMBLE KALMAN FILTERS</span></strong></p>
</div>
</section>
<section id="ensemble-kalman-filter-enkf" class="slide level2 center">
<h2>Ensemble Kalman Filter — EnKF</h2>
<p>The ensemble Kalman filter (EnKF) is an elegant approach that avoids</p>
<ul>
<li><p>the steps of <span style="color: magenta">linearization</span> in the classical Kalman Filter,</p></li>
<li><p>and the need for <span style="color: magenta">adjoints</span> in the variational approach.</p></li>
</ul>
<p>It is still based on a Kalman filter, but an <span style="color: magenta">ensemble of realizations</span> is used to compute an estimate of the population mean and variance, thus avoiding the need to compute inverses of potentially large matrices to obtain the posterior covariance, as was the case above in equations for <span class="math inline">\(\mathbf{K}_{k+1}\)</span> and <span class="math inline">\(\mathbf{P}_{k+1}^a\)</span> (<a href="#/kf---correctoranalysis-step" class="quarto-xref">Equation&nbsp;7</a>).</p>
<p>The EnKF and its variants have been successfully developed and implemented in <span style="color: magenta">meteorology and oceanography</span>, including in operational weather forecasting systems. Because the method is simple to implement, it has been widely used in these fields.</p>
</section>
<section id="section-13" class="slide level2 center">
<h2></h2>
<p>But it has spread out to other geoscience disciplines and beyond. For instance, to name a few domains, it has been applied in greenhouse gas inverse modeling, air quality forecasting, extra-terrestrial atmosphere forecasting , detection and attribution in climate sciences, geomagnetism re-analysis , and ice-sheet parameter estimation and forecasting. It has also been used in petroleum reservoir estimation, in adaptive optics for extra large telescopes, and highway traffic estimation.</p>
<p>More recently, the idea was proposed to exploit the EnKF as a universal approach for all inverse problems. The term EKI, Ensemble Kalman Inversion, is used to describe this approach.</p>
</section>
<section id="principle-of-the-enkf" class="slide level2 center">
<h2>Principle of the EnKF</h2>
<p>The EnKF was originally proposed by G. Evensen in 1994 and amended in <span class="citation" data-cites="evensen2009data">Evensen et al. (<a href="#/references" role="doc-biblioref" onclick="">2009</a>)</span>.</p>
<div id="def-EnKF" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>The ensemble Kalman filter (EnKF) is a Kalman filter that uses an ensemble of realizations to compute estimates of the population mean and covariance.</p>
</div>
<p>Since it is based on <span style="color: magenta">Gaussian</span> statistics (mean and covariance) it does not solve the Bayesian filtering problem in the limit of a large number of particles, as opposed to the more general <em>particle filter</em>seeAdvanced Course. Nonetheless, it turns out to be an excellent <span style="color: magenta">approximate</span> algorithm for the filtering problem.</p>
<p>As in the particle filter, the EnKF is based on the concept of particles, a collection of state vectors, which are called the members of the <span style="color: magenta">ensemble</span>.</p>
<ul>
<li>Rather than propagating huge covariance matrices, the errors are emulated by scattered particles, a collection of state vectors whose variability is meant to be representative of the uncertainty of the system’s state resulting from the forecaster’s ignorance.</li>
</ul>
</section>
<section id="section-14" class="slide level2 center">
<h2></h2>
<ul>
<li><p>Just like the particle filter, the members are propagated by the <span style="color: magenta">nonlinear</span> model, without any linearization. Not only does this avoid the derivation of the tangent linear model, but it also circumvents the approximate linearization.</p></li>
<li><p>Finally, as opposed to the particle filter, the EnKF does not irremediably suffer from the curse of dimensionality.</p></li>
</ul>
<p>To sum up, here are the important remarks:</p>
<ul>
<li><p>the EnKF avoids the <span style="color: magenta">linearization</span> step of the KF;</p></li>
<li><p>the EnKF avoids the <span style="color: magenta">inversion</span> of potentially large matrices;</p></li>
<li><p>the EnKF does not require any <span style="color: magenta">adjoint</span>, as in variational assimilation;</p></li>
<li><p>the EnKF has been applied to a vast number of <span style="color: magenta">real-world</span> problems.</p></li>
</ul>
</section>
<section id="enkf-the-three-steps" class="slide level2 center">
<h2>EnKF — the Three Steps</h2>
<ol type="1">
<li><p><strong>Initialization:</strong> generate an ensemble of <span class="math inline">\(m\)</span> random states <span class="math inline">\(\left\{ \mathbf{x}_{i,0}^{\mathrm{f}}\right\} _{i=1,\ldots,m}\)</span> at time <span class="math inline">\(t=0.\)</span></p></li>
<li><p><strong>Forecast:</strong> compute the prediction for each member of the ensemble.</p></li>
<li><p><strong>Analysis:</strong> correct the prediction in light of the observations.</p></li>
</ol>
<p>Please see the Algorithm <span class="citation" data-cites="alg-EnKF">(<a href="#/references" role="doc-biblioref" onclick=""><strong>alg-EnKF?</strong></a>)</span> below for details of each step.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p>Propagation can equivalently be performed either at the end of the analysis step or at the beginning of the forecast step.</p></li>
<li><p>The Kalman gain is not computed directly, but <span style="color: magenta">estimated</span> from the ensemble statistics.</p></li>
<li><p>With the important exception of the Kalman gain computation, all operations on the ensemble members are independent. As a result, <span style="color: magenta">parallelization</span> is straightforward.</p></li>
<li><p>This is one of the main reasons for the <span style="color: magenta">success/popularity</span> of the EnKF.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="enkf-analysis-step" class="slide level2 center">
<h2>EnKF — Analysis Step</h2>
<p>The EnKF seeks to mimic the analysis step of the Kalman filter but with an <span style="color: magenta">ensemble of limited size</span> in place of the <span style="color: magenta">unwieldy covariance matrices</span>.</p>
<p>The goal is to perform for <span style="color: magenta">each member</span> of the ensemble an analysis of the form, <span id="eq-member-update"><span class="math display">\[\mathbf{x}_{i}^{{\rm a}}=\mathbf{x}_{i}^{{\rm f}}+\mathbf{K}\left[\mathbf{y}_{i}-\mathcal{H}(\mathbf{x}^{\mathrm{f}}_{i})\right], \tag{8}\]</span></span> where</p>
<ul>
<li><p><span class="math inline">\(i=1,\ldots,m\)</span> is the member index in the ensemble,</p></li>
<li><p><span class="math inline">\(\mathbf{x}^{\mathrm{f}}_{i}\)</span> is the forecast state vector <span class="math inline">\(i\)</span>, which represents a background state or prior at the analysis time.</p></li>
</ul>
<p>To mimic the Kalman filter, <span class="math inline">\(\mathbf{K}\)</span> must be identified with the <span style="color: magenta">Kalman gain</span> <span class="math display">\[\mathbf{K}=\mathbf{P}^{\mathrm{f}}\mathbf{H}^{\mathrm{T}}{\mathbf{H}\mathbf{P}^{\mathrm{f}}\mathbf{H}^{\mathrm{T}}+\mathbf{R}}^{-1},\label{eq:kalman-gain}\]</span> that we wish to <span style="color: magenta">estimate from the ensemble statistics</span>.</p>
</section>
<section id="section-15" class="slide level2 center">
<h2></h2>
<ul>
<li><p>First, we compute the <span style="color: magenta">forecast error covariance matrix</span> as a sum over the ensemble, <span class="math display">\[\mathbf{P}^{\mathrm{f}}=\frac{1}{m-1}\sum_{i=1}^{m}\left({\mathbf{x}_{i}^{{\rm f}}-\overline{\mathbf{x}}^{{\rm f}}}\right)\left({\mathbf{x}_{i}^{{\rm f}}-\overline{\mathbf{x}}^{{\rm f}}}\right)^{\mathrm{T}},\]</span> with <span class="math inline">\(\overline{\mathbf{x}}=\frac{1}{m}\sum_{i=1}^{m}\mathbf{x}_{i}^{{\rm f}}.\)</span></p></li>
<li><p>The forecast error covariance matrix can be <span style="color: magenta">factorized</span> into <span class="math display">\[\mathbf{P}^{\mathrm{f}}=\mathbf{X}_{\mathrm{f}}\mathbf{X}_{\mathrm{f}}^{\mathrm{T}},\]</span> where <span class="math inline">\(\mathbf{X}_{\mathrm{f}}\)</span> is a <span class="math inline">\(n\times m\)</span> matrix whose columns are the <span style="color: magenta"><em>normalized anomalies</em></span> or <em>normalized perturbations</em> , i.e.&nbsp;for <span class="math inline">\(i=1,\ldots,m\)</span> <span class="math display">\[\left[\mathbf{X}_{\mathrm{f}}\right]_{i}=\frac{\mathbf{x}_{i}^{{\rm f}}-\overline{\mathbf{x}}^{{\rm f}}}{\sqrt{m-1}}.\]</span></p></li>
</ul>
</section>
<section id="section-16" class="slide level2 center">
<h2></h2>
<p>We can now obtain from <a href="#/enkf-analysis-step" class="quarto-xref">Equation&nbsp;8</a> a <span style="color: magenta">posterior ensemble</span> <span class="math inline">\(\left\{ \mathbf{x}_{i}^{{\rm a}}\right\} _{i=1,\ldots,m}\)</span> from which we can compute the posterior statistics.</p>
<p>Hence, the <span style="color: magenta">posterior state</span> and an ensemble of <span style="color: magenta">posterior perturbations</span> can be estimated from <span class="math display">\[\overline{\mathbf{x}}^{{\rm a}}=\frac{1}{m}\sum_{i=1}^{m}\mathbf{x}^{\mathrm{a}}_{i}\,,\quad\left[\mathbf{X}_{\mathrm{a}}\right]_{i}=\frac{\mathbf{x}_{i}^{{\rm a}}-\overline{\mathbf{x}}^{{\rm a}}}{\sqrt{m-1}}.\]</span></p>
<p>Since <span class="math inline">\(\mathbf{y}_{i}\equiv\mathbf{y}\)</span> was assumed, the normalized anomalies, <span class="math inline">\(\mathbf{X}_{i}^{{\rm a}}\equiv\left[\mathbf{X}_{\mathrm{a}}\right]_{i}\)</span>, i.e.&nbsp;the normalized deviations of the ensemble members from the mean are obtained from <a href="#/enkf-analysis-step" class="quarto-xref">Equation&nbsp;8</a> minus the mean update, <span class="math display">\[\mathbf{X}_{i}^{{\rm a}}=\mathbf{X}_{i}^{{\rm f}}+\mathbf{K}\left(\mathbf{0}-\mathbf{H}\mathbf{X}_{i}^{{\rm f}}\right)=\left(\mathbf{I}_{n}-\mathbf{K}\mathbf{H}\right)\mathbf{X}_{i}^{{\rm f}},\label{eq:anomaly-update}\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_{i}^{{\rm f}}\equiv\left[\mathbf{X}_{\mathrm{f}}\right]_{i}\)</span>, which yields the <span style="color: magenta">analysis error covariance matrix</span>,</p>
</section>
<section id="section-17" class="slide level2 center">
<h2></h2>
<p><span class="math display">\[\begin{aligned}
        \mathbf{P}^{{\rm a}} &amp; =\mathbf{X}_{\mathrm{a}}\mathbf{X}_{\mathrm{a}}^{\mathrm{T}}\\
         &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{X}_{\mathrm{f}}\mathbf{X}_{\mathrm{f}}^{\mathrm{T}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}}\\
         &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}}.
        \end{aligned}\]</span></p>
<p>Note that such a computation is never carried out in practice. However, theoretically, in order to mimic the <span style="color: magenta">best linear unbiased estimator</span> (BLUE) analysis of the Kalman filter, we should have obtained <span class="math display">\[\begin{aligned}
    \mathbf{P}^{{\rm a}} &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}}+\mathbf{K}\mathbf{R}\mathbf{K}^{\mathrm{T}}\\
     &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}.
    \end{aligned}\]</span></p>
<ul>
<li>Therefore, the error covariances are <span style="color: magenta">underestimated</span> since the second positive term, related to the observation errors, is ignored, which is likely to lead to the <span style="color: magenta">divergence</span> of the EnKF when the scheme is cycled.</li>
</ul>
<p>An elegant solution around this problem is to <span style="color: magenta">perturb</span> the observation vector for each member: <span class="math inline">\(\mathbf{y}_{i}=\mathbf{y}+\mathbf{u}_{i}\)</span>, where <span class="math inline">\(\mathbf{u}_{i}\)</span> is drawn from the Gaussian distribution <span class="math inline">\(\mathbf{u}_{i}\sim N(\mathbf{0},\mathbf{R}).\)</span></p>
</section>
<section id="section-18" class="slide level2 center">
<h2></h2>
<ul>
<li><p>Let us define <span class="math inline">\(\overline{\mathbf{u}}\)</span> the mean of the sampled <span class="math inline">\(\mathbf{u}_{i}\)</span>, and the innovation perturbations <span class="math display">\[\left[\mathbf{Y}_{\mathrm{f}}\right]_{i}=\frac{\mathbf{H}\mathbf{x}_{i}^{{\rm f}}-\mathbf{u}_{i}-\mathbf{H}\overline{\mathbf{x}}^{{\rm f}}+\overline{\mathbf{u}}}{\sqrt{m-1}}.\label{eq:innovation-pert}\]</span></p></li>
<li><p>The <span style="color: magenta">posterior anomalies</span> are modified accordingly, <span class="math display">\[\mathbf{X}_{i}^{{\rm a}}=\mathbf{X}_{i}^{{\rm f}}-\mathbf{K}\mathbf{Y}_{i}^{{\rm f}}=(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{X}_{i}^{{\rm f}}+\frac{\mathbf{K}(\mathbf{u}_{i}-\overline{\mathbf{u}})}{\sqrt{m-1}}.\label{eq:anomaly-update-correction}\]</span></p></li>
</ul>
<p>These anomalies yield the <span style="color: magenta">analysis error covariance matrix</span>,</p>
</section>
<section id="section-19" class="slide level2 center">
<h2></h2>
<p><span class="math display">\[\begin{aligned}
    \mathbf{P}^{{\rm a}}= &amp; (\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}}
      +\mathbf{K}\left[\frac{1}{m-1}\sum_{i=1}^{m}(\mathbf{u}_{i}-\overline{\mathbf{u}})(\mathbf{u}_{i}-\overline{\mathbf{u}})^{\mathrm{T}}\right]\mathbf{K}^{\mathrm{T}}\\
     &amp; +\frac{1}{\sqrt{m-1}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}(\mathbf{u}_{i}-\overline{\mathbf{u}})^{\mathrm{T}}\mathbf{K}^{\mathrm{T}}\\
     &amp; +\frac{1}{\sqrt{m-1}}\mathbf{K}(\mathbf{u}_{i}-\overline{\mathbf{u}})\mathbf{P}^{\mathrm{f}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}},
    \end{aligned}\]</span> whose expectation over the random noise gives the <span style="color: magenta">proper</span> expected <span style="color: magenta">posterior covariances</span>, <span class="math display">\[\begin{aligned}
    \mathrm{E}\left[\mathbf{P}^{{\rm a}}\right] &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}}
      +\mathbf{K}\mathrm{E}\left[\frac{1}{m-1}\sum_{i=1}^{m}(\mathbf{u}_{i}-\overline{\mathbf{u}})(\mathbf{u}_{i}-\overline{\mathbf{u}})^{\mathrm{T}}\right]\mathbf{K}^{\mathrm{T}}\\
     &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})^{\mathrm{T}}+\mathbf{K}\mathbf{R}\mathbf{K}\\
     &amp; =(\mathbf{I}_{n}-\mathbf{K}\mathbf{H})\mathbf{P}^{\mathrm{f}}.
    \end{aligned}\]</span></p>
</section>
<section id="section-20" class="slide level2 center">
<h2></h2>
<p>Note that the <span style="color: magenta">gain</span> can be formulated in terms of the anomaly matrices only, <span class="math display">\[\mathbf{K}=\mathbf{X}_{\mathrm{f}}\mathbf{Y}_{\mathrm{f}}^{\mathrm{T}}\left(\mathbf{Y}_{\mathrm{f}}\mathbf{Y}_{\mathrm{f}}^{\mathrm{T}}\right)^{-1},\label{eq:kalman-gain-pert}\]</span> since</p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}_{\mathrm{f}}\mathbf{Y}_{\mathrm{f}}^{\mathrm{T}}\)</span> is a sample estimate for <span class="math inline">\(\mathbf{P}^{\mathrm{f}}\mathbf{H}^{\mathrm{T}}\)</span> and</p></li>
<li><p><span class="math inline">\(\mathbf{Y}_{\mathrm{f}}\mathbf{Y}_{{\rm f}}^{\mathrm{T}}\)</span> is a sample estimate for <span class="math inline">\(\mathbf{H}\mathbf{P}^{\mathrm{f}}\mathbf{H}^{\mathrm{T}}+\mathbf{R}.\)</span></p></li>
</ul>
<p>In this form, it is striking that the updated perturbations are linear combinations of the forecast perturbations. The new perturbations are sought within the ensemble subspace of the initial perturbations.</p>
<p>Similarly, the state analysis is sought within the affine space <span class="math inline">\(\overline{\mathbf{x}}^{{\rm f}}+\mathrm{vec}\left(\mathbf{X}_{1}^{{\rm f}},\mathbf{X}_{2}^{{\rm f}},\ldots,\mathbf{X}_{m}^{{\rm f}}\right).\)</span></p>
</section>
<section id="enkf-forecast-step" class="slide level2 center">
<h2>EnKF — Forecast Step</h2>
<p>In the forecast step, the updated ensemble obtained at the analysis step is <span style="color: magenta">propagated</span> by the model over a time step, <span class="math display">\[\mbox{for}\quad i=1,\ldots,m\quad\mathbf{x}_{i,k+1}^{{\rm f}}=\mathcal{M}_{k+1}(\mathbf{x}^{\mathrm{a}}_{i,k}).\]</span></p>
<p>A forecast can be computed from the mean of the<span style="color: magenta">forecast ensemble</span>, while the forecast error covariances can be estimated from the forecast perturbations.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>These are only optional diagnostics in the scheme and they are not required in the cycling of the EnKF.</p></li>
<li><p>It is important to observe that using the <span style="color: magenta">tangent linear model</span>(TLM) operator, or any linearization thereof, was <span style="color: red">avoided</span>.</p></li>
<li><p>This difference should particularly matter in a significantly <span style="color: magenta">nonlinear</span> regime.</p></li>
<li><p>However, as we shall see in the Advanced Course Lectures, in strongly nonlinear regimes, the EnKF is largely dominated by schemes known as the iterative EnKF and the iterative ensemble Kalman smoother</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="comparison-enkf-and-4d-var" class="slide level2 center">
<h2>Comparison: EnKf and 4D-Var</h2>
<div class="center">
<p><img data-src="../images/enKF_4DVar.png" style="width:50.0%"></p>
</div>
<p><span style="color: magenta">Principle</span> of data assimilation: Having a physical model able to forecast the evolution of a system from time <span class="math inline">\(t=t_{0}\)</span> to time<span class="math inline">\(t=T_{f}\)</span> (cyan curve), the aim of DA is to use available observations (blue triangles) to correct the model projections and get closer to the (unknown) truth (dotted line).</p>
<p>In <span style="color: magenta">EnKF</span>s, the initial system state and its uncertainty (green square and ellipsoid) are represented by <span class="math inline">\(m\)</span> members.</p>
</section>
<section id="section-21" class="slide level2 center">
<h2></h2>
<ul>
<li><p>The <span style="color: magenta">members are propagated</span> forward in time during <span class="math inline">\(n_{1}\)</span> model time steps <span class="math inline">\(dt\)</span> to <span class="math inline">\(t=T_{1}\)</span> where observations are available (forecast phase, orange dashed lines).</p></li>
<li><p>At <span class="math inline">\(t=T_{1}\)</span> the analysis uses the observations and their uncertainty (blue triangle and ellipsoid) to produce a new system state that is <span style="color: magenta">closer to the observations</span> and with a <span style="color: magenta">lower uncertainty</span> (red square and ellipsoid).</p></li>
<li><p>A <span style="color: magenta">new forecast</span>is issued from the analyszd state and this procedure is repeated until the end of the assimilation window at <span class="math inline">\(t=T_{f}.\)</span></p></li>
<li><p>The model state should get closer to the truth and with lower uncertainty as more observations are assimilated.</p></li>
</ul>
<p>Time-dependent variational methods (<span style="color: magenta">4D-Var</span>) iterate over the assimilation window to find the trajectory that minimises the misfit (<span class="math inline">\(J_{0}\)</span>) between the model and all observations available from <span class="math inline">\(t_{0}\)</span> to <span class="math inline">\(T_{f}\)</span> (violet curve).</p>
<p>For <span style="color: magenta">linear dynamics, Gaussian errors and infinite ensemble sizes</span>, the states produced at the end of the assimilation window by the two methods should be equivalent <span class="citation" data-cites="li2001optimality">(<a href="#/references" role="doc-biblioref" onclick="">Li and Navon 2001</a>)</span>.</p>
</section>
<section id="enkf-the-algorithm" class="slide level2 center">
<h2>EnKF — the Algorithm</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="list" style="font-size: 75%;">
<p><strong>Given</strong>:&nbsp;For&nbsp;<span class="math inline">\(k=0,\ldots,K,\)</span>&nbsp;observation&nbsp;error&nbsp;cov.&nbsp;matrices</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\mathbf{R}_{k}\)</span>,&nbsp;observation&nbsp;models&nbsp;<span class="math inline">\(\mathcal{H}_{k}\)</span>,&nbsp;forward&nbsp;models&nbsp;<span class="math inline">\(\mathcal{M}_{k}.\)</span>&nbsp;&nbsp;<br>
<strong>Compute</strong>:&nbsp;the&nbsp;ensemble&nbsp;forecast&nbsp;<span class="math inline">\(\left\{ \mathbf{x}_{i,k}^{\mathrm{f}}\right\} _{i=1,\ldots,m,\,k=1,\ldots,K}\)</span>&nbsp;</p>
<p><span class="math inline">\(\left(\mathbf{x}_{i,0}^{\mathrm{f}}\right)_{i=1,\ldots,m}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Initialize&nbsp;the&nbsp;ensemble</p>
<p>for&nbsp;<span class="math inline">\(k=0\)</span>&nbsp;to&nbsp;<span class="math inline">\(K\)</span>&nbsp;do&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Loop&nbsp;over&nbsp;time</p>
<p>&nbsp;&nbsp;for&nbsp;&nbsp;<span class="math inline">\(i=1\)</span>&nbsp;to&nbsp;<span class="math inline">\(m\)</span>&nbsp;do&nbsp;#Draw&nbsp;a&nbsp;stat.&nbsp;consistent&nbsp;obs.&nbsp;set</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\mathbf{u}_{i}\sim{\cal N}(0,\mathbf{R}_{k})\)</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\mathbf{y}_{i,k}=\mathbf{y}_{k}+\mathbf{u}_{i}\)</span></p>
<p>&nbsp;&nbsp;end&nbsp;for</p>
</div>
</div><div class="column" style="width:50%;">
<div class="list" style="font-size: 75%;">
<p>#Compute&nbsp;the&nbsp;ensemble&nbsp;means</p>
<p>&nbsp;&nbsp;<span class="math inline">\(\overline{\mathbf{x}}_{k}^{\mathrm{f}}=\frac{1}{m}\sum_{i=1}^{m}\mathbf{x}^{\mathrm{f}}_{i,k}\,,\overline{\mathbf{u}}=\frac{1}{m}\sum_{i=1}^{m}\mathbf{u}_{i}\)</span>&nbsp;</p>
<p>&nbsp;&nbsp;<span class="math inline">\(\left[\mathbf{X}_{\mathrm{f}}\right]_{i,k}=\frac{\mathbf{x}_{i,k}^{\mathrm{f}}-\overline{\mathbf{x}}_{k}^{\mathrm{f}}}{\sqrt{m-1}},\)</span>&nbsp;&nbsp;&nbsp;#Compute&nbsp;the&nbsp;normalized&nbsp;anomalies</p>
<p>&nbsp;&nbsp;<span class="math inline">\(\left[\mathbf{Y}_{\mathrm{f}}\right]_{i,k}=\frac{\mathbf{H}_{k}\mathbf{x}_{i,k}^{\mathrm{f}}-\mathbf{u}_{i}-\mathbf{H}_{k}\overline{\mathbf{x}}_{k}^{\mathrm{f}}+\overline{\mathbf{u}}}{\sqrt{m-1}}\)</span></p>
<p>&nbsp;&nbsp;&nbsp;<span class="math inline">\(K_{k}=\mathbf{X}_{k}^{\mathrm{f}}\left({\mathbf{Y}_{k}^{\mathrm{f}}}\right)^{\mathrm{T}}\left(\mathbf{Y}_{k}^{\mathrm{f}}\left({\mathbf{Y}_{k}^{\mathrm{f}}}\right)^{\mathrm{T}}\right)^{-1}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;#Compute&nbsp;the&nbsp;gain</p>
<p>&nbsp;&nbsp;for&nbsp;<span class="math inline">\(i=1\)</span>&nbsp;to&nbsp;<span class="math inline">\(m\)</span>&nbsp;do&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Update&nbsp;the&nbsp;ensemble</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\mathbf{x}_{i,k}^{{\rm a}}=\mathbf{x}_{i,k}^{\mathrm{f}}+K_{k}\left({\mathbf{y}_{i,k}-\mathcal{H}_{k}\left(\mathbf{x}^{\mathrm{f}}_{i,k}\right)}\right)\)</span>&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\({\displaystyle \mathbf{x}_{i,k+1}^{\mathrm{f}}=\mathcal{M}_{k+1}\left(\mathbf{x}^{\mathrm{a}}_{i,k}\right)}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Compute&nbsp;the&nbsp;ensemble&nbsp;forecast</p>
<p>&nbsp;&nbsp;end&nbsp;for</p>
<p>end&nbsp;for</p>
</div>
</div>
</div>
</section>
<section id="localization-and-inflation" class="slide level2 center">
<h2>Localization and Inflation</h2>
<p>We have traded the extended Kalman filter for a seemingly considerably cheaper filter meant to achieve similar performances.</p>
<p>But this comes with significant <span style="color: red">drawbacks</span>.</p>
<ul>
<li><p>Fundamentally, one cannot hope to represent the full error covariance matrix of a complex high-dimensional system with only a f<span style="color: magenta">ew modes</span> <span class="math inline">\(m\ll n\)</span>, usually from a few dozens to a few hundreds.</p></li>
<li><p>This implies large <span style="color: magenta"><em>sampling errors</em></span>, meaning that the error covariance matrix is only sampled by a limited number of modes.</p></li>
<li><p>This <span style="color: magenta">rank-deficiency</span>is accompanied by <span style="color: magenta">spurious correlations</span> at long distances that strongly affect the filter performance.</p></li>
<li><p>Even though the unstable degrees of freedom of dynamical systems that we wish to control with a filter are usually far fewer than the dimension of the system, they often still represent a substantial fraction of the total degrees of freedom. Forecasting an ensemble of such size is usually not affordable.</p></li>
</ul>
</section>
<section id="section-22" class="slide level2 center">
<h2></h2>
<p>The consequence of this issue always is the <span style="color: red">divergence of the filter</span>.</p>
<p>Hence, the EnKF is useful on the condition that efficient <span style="color: magenta">fixes</span> are applied.</p>
<ul>
<li><p>To make it a viable algorithm, one first needs to cope with the rank-deficiency of the filter and with its manifestations, i.e. sampling errors.</p></li>
<li><p>Fortunately, there are clever tricks to overcome this major issue, known as <span style="color: magenta"><em>localization</em></span> and <span style="color: magenta"><em>inflation</em></span>, which explains, ultimately, the broad success of the EnKF in geosciences and engineering.</p></li>
</ul>
</section>
<section id="localization" class="slide level2 center">
<h2>Localization</h2>
<p>For many systems with <span style="color: magenta">geographic spread</span>, distant observables are weakly correlated.</p>
<p>In other words, two distant parts of the system are almost <span style="color: magenta">independent</span> at least for short time scales.</p>
<p>It is possible to exploit this relative independence and <span style="color: magenta">spatially localize</span>the analysis. This has been naturally termed <span style="color: magenta"><em>localization</em></span>.</p>
<p>There are two types of localization:</p>
<ul>
<li><p><span style="color: magenta">Domain</span> localization, where instead of performing a global analysis valid at any location in the domain, we perform a local analysis to update the local state variables using local observations.</p></li>
<li><p><span style="color: magenta">Covariance</span> localization focuses on the forecast error covariance matrix. It is based on the remark that the forecast error covariance matrix <span class="math inline">\(\mathbf{P}_{{\rm f}}\)</span> is of low rank, at most <span class="math inline">\(m-1,\)</span> and that this rank-deficiency could be cured by filtering these empirical covariances.</p></li>
</ul>
<p>For implementation details, please consult <span class="citation" data-cites="asch2016data">(<a href="#/references" role="doc-biblioref" onclick="">Asch, Bocquet, and Nodet 2016</a>)</span>.</p>
</section>
<section id="inflation" class="slide level2 center">
<h2>Inflation</h2>
<p>Even when the analysis is made local, the error covariance matrices are still evaluated with an ensemble of limited size.</p>
<ul>
<li><p>This often leads to sampling errors and <span style="color: magenta">spurious correlations</span>.</p></li>
<li><p>With a proper localization scheme, they might be significantly reduced.</p></li>
<li><p>However small are the residual errors, they will accumulate and they will <span style="color: magenta">carry over</span> to the next cycles of the sequential EnKF scheme. As a consequence, there is always a risk that the filter may ultimately diverge.</p></li>
</ul>
<p>One way around is to <span style="color: magenta">inflate</span> the error covariance matrix by a factor <span class="math inline">\(\lambda^{2}\)</span> slightly greater than <span class="math inline">\(1\)</span> before or after the analysis.</p>
<ul>
<li>For instance, after the analysis, <span class="math display">\[\mathbf{P}^{\rm a}\longrightarrow\lambda^{2}\mathbf{P}^{\mathrm{a}}.\]</span></li>
</ul>
</section>
<section id="section-23" class="slide level2 center">
<h2></h2>
<ul>
<li>Another way to achieve this is to inflate the <span style="color: magenta">ensemble</span>, <span class="math display">\[\mathbf{x}^{\mathrm{a}}_{i}\longrightarrow\overline{\mathbf{x}}^{{\rm a}}+\lambda{\mathbf{x}^{\mathrm{a}}_{i}-\overline{\mathbf{x}}^{{\rm a}}},\]</span> which can alternatively be enforced on the prior (forecast) ensemble. This type of inflation is called <span style="color: magenta"><em>multiplicative inflation.</em></span></li>
</ul>
<p>For implementation details, please consult <span class="citation" data-cites="asch2016data">(<a href="#/references" role="doc-biblioref" onclick="">Asch, Bocquet, and Nodet 2016</a>)</span>.</p>
<!-- 
-   Many variants of the EnKF algorithm have been proposed to overcome
    some of its weaknesses. We will just mention some of them, and refer
    the reader to \[Asch2016, Evensen2009\] for full details and further
    references.

    -   The [*ensemble square root*]{style="color: magenta"}, or
        deterministic ensemble Kalman filter, which does not perturb the
        observations.

    -   The [*local ensemble*]{style="color: magenta"}[
        ]{style="color: magenta"}Kalman filter that remedies so-called
        divergence of the EnKF due to the rank deficiency of its
        approximated covariance matrix.

    -   The [*maximum likelihood*]{style="color: magenta"} ensemble
        filter that generalizes the BLUE update to nonlinear observation
        operators.

    -   [*Hierarchical*]{style="color: magenta"} EnKF based on the use
        of a Bayesian statistical hierarchy.  -->
</section>
<section id="section-24" class="slide level2 center">
<h2></h2>
<div class="center">
<p><strong><span style="color: blue">EXAMPLES</span></strong></p>
</div>
</section>
<section id="section-25" class="slide level2 center">
<h2></h2>
<p>As in the previous Lecture, we consider the same scalar 4D-Var example, but this time apply the Kalman filter to it.</p>
<p>We take the most <span style="color: magenta">simple linear forecast model</span>, <span class="math display">\[\frac{\mathrm{d}x}{\mathrm{d}t}=-\alpha x,\]</span> with <span class="math inline">\(\alpha\)</span> a known positive constant.</p>
<p>We assume the same discrete dynamics considered in with a <span style="color: magenta">single observation</span> at time step <span class="math inline">\(3.\)</span></p>
<p>The <span style="color: magenta">stochastic system</span> <a href="#/kalman-filters---stochastic-model" class="quarto-xref">1</a>-<a href="#/kalman-filters---stochastic-model" class="quarto-xref">2</a> is <span class="math display">\[\begin{aligned}
    x_{k+1}^{\mathrm{t}} &amp; =M(x_{k}^{\mathrm{t}})+w_{k},\\
    y_{k+1} &amp; =x_{k}^{\mathrm{t}}+v_{k},
    \end{aligned}\]</span> where <span class="math inline">\(w_{k}\thicksim\mathcal{N}(0,\sigma_{Q}^{2}),\)</span> <span class="math inline">\(v_{k}\thicksim\mathcal{N}(0,\sigma_{R}^{2})\)</span> and <span class="math inline">\(x_{0}^{\mathrm{t}}-x_{0}^{\mathrm{b}}\thicksim\mathcal{N}(0,\sigma_{B}^{2}).\)</span></p>
</section>
<section id="section-26" class="slide level2 center">
<h2></h2>
<p>The <span style="color: magenta">Kalman filter</span> steps are</p>
<p><strong>Forecast:</strong></p>
<p><span class="math display">\[\begin{aligned}
x_{k+1}^{\mathrm{f}} &amp; =M(x_{k}^{\mathrm{a}})=\gamma x_{k},\\
P_{k+1}^{\mathrm{f}} &amp; =\gamma^{2}P_{k}^{\mathrm{a}}+\sigma_{Q}^{2}.
\end{aligned}\]</span></p>
<p><strong>Analysis:</strong></p>
<p><span class="math display">\[\begin{aligned}
K_{k+1} &amp; =P_{k+1}^{\mathrm{f}}H\left(H^{2}P_{k+1}^{\mathrm{f}}+\sigma_{R}^{2}\right)^{-1},\\
x_{k+1}^{\mathrm{a}} &amp; =x_{k+1}^{\mathrm{f}}+K_{k+1}(x_{k+1}^{\mathrm{o}}-Hx_{k+1}^{\mathrm{f}}),\\
P_{k+1}^{\mathrm{a}} &amp; =(1-K_{k+1}H)P_{k+1}^{\mathrm{f}}=\left(\frac{1}{P_{k+1}^{\mathrm{f}}}+\frac{1}{\sigma_{R}^{2}}\right)^{-1},\quad H=1.
\end{aligned}\]</span></p>
<p><strong>Initialization:</strong></p>
<p><span class="math display">\[\begin{equation}
x_{0}^{\mathrm{a}}  =x_{0}^{\mathrm{b}},\quad
P_{0}^{\mathrm{a}}  =\sigma_{B}^{2}.
\end{equation}\]</span></p>
</section>
<section id="section-27" class="slide level2 center">
<h2></h2>
<p>We start with the initial state, at time step <span class="math inline">\(k=0.\)</span> The initial conditions are as above. The forecast is <span class="math display">\[\begin{aligned}
    x_{1}^{\mathrm{f}} &amp; =M(x_{0}^{\mathrm{a}})=\gamma x_{0}^{\mathrm{b}},\\
    P_{1}^{\mathrm{f}} &amp; =\gamma^{2}\sigma_{B}^{2}+\sigma_{Q}^{2}.
    \end{aligned}\]</span></p>
<p>Since there is no observation available, <span class="math inline">\(H=0,\)</span> and the analysis gives,</p>
<p><span class="math display">\[\begin{aligned}
    K_{1} &amp; =0,\\
    x_{1}^{\mathrm{a}} &amp; =x_{1}^{\mathrm{f}}=\gamma x_{0}^{\mathrm{b}},\\
    P_{1}^{\mathrm{a}} &amp; =P_{1}^{\mathrm{f}}=\gamma^{2}\sigma_{B}^{2}+\sigma_{Q}^{2}.
    \end{aligned}\]</span></p>
<p>At the next time step, <span class="math inline">\(k=1,\)</span> and the forecast gives</p>
<p><span class="math display">\[\begin{aligned}
    x_{2}^{\mathrm{f}} &amp; =M(x_{1}^{\mathrm{a}})=\gamma^{2}x_{0}^{\mathrm{b}},\\
    P_{2}^{\mathrm{f}} &amp; =\gamma^{2}P_{1}^{\mathrm{a}}+\sigma_{Q}^{2}=\gamma^{4}\sigma_{B}^{2}+(\gamma^{2}+1)\sigma_{Q}^{2}.
    \end{aligned}\]</span></p>
</section>
<section id="section-28" class="slide level2 center">
<h2></h2>
<p>Once again there is no observation available, <span class="math inline">\(H=0,\)</span> and the analysis yields <span class="math display">\[\begin{aligned}
    K_{2} &amp; =0,\\
    x_{2}^{\mathrm{a}} &amp; =x_{2}^{\mathrm{f}}=\gamma^{2}x_{0}^{\mathrm{b}},\\
    P_{2}^{\mathrm{a}} &amp; =P_{2}^{\mathrm{f}}=\gamma^{4}\sigma_{B}^{2}+(\gamma^{2}+1)\sigma_{Q}^{2}.
    \end{aligned}\]</span></p>
<p>Moving on to <span class="math inline">\(k=2,\)</span> we have the new forecast, <span class="math display">\[\begin{aligned}
    x_{3}^{\mathrm{f}} &amp; =M(x_{2}^{\mathrm{a}})=\gamma^{3}x_{0}^{\mathrm{b}},\\
    P_{3}^{\mathrm{f}} &amp; =\gamma^{2}P_{2}^{\mathrm{a}}+\sigma_{Q}^{2}=\gamma^{6}\sigma_{B}^{2}+(\gamma^{4}+\gamma^{2}+1)\sigma_{Q}^{2}.
    \end{aligned}\]</span></p>
<p>Now there is an <span style="color: magenta">observation</span>, <span class="math inline">\(x_{3}^{\mathrm{o}},\)</span> available, so <span class="math inline">\(H=1\)</span> and the analysis is <span class="math display">\[\begin{aligned}
    K_{3} &amp; =P_{3}^{\mathrm{f}}\left(P_{3}^{\mathrm{f}}+\sigma_{R}^{2}\right)^{-1},\\
    x_{3}^{\mathrm{a}} &amp; =x_{3}^{\mathrm{f}}+K_{3}(x_{3}^{\mathrm{o}}-x_{3}^{\mathrm{f}}),\\
    P_{3}^{\mathrm{a}} &amp; =(1-K_{3})P_{3}^{\mathrm{f}}.
    \end{aligned}\]</span></p>
</section>
<section id="section-29" class="slide level2 center">
<h2></h2>
<p>Substituting and simplifying, we find <span id="eq-saclarKF_xa"><span class="math display">\[x_{3}^{\mathrm{a}}=\gamma^{3}x_{0}^{\mathrm{b}}+\frac{\gamma^{6}\sigma_{B}^{2}+(\gamma^{4}+\gamma^{2}+1)\sigma_{Q}^{2}}{\sigma_{R}^{2}+\gamma^{6}\sigma_{B}^{2}+(\gamma^{4}+\gamma^{2}+1)\sigma_{Q}^{2}}\left(x_{3}^{\mathrm{o}}-\gamma^{3}x_{0}^{\mathrm{b}}\right).\label{eq:saclarKF_xa} \tag{9}\]</span></span></p>
<p><strong>Case 1:</strong> Assume we have a <span style="color: magenta">perfect model</span>, then <span class="math inline">\(\sigma_{Q}^{2}=0\)</span> and the Kalman filter state <a href="#/section-29" class="quarto-xref">9</a> becomes <span class="math display">\[x_{3}^{\mathrm{a}}=\gamma^{3}x_{0}^{\mathrm{b}}+\frac{\gamma^{6}\sigma_{B}^{2}}{\sigma_{R}^{2}+\gamma^{6}\sigma_{B}^{2}}\left(x_{3}^{\mathrm{o}}-\gamma^{3}x_{0}^{\mathrm{b}}\right),\]</span> which is precisely the 4D-Var expression obtained before.</p>
<p><strong>Case 2:</strong> When the parameter <span class="math inline">\(\alpha\)</span> tends to zero, then <span class="math inline">\(\gamma\)</span> tends to one, the <span style="color: magenta">model is stationary</span> and the Kalman filter state <a href="#/section-29" class="quarto-xref">9</a> becomes <span class="math display">\[x_{3}^{\mathrm{a}}=x_{0}^{\mathrm{b}}+\frac{\sigma_{B}^{2}+3\sigma_{Q}^{2}}{\sigma_{R}^{2}+\sigma_{B}^{2}+3\sigma_{Q}^{2}}\left(x_{3}^{\mathrm{o}}-x_{0}^{\mathrm{b}}\right),\]</span></p>
</section>
<section id="section-30" class="slide level2 center">
<h2></h2>
<p>which, when <span class="math inline">\(\sigma_{Q}^{2}=0,\)</span> reduces to the 3D-Var solution, <span class="math display">\[x_{3}^{\mathrm{a}}=x_{0}^{\mathrm{b}}+\frac{\sigma_{B}^{2}}{\sigma_{R}^{2}+\sigma_{B}^{2}}\left(x_{3}^{\mathrm{o}}-x_{0}^{\mathrm{b}}\right),\]</span> that was obtained before.</p>
<p><strong>Case 3:</strong> When <span class="math inline">\(\alpha\)</span> tends to infinity, then <span class="math inline">\(\gamma\)</span> goes to zero, and we are in the case where there is <span style="color: magenta">no longer any memory</span> with <span class="math display">\[x_{3}^{\mathrm{a}}=\frac{\sigma_{Q}^{2}}{\sigma_{R}^{2}+\sigma_{Q}^{2}}x_{3}^{\mathrm{o}}.\]</span> Then, if the model is perfect, <span class="math inline">\(\sigma_{Q}^{2}=0\)</span> and <span class="math inline">\(x_{3}^{\mathrm{a}}=0.\)</span> If the observation is perfect, <span class="math inline">\(\sigma_{R}^{2}=0\)</span> and <span class="math inline">\(x_{3}^{\mathrm{a}}=x_{3}^{\mathrm{o}}.\)</span></p>
<p>This example shows the <span style="color: magenta">complete chain</span>, from the Kalman filter solution, through the 4D-Var, and finally reaching the 3D-Var one. Hopefully this clarifies the relationship between the three and demonstrates why the Kalman filter provides the <span style="color: magenta">most general solution</span> possible.</p>
</section>
<section id="section-31" class="slide level2 larger center">
<h2></h2>
<div class="center">
<p><strong><span style="color: blue">PRACTICAL GUIDELINES</span></strong></p>
</div>
</section>
<section id="general-guidelines" class="slide level2 center">
<h2>General Guidelines</h2>
<p>We briefly point out some important practical considerations. It should now be clear that there are four basic ingredients in any inverse or data assimilation problem:</p>
<ol type="1">
<li><p>Observation or measured data.</p></li>
<li><p>A forward or direct model of the real-world context.</p></li>
<li><p>A backwards or adjoint model, in the variational case. A probabilistic framework, in the statistical case.</p></li>
<li><p>An optimization cycle.</p></li>
</ol>
<p>But where does one start?</p>
<p>The traditional approach, often employed in mathematical and numerical modeling, is to begin with some simplified, or at least well-known, situation.</p>
<p>Once the above four items have been successfully implemented and tested on this instance, we then proceed to take into account more and more reality in the form of real data, more realistic models, more robust optimization procedures, etc.</p>
</section>
<section id="section-32" class="slide level2 center">
<h2></h2>
<p>In other words, we introduce uncertainty, but into a system where we at least control some of the aspects.</p>
<p>Twin experiments, or synthetic runs, are a basic and indispensable tool for all inverse problems. In order to evaluate the performance of a data assimilation system we invariably begin with the following methodology.</p>
<ol type="1">
<li><p>Fix all parameters and unknowns and define a reference trajectory, obtained from a run of the direct model call this the “truth”.</p></li>
<li><p>Derive a set of (synthetic) measurements, or background data, from this “true” run.</p></li>
<li><p>Optionally, perturb these observations in order to generate a more realistic observed state.</p></li>
<li><p>Run the data assimilation or inverse problem algorithm, starting from an initial guess (different from the “true” initial state used above), using the synthetic observations.</p></li>
<li><p>Evaluate the performance, modify the model/algorithm/observations, and cycle back to step 1.</p></li>
</ol>
</section>
<section id="section-33" class="slide level2 center">
<h2></h2>
<p>Twin experiments thus provide a <span style="color: magenta">well-structured methodological framework</span>.</p>
<p>Within this framework we can perform different “<span style="color: magenta">stress tests</span>” of our system.</p>
<p>We can modify the observation network,</p>
<ul>
<li><p>increase or decrease (even switch off) the uncertainty,</p></li>
<li><p>test the robustness of the optimization method,</p></li>
<li><p>even modify the model.</p></li>
</ul>
<p>In fact, these experiments can be performed on the full physical model, or on some simpler (or reduced-order) model.</p>
<p>Toy models are, by definition, simplified models that we can play with. Yes, but these are of course “serious games.” In certain complex physical contexts, of which meteorology is a famous example, we have well-established toy models, often of increasing complexity. These can be substituted for the real model, whose computational complexity is often too large, and provide a cheaper test-bed.</p>
</section>
<section id="section-34" class="slide level2 center">
<h2></h2>
<p>Some well-known examples of toy models are:</p>
<ul>
<li><p>Lorenz models that are used as an avatar for weather simulations.</p></li>
<li><p>Various harmonic oscillators that are used to simulate dynamic systems.</p></li>
<li><p>Other well-known models are the Ising model in physics, the <span style="color: magenta">Lotka-Volterra</span> model in life sciences, and the Schelling model in social sciences.</p></li>
</ul>
<p>Machine Learning (ML) is becoming more and more present in our daily lives, and in scientific research. The use of ML in DA and Inverse modeling will be dealt with in the <span style="color: magenta">Advanced Course</span>, where we will consider:</p>
<ul>
<li><p>ML-based Surrogate Models.</p></li>
<li><p>ML-based Surrogate Models.</p></li>
<li><p>Scientific ML.</p></li>
</ul>
</section>
<section id="kalman-filter-extensions" class="slide level2 center">
<h2>Kalman Filter — extensions</h2>
<p>There are many variants, extensions and <span style="color: magenta">generalizations</span> of the Kalman Filter.</p>
<p>In the <span style="color: magenta">Advanced</span> Course, we will study in more detail:</p>
<p>ensemble Kalman Filters</p>
<ul>
<li><p>Bayesian and nonlinear Kalman Filters: extended, unscented</p></li>
<li><p>particle filters</p></li>
</ul>
<p>One usually has to choose between</p>
<ul>
<li><p>linear Kalman filters</p></li>
<li><p>ensemble Kalman filters</p></li>
<li><p>nonlinear filters</p></li>
<li><p>hybrid variational-filter methods.</p></li>
</ul>
<p>These questions will be addressed later.</p>
</section>
<section id="where-are-we-in-the-inference-cycle" class="slide level2 center">
<h2>Where are we in the Inference cycle</h2>

<img data-src="../images/inference_cycle.png" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;4: DA is the inductive phase of DTs
</p></section>
<section id="open-source-software" class="slide level2 center">
<h2>Open-source software</h2>
<p>Various open-source repositories and codes are available for both academic and operational data assimilation.</p>
<ol type="1">
<li><p>DARC: <a href="https://research.reading.ac.uk/met-darc/" class="uri">https://research.reading.ac.uk/met-darc/</a> from Reading, UK.</p></li>
<li><p>DAPPER: <a href="https://github.com/nansencenter/DAPPER" class="uri">https://github.com/nansencenter/DAPPER</a> from Nansen, Norway.</p></li>
<li><p>DART: <a href="https://dart.ucar.edu/" class="uri">https://dart.ucar.edu/</a> from NCAR, US, specialized in ensemble DA.</p></li>
<li><p>OpenDA: <a href="https://www.openda.org/" class="uri">https://www.openda.org/</a>.</p></li>
<li><p>Verdandi: <a href="http://verdandi.sourceforge.net/" class="uri">http://verdandi.sourceforge.net/</a> from INRIA, France.</p></li>
<li><p>PyDA: <a href="https://github.com/Shady-Ahmed/PyDA" class="uri">https://github.com/Shady-Ahmed/PyDA</a>, a Python implementation for academic use.</p></li>
<li><p>Filterpy: <a href="https://github.com/rlabbe/filterpy" class="uri">https://github.com/rlabbe/filterpy</a>, dedicated to KF variants.</p></li>
<li><p>EnKF; <a href="https://enkf.nersc.no/" class="uri">https://enkf.nersc.no/</a>, the original Ensemble KF from Geir Evensen.</p></li>
</ol>
<!-- 
1.  K. Law, A. Stuart, K. Zygalakis. *Data Assimilation. A Mathematical
    Introduction*. Springer, 2015.

2.  S. Sarkka. *Bayesian Filtering and Smoothing.* Cambridge University
    Press, 2013.

3.  S. Sarkka, A. Solin. Applied Stochastic Differential Equations.
    Cambridge University Press, 2019.

4.  G. Evensen. *Data assimilation, The Ensemble Kalman Filter*, 2nd
    ed., Springer, 2009.

5.  A. Tarantola. *Inverse problem theory and methods for model
    parameter estimation.* SIAM. 2005.

6.  O. Talagrand. Assimilation of observations, an introduction. *J.
    Meteorological Soc. Japan*, **75**, 191, 1997.

7.  F.X. Le Dimet, O. Talagrand. Variational algorithms for analysis and
    assimilation of meteorological observations: theoretical aspects.
    *Tellus,* **38**(2), 97, 1986.

8.  J.-L. Lions. Exact controllability, stabilization and perturbations
    for distributed systems. *SIAM Rev.*, **30**(1):1, 1988.

9.  J. Nocedal, S.J. Wright. *Numerical Optimization*. Springer, 2006.

10. F. Tröltzsch. *Optimal Control of Partial Differential Equations*.
    AMS, 2010.

[^1]: Apparently, following a prior invention by Stratonovich, one year
    earlier. -->
</section>
<section id="references" class="title-slide slide level1 unnumbered smaller scrollable">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-asch2022toolbox" class="csl-entry" role="listitem">
Asch, Mark. 2022. <em>A Toolbox for Digital Twins: From Model-Based to Data-Driven</em>. SIAM.
</div>
<div id="ref-asch2016data" class="csl-entry" role="listitem">
Asch, Mark, Marc Bocquet, and Maëlle Nodet. 2016. <em>Data Assimilation: Methods, Algorithms, and Applications</em>. SIAM.
</div>
<div id="ref-evensen2009data" class="csl-entry" role="listitem">
Evensen, Geir et al. 2009. <em>Data Assimilation: The Ensemble Kalman Filter</em>. Vol. 2. Springer.
</div>
<div id="ref-law2015data" class="csl-entry" role="listitem">
Law, Kody, Andrew Stuart, and Kostas Zygalakis. 2015. <span>“Data Assimilation.”</span> <em>Cham, Switzerland: Springer</em> 214: 52.
</div>
<div id="ref-li2001optimality" class="csl-entry" role="listitem">
Li, Zhijin, and IM Navon. 2001. <span>“Optimality of Variational Data Assimilation and Its Relationship with the Kalman Filter and Smoother.”</span> <em>Quarterly Journal of the Royal Meteorological Society</em> 127 (572): 661–83.
</div>
<div id="ref-sarkka2023bayesian" class="csl-entry" role="listitem">
Särkkä, Simo, and Lennart Svensson. 2023. <em>Bayesian Filtering and Smoothing</em>. Vol. 17. Cambridge university press.
</div>
</div>


</section>


    <div class="quarto-auto-generated-content">
<p><img src="images/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://flexie.github.io/CSE-8803-Twin/">🔗 https://flexie.github.io/CSE-8803-Twin/</a></p>
</div>
</div></div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>