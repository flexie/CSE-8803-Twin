<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.528">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Digital Twins for Physical Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../images/digitaltwin.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://flexie.github.io/CSE-8803-Twin/" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../syllabus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../goals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Goals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../schedule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedule</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../seminar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Seminar</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Final Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#normalizing-flows-as-generative-model" id="toc-normalizing-flows-as-generative-model" class="nav-link active" data-scroll-target="#normalizing-flows-as-generative-model">Normalizing Flows as generative model</a></li>
  <li><a href="#normalizing-flows-on-images" id="toc-normalizing-flows-on-images" class="nav-link" data-scroll-target="#normalizing-flows-on-images">Normalizing Flows on images</a>
  <ul class="collapse">
  <li><a href="#dequantization" id="toc-dequantization" class="nav-link" data-scroll-target="#dequantization">Dequantization</a></li>
  <li><a href="#variational-dequantization" id="toc-variational-dequantization" class="nav-link" data-scroll-target="#variational-dequantization">Variational Dequantization</a></li>
  <li><a href="#coupling-layers" id="toc-coupling-layers" class="nav-link" data-scroll-target="#coupling-layers">Coupling layers</a></li>
  <li><a href="#training-loop" id="toc-training-loop" class="nav-link" data-scroll-target="#training-loop">Training loop</a></li>
  </ul></li>
  <li><a href="#multi-scale-architecture" id="toc-multi-scale-architecture" class="nav-link" data-scroll-target="#multi-scale-architecture">Multi-scale architecture</a>
  <ul class="collapse">
  <li><a href="#squeeze-and-split" id="toc-squeeze-and-split" class="nav-link" data-scroll-target="#squeeze-and-split">Squeeze and Split</a></li>
  <li><a href="#building-a-multi-scale-flow" id="toc-building-a-multi-scale-flow" class="nav-link" data-scroll-target="#building-a-multi-scale-flow">Building a multi-scale flow</a></li>
  </ul></li>
  <li><a href="#analysing-the-flows" id="toc-analysing-the-flows" class="nav-link" data-scroll-target="#analysing-the-flows">Analysing the flows</a>
  <ul class="collapse">
  <li><a href="#training-flow-variants" id="toc-training-flow-variants" class="nav-link" data-scroll-target="#training-flow-variants">Training flow variants</a></li>
  <li><a href="#density-modeling-and-sampling" id="toc-density-modeling-and-sampling" class="nav-link" data-scroll-target="#density-modeling-and-sampling">Density modeling and sampling</a></li>
  <li><a href="#interpolation-in-latent-space" id="toc-interpolation-in-latent-space" class="nav-link" data-scroll-target="#interpolation-in-latent-space">Interpolation in latent space</a></li>
  <li><a href="#visualization-of-latents-in-different-levels-of-multi-scale" id="toc-visualization-of-latents-in-different-levels-of-multi-scale" class="nav-link" data-scroll-target="#visualization-of-latents-in-different-levels-of-multi-scale">Visualization of latents in different levels of multi-scale</a></li>
  <li><a href="#visualizing-dequantization" id="toc-visualizing-dequantization" class="nav-link" data-scroll-target="#visualizing-dequantization">Visualizing Dequantization</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#invertible-1x1-convolution" id="toc-invertible-1x1-convolution" class="nav-link" data-scroll-target="#invertible-1x1-convolution">Invertible 1x1 convolution</a>
  <ul class="collapse">
  <li><a href="#a-small-pitfall" id="toc-a-small-pitfall" class="nav-link" data-scroll-target="#a-small-pitfall">A small pitfall</a></li>
  <li><a href="#a-complete-flow-block" id="toc-a-complete-flow-block" class="nav-link" data-scroll-target="#a-complete-flow-block">A complete flow block</a></li>
  </ul></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1">Conclusion</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<p>In this tutorial, we will take a closer look at complex, deep normalizing flows. The most popular, current application of deep normalizing flows is to model datasets of images. As for other generative models, images are a good domain to start working on because (1) CNNs are widely studied and strong models exist, (2) images are high-dimensional and complex, and (3) images are discrete integers. In this tutorial, we will review current advances in normalizing flows for image modeling, and get hands-on experience on coding normalizing flows. Note that normalizing flows are commonly parameter heavy and therefore computationally expensive. We will use relatively simple and shallow flows to save computational cost and allow you to run the notebook on CPU, but keep in mind that a simple way to improve the scores of the flows we study here is to make them deeper.</p>
<p>Throughout this notebook, we make use of <a href="https://pytorch-lightning.readthedocs.io/en/latest/">PyTorch Lightning</a>. The first cell imports our usual libraries.</p>
<div id="cell-1" class="cell" data-outputid="0fe20bf1-91a6-4fbc-a292-22e5b09aa6bc" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Standard libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">## Imports for plotting</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> set_matplotlib_formats</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>set_matplotlib_formats(<span class="st">'svg'</span>, <span class="st">'pdf'</span>) <span class="co"># For export</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> to_rgb</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>matplotlib.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sns.reset_orig()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">## Progress bar</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">## PyTorch</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Torchvision</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> MNIST</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch Lightning</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>: <span class="co"># Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">--</span>quiet pytorch<span class="op">-</span>lightning<span class="op">&gt;=</span><span class="fl">1.4</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> pytorch_lightning <span class="im">as</span> pl</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_lightning.callbacks <span class="im">import</span> LearningRateMonitor, ModelCheckpoint</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to the folder where the datasets are/should be downloaded (e.g. MNIST)</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>DATASET_PATH <span class="op">=</span> <span class="st">"../data"</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to the folder where the pretrained models are saved</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>CHECKPOINT_PATH <span class="op">=</span> <span class="st">"../saved_models/tutorial11"</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting the seed</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure that all operations are deterministic on GPU (if used) for reproducibility</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Fetching the device that will be used throughout this notebook</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>) <span class="cf">if</span> <span class="kw">not</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">"cuda:0"</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Using device"</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`
  set_matplotlib_formats('svg', 'pdf') # For export
INFO:lightning_fabric.utilities.seed:Seed set to 42</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Using device cpu</code></pre>
</div>
</div>
<p>We will use the MNIST dataset in this notebook. MNIST constitutes, despite its simplicity, a challenge for small generative models as it requires the global understanding of an image. At the same time, we can easily judge whether generated images come from the same distribution as the dataset (i.e.&nbsp;represent real digits), or not.</p>
<p>To deal better with the discrete nature of the images, we transform them from a range of 0-1 to a range of 0-255 as integers.</p>
<div id="cell-3" class="cell" data-outputid="1312d35a-d0c4-403b-e993-f51755aee71a" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert images from 0-1 to 0-255 (integers)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discretize(sample):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (sample <span class="op">*</span> <span class="dv">255</span>).to(torch.int32)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformations applied on each image =&gt; make them a tensor and discretize</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                                discretize])</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading the training dataset. We need to split it into a training and validation part</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> MNIST(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>train_set, val_set <span class="op">=</span> torch.utils.data.random_split(train_dataset, [<span class="dv">50000</span>, <span class="dv">10000</span>])</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading the test set</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>test_set <span class="op">=</span> MNIST(root<span class="op">=</span>DATASET_PATH, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>transform, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># We define a set of data loaders that we can use for various purposes later.</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that for actually training a model, we will use different data loaders</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># with a lower batch size.</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">256</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> data.DataLoader(val_set, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> data.DataLoader(test_set, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>, drop_last<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz
Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz
Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 9912422/9912422 [00:00&lt;00:00, 88727407.70it/s]
100%|██████████| 28881/28881 [00:00&lt;00:00, 31204454.87it/s]
100%|██████████| 1648877/1648877 [00:02&lt;00:00, 579297.24it/s]
100%|██████████| 4542/4542 [00:00&lt;00:00, 15107477.21it/s]
INFO:lightning_fabric.utilities.seed:Seed set to 42
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(</code></pre>
</div>
</div>
<p>In addition, we will define below a function to simplify the visualization of images/samples. Some training examples of the MNIST dataset is shown below.</p>
<div id="cell-5" class="cell" data-outputid="3f7b89be-b850-48c1-80ff-d00bb55b7daa" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_imgs(imgs, title<span class="op">=</span><span class="va">None</span>, row_size<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Form a grid of pictures (we use max. 8 columns)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    num_imgs <span class="op">=</span> imgs.shape[<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">isinstance</span>(imgs, torch.Tensor) <span class="cf">else</span> <span class="bu">len</span>(imgs)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    is_int <span class="op">=</span> imgs.dtype<span class="op">==</span>torch.int32 <span class="cf">if</span> <span class="bu">isinstance</span>(imgs, torch.Tensor) <span class="cf">else</span> imgs[<span class="dv">0</span>].dtype<span class="op">==</span>torch.int32</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    nrow <span class="op">=</span> <span class="bu">min</span>(num_imgs, row_size)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    ncol <span class="op">=</span> <span class="bu">int</span>(math.ceil(num_imgs<span class="op">/</span>nrow))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> torchvision.utils.make_grid(imgs, nrow<span class="op">=</span>nrow, pad_value<span class="op">=</span><span class="dv">128</span> <span class="cf">if</span> is_int <span class="cf">else</span> <span class="fl">0.5</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    np_imgs <span class="op">=</span> imgs.cpu().numpy()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the grid</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="fl">1.5</span><span class="op">*</span>nrow, <span class="fl">1.5</span><span class="op">*</span>ncol))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    plt.imshow(np.transpose(np_imgs, (<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)), interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> title <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        plt.title(title)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>show_imgs([train_set[i][<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-4-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="normalizing-flows-as-generative-model" class="level2">
<h2 class="anchored" data-anchor-id="normalizing-flows-as-generative-model">Normalizing Flows as generative model</h2>
<p>In the previous lectures, we have seen Energy-based models, Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) as example of generative models. However, none of them explicitly learn the probability density function <span class="math inline">\(p(x)\)</span> of the real input data. While VAEs model a lower bound, energy-based models only implicitly learn the probability density. GANs on the other hand provide us a sampling mechanism for generating new data, without offering a likelihood estimate. The generative model we will look at here, called Normalizing Flows, actually models the true data distribution <span class="math inline">\(p(x)\)</span> and provides us with an exact likelihood estimate. Below, we can visually compare VAEs, GANs and Flows (figure credit - <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Lilian Weng</a>):</p>
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/comparison_GAN_VAE_NF.png?raw=1" width="600px">
</center>
<p>The major difference compared to VAEs is that flows use <em>invertible</em> functions <span class="math inline">\(f\)</span> to map the input data <span class="math inline">\(x\)</span> to a latent representation <span class="math inline">\(z\)</span>. To realize this, <span class="math inline">\(z\)</span> must be of the same shape as <span class="math inline">\(x\)</span>. This is in contrast to VAEs where <span class="math inline">\(z\)</span> is usually much lower dimensional than the original input data. However, an invertible mapping also means that for every data point <span class="math inline">\(x\)</span>, we have a corresponding latent representation <span class="math inline">\(z\)</span> which allows us to perform lossless reconstruction (<span class="math inline">\(z\)</span> to <span class="math inline">\(x\)</span>). In the visualization above, this means that <span class="math inline">\(x=x'\)</span> for flows, no matter what invertible function <span class="math inline">\(f\)</span> and input <span class="math inline">\(x\)</span> we choose.</p>
<p>Nonetheless, how are normalizing flows modeling a probability density with an invertible function? The answer to this question is the rule for change of variables. Specifically, given a prior density <span class="math inline">\(p_z(z)\)</span> (e.g.&nbsp;Gaussian) and an invertible function <span class="math inline">\(f\)</span>, we can determine <span class="math inline">\(p_x(x)\)</span> as follows:</p>
<p><span class="math display">\[
\begin{split}
    \int p_x(x) dx &amp; = \int p_z(z) dz = 1 \hspace{1cm}\text{(by definition of a probability distribution)}\\
    \Leftrightarrow p_x(x) &amp; = p_z(z) \left|\frac{dz}{dx}\right| = p_z(f(x)) \left|\frac{df(x)}{dx}\right|
\end{split}
\]</span></p>
<p>Hence, in order to determine the probability of <span class="math inline">\(x\)</span>, we only need to determine its probability in latent space, and get the derivate of <span class="math inline">\(f\)</span>. Note that this is for a univariate distribution, and <span class="math inline">\(f\)</span> is required to be invertible and smooth. For a multivariate case, the derivative becomes a Jacobian of which we need to take the determinant. As we usually use the log-likelihood as objective, we write the multivariate term with logarithms below:</p>
<p><span class="math display">\[
\log p_x(\mathbf{x}) = \log p_z(f(\mathbf{x})) + \log{} \left|\det \frac{df(\mathbf{x})}{d\mathbf{x}}\right|
\]</span></p>
<p>Although we now know how a normalizing flow obtains its likelihood, it might not be clear what a normalizing flow does intuitively. For this, we should look from the inverse perspective of the flow starting with the prior probability density <span class="math inline">\(p_z(z)\)</span>. If we apply an invertible function on it, we effectively “transform” its probability density. For instance, if <span class="math inline">\(f^{-1}(z)=z+1\)</span>, we shift the density by one while still remaining a valid probability distribution, and being invertible. We can also apply more complex transformations, like scaling: <span class="math inline">\(f^{-1}(z)=2z+1\)</span>, but there you might see a difference. When you scale, you also change the volume of the probability density, as for example on uniform distributions (figure credit - <a href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang</a>):</p>
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/uniform_flow.png?raw=1" width="300px">
</center>
<p>You can see that the height of <span class="math inline">\(p(y)\)</span> should be lower than <span class="math inline">\(p(x)\)</span> after scaling. This change in volume represents <span class="math inline">\(\left|\frac{df(x)}{dx}\right|\)</span> in our equation above, and ensures that even after scaling, we still have a valid probability distribution. We can go on with making our function <span class="math inline">\(f\)</span> more complex. However, the more complex <span class="math inline">\(f\)</span> becomes, the harder it will be to find the inverse <span class="math inline">\(f^{-1}\)</span> of it, and to calculate the log-determinant of the Jacobian <span class="math inline">\(\log{} \left|\det \frac{df(\mathbf{x})}{d\mathbf{x}}\right|\)</span>. An easier trick to stack multiple invertible functions <span class="math inline">\(f_{1,...,K}\)</span> after each other, as all together, they still represent a single, invertible function. Using multiple, learnable invertible functions, a normalizing flow attempts to transform <span class="math inline">\(p_z(z)\)</span> slowly into a more complex distribution which should finally be <span class="math inline">\(p_x(x)\)</span>. We visualize the idea below (figure credit - <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Lilian Weng</a>):</p>
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/normalizing_flow_layout.png?raw=1" width="700px">
</center>
<p>Starting from <span class="math inline">\(z_0\)</span>, which follows the prior Gaussian distribution, we sequentially apply the invertible functions <span class="math inline">\(f_1,f_2,...,f_K\)</span>, until <span class="math inline">\(z_K\)</span> represents <span class="math inline">\(x\)</span>. Note that in the figure above, the functions <span class="math inline">\(f\)</span> represent the inverted function from <span class="math inline">\(f\)</span> we had above (here: <span class="math inline">\(f:Z\to X\)</span>, above: <span class="math inline">\(f:X\to Z\)</span>). This is just a different notation and has no impact on the actual flow design because all <span class="math inline">\(f\)</span> need to be invertible anyways. When we estimate the log likelihood of a data point <span class="math inline">\(x\)</span> as in the equations above, we run the flows in the opposite direction than visualized above. Multiple flow layers have been proposed that use a neural network as learnable parameters, such as the planar and radial flow. However, we will focus here on flows that are commonly used in image modeling, and will discuss them in the rest of the notebook along with the details of how to train a normalizing flow.</p>
</section>
<section id="normalizing-flows-on-images" class="level2">
<h2 class="anchored" data-anchor-id="normalizing-flows-on-images">Normalizing Flows on images</h2>
<p>To become familiar with normalizing flows, especially for the application of image modeling, it is best to discuss the different elements in a flow along with the implementation. As a general concept, we want to build a normalizing flow that maps an input image (here MNIST) to an equally sized latent space:</p>
<center width="100%" style="padding: 10px">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/image_to_gaussian.svg?raw=1" width="450px">
</center>
<p>As a first step, we will implement a template of a normalizing flow in PyTorch Lightning. During training and validation, a normalizing flow performs density estimation in the forward direction. For this, we apply a series of flow transformations on the input <span class="math inline">\(x\)</span> and estimate the probability of the input by determining the probability of the transformed point <span class="math inline">\(z\)</span> given a prior, and the change of volume caused by the transformations. During inference, we can do both density estimation and sampling new points by inverting the flow transformations. Therefore, we define a function <code>_get_likelihood</code> which performs density estimation, and <code>sample</code> to generate new examples. The functions <code>training_step</code>, <code>validation_step</code> and <code>test_step</code> all make use of <code>_get_likelihood</code>.</p>
<p>The standard metric used in generative models, and in particular normalizing flows, is bits per dimensions (bpd). Bpd is motivated from an information theory perspective and describes how many bits we would need to encode a particular example in our modeled distribution. The less bits we need, the more likely the example in our distribution. When we test for the bits per dimension of our test dataset, we can judge whether our model generalizes to new samples of the dataset and didn’t memorize the training dataset. In order to calculate the bits per dimension score, we can rely on the negative log-likelihood and change the log base (as bits are binary while NLL is usually exponential):</p>
<p><span class="math display">\[\text{bpd} = \text{nll} \cdot \log_2\left(\exp(1)\right) \cdot \left(\prod d_i\right)^{-1}\]</span></p>
<p>where <span class="math inline">\(d_1,...,d_K\)</span> are the dimensions of the input. For images, this would be the height, width and channel number. We divide the log likelihood by these extra dimensions to have a metric which we can compare for different image resolutions. In the original image space, MNIST examples have a bits per dimension score of 8 (we need 8 bits to encode each pixel as there are 256 possible values).</p>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageFlow(pl.LightningModule):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, flows, import_samples<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">            flows - A list of flows (each a nn.Module) that should be applied on the images.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">            import_samples - Number of importance samples to use during testing (see explanation below). Can be changed at any time</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flows <span class="op">=</span> nn.ModuleList(flows)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.import_samples <span class="op">=</span> import_samples</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create prior distribution for final latent space</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior <span class="op">=</span> torch.distributions.normal.Normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Example input for visualizing the graph</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.example_input_array <span class="op">=</span> train_set[<span class="dv">0</span>][<span class="dv">0</span>].unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, imgs):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The forward function is only used for visualizing the graph</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._get_likelihood(imgs)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, imgs):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Given a batch of images, return the latent representation z and ldj of the transformations</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        z, ldj <span class="op">=</span> imgs, torch.zeros(imgs.shape[<span class="dv">0</span>], device<span class="op">=</span><span class="va">self</span>.device)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> flow <span class="kw">in</span> <span class="va">self</span>.flows:</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>            z, ldj <span class="op">=</span> flow(z, ldj, reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _get_likelihood(<span class="va">self</span>, imgs, return_ll<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Given a batch of images, return the likelihood of those.</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co">        If return_ll is True, this function returns the log likelihood of the input.</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Otherwise, the ouptut metric is bits per dimension (scaled negative log likelihood)</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Insert your code here!!!</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate negative log likelihood</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (HINT: Step1- Use encoder, Step2- Calculate likelihood pz, Step3- Find likelihood px)</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#-------------------------</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculating bits per dimension</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        bpd <span class="op">=</span> nll <span class="op">*</span> np.log2(np.exp(<span class="dv">1</span>)) <span class="op">/</span> np.prod(imgs.shape[<span class="dv">1</span>:])</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> bpd.mean() <span class="cf">if</span> <span class="kw">not</span> return_ll <span class="cf">else</span> log_px</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, img_shape, z_init<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Sample a batch of images from the flow.</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample latent representation from prior</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> z_init <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> <span class="va">self</span>.prior.sample(sample_shape<span class="op">=</span>img_shape).to(device)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z_init.to(device)</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transform z to x by inverting the flows</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>        ldj <span class="op">=</span> torch.zeros(img_shape[<span class="dv">0</span>], device<span class="op">=</span>device)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> flow <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.flows):</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>            z, ldj <span class="op">=</span> flow(z, ldj, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>):</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> optim.Adam(<span class="va">self</span>.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># An scheduler is optional, but can help in flows to get the last bpd improvement</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>        scheduler <span class="op">=</span> optim.lr_scheduler.StepLR(optimizer, <span class="dv">1</span>, gamma<span class="op">=</span><span class="fl">0.99</span>)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [optimizer], [scheduler]</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> training_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalizing flows are trained by maximum likelihood =&gt; return bpd</span></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>._get_likelihood(batch[<span class="dv">0</span>])</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">'train_bpd'</span>, loss)</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>._get_likelihood(batch[<span class="dv">0</span>])</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">'val_bpd'</span>, loss)</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform importance sampling during testing =&gt; estimate likelihood M times for each image</span></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>        samples <span class="op">=</span> []</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.import_samples):</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>            img_ll <span class="op">=</span> <span class="va">self</span>._get_likelihood(batch[<span class="dv">0</span>], return_ll<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>            samples.append(img_ll)</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>        img_ll <span class="op">=</span> torch.stack(samples, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># To average the probabilities, we need to go from log-space to exp, and back to log.</span></span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Logsumexp provides us a stable implementation for this</span></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>        img_ll <span class="op">=</span> torch.logsumexp(img_ll, dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">-</span> np.log(<span class="va">self</span>.import_samples)</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate final bpd</span></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>        bpd <span class="op">=</span> <span class="op">-</span>img_ll <span class="op">*</span> np.log2(np.exp(<span class="dv">1</span>)) <span class="op">/</span> np.prod(batch[<span class="dv">0</span>].shape[<span class="dv">1</span>:])</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>        bpd <span class="op">=</span> bpd.mean()</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">'test_bpd'</span>, bpd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>test_step</code> function differs from the training and validation step in that it makes use of importance sampling. We will discuss the motiviation and details behind this after understanding how flows model discrete images in continuous space.</p>
<section id="dequantization" class="level3">
<h3 class="anchored" data-anchor-id="dequantization">Dequantization</h3>
<p>Normalizing flows rely on the rule of change of variables, which is naturally defined in continuous space. Applying flows directly on discrete data leads to undesired density models where arbitrarly high likelihood are placed on a few, particular values. See the illustration below:</p>
<center>
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/dequantization_issue.svg?raw=1" width="40%">
</center>
<p>The black points represent the discrete points, and the green volume the density modeled by a normalizing flow in continuous space. The flow would continue to increase the likelihood for <span class="math inline">\(x=0,1,2,3\)</span> while having no volume on any other point. Remember that in continuous space, we have the constraint that the overall volume of the probability density must be 1 (<span class="math inline">\(\int p(x)dx=1\)</span>). Otherwise, we don’t model a probability distribution anymore. However, the discrete points <span class="math inline">\(x=0,1,2,3\)</span> represent delta peaks with no width in continuous space. This is why the flow can place an infinite high likelihood on these few points while still representing a distribution in continuous space. Nonetheless, the learned density does not tell us anything about the distribution among the discrete points, as in discrete space, the likelihoods of those four points would have to sum to 1, not to infinity.</p>
<p>To prevent such degenerated solutions, a common solution is to add a small amount of noise to each discrete value, which is also referred to as dequantization. Considering <span class="math inline">\(x\)</span> as an integer (as it is the case for images), the dequantized representation <span class="math inline">\(v\)</span> can be formulated as <span class="math inline">\(v=x+u\)</span> where <span class="math inline">\(u\in[0,1)^D\)</span>. Thus, the discrete value <span class="math inline">\(1\)</span> is modeled by a distribution over the interval <span class="math inline">\([1.0, 2.0)\)</span>, the value <span class="math inline">\(2\)</span> by an volume over <span class="math inline">\([2.0, 3.0)\)</span>, etc. Our objective of modeling <span class="math inline">\(p(x)\)</span> becomes:</p>
<p><span class="math display">\[ p(x) = \int p(x+u)du = \int \frac{q(u|x)}{q(u|x)}p(x+u)du = \mathbb{E}_{u\sim q(u|x)}\left[\frac{p(x+u)}{q(u|x)} \right]\]</span></p>
<p>with <span class="math inline">\(q(u|x)\)</span> being the noise distribution. For now, we assume it to be uniform, which can also be written as <span class="math inline">\(p(x)=\mathbb{E}_{u\sim U(0,1)^D}\left[p(x+u) \right]\)</span>.</p>
<p>In the following, we will implement Dequantization as a flow transformation itself. After adding noise to the discrete values, we additionally transform the volume into a Gaussian-like shape. This is done by scaling <span class="math inline">\(x+u\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and applying the invert of the sigmoid function <span class="math inline">\(\sigma(z)^{-1} = \log z - \log 1-z\)</span>. If we would not do this, we would face two problems:</p>
<ol type="1">
<li>The input is scaled between 0 and 256 while the prior distribution is a Gaussian with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(1\)</span>. In the first iterations after initializing the parameters of the flow, we would have extremely low likelihoods for large values like <span class="math inline">\(256\)</span>. This would cause the training to diverge instantaneously.</li>
<li>As the output distribution is a Gaussian, it is beneficial for the flow to have a similarly shaped input distribution. This will reduce the modeling complexity that is required by the flow.</li>
</ol>
<p>Overall, we can implement dequantization as follows:</p>
<div id="cell-11" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dequantization(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">1e-5</span>, quants<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha - small constant that is used to scale the original input.</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">                    Prevents dealing with values very close to 0 and 1 when inverting the sigmoid</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">            quants - Number of possible discrete values (usually 256 for 8-bit image)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quants <span class="op">=</span> quants</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z, ldj, reverse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            z, ldj <span class="op">=</span> <span class="va">self</span>.dequant(z, ldj)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            z, ldj <span class="op">=</span> <span class="va">self</span>.sigmoid(z, ldj, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            z, ldj <span class="op">=</span> <span class="va">self</span>.sigmoid(z, ldj, reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z <span class="op">*</span> <span class="va">self</span>.quants</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">+=</span> np.log(<span class="va">self</span>.quants) <span class="op">*</span> np.prod(z.shape[<span class="dv">1</span>:])</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.floor(z).clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="va">self</span>.quants<span class="op">-</span><span class="dv">1</span>).to(torch.int32)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, z, ldj, reverse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Applies an invertible sigmoid transformation</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">+=</span> (<span class="op">-</span>z<span class="op">-</span><span class="dv">2</span><span class="op">*</span>F.softplus(<span class="op">-</span>z)).<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.sigmoid(z)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reversing scaling for numerical stability</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">-=</span> np.log(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha) <span class="op">*</span> np.prod(z.shape[<span class="dv">1</span>:])</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> (z <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">self</span>.alpha) <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha) <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">self</span>.alpha  <span class="co"># Scale to prevent boundaries 0 and 1</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">+=</span> np.log(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha) <span class="op">*</span> np.prod(z.shape[<span class="dv">1</span>:])</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">+=</span> (<span class="op">-</span>torch.log(z) <span class="op">-</span> torch.log(<span class="dv">1</span><span class="op">-</span>z)).<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.log(z) <span class="op">-</span> torch.log(<span class="dv">1</span><span class="op">-</span>z)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dequant(<span class="va">self</span>, z, ldj):</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transform discrete values to continuous volumes</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> z.to(torch.float32)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> z <span class="op">+</span> torch.rand_like(z).detach()</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> z <span class="op">/</span> <span class="va">self</span>.quants</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>        ldj <span class="op">-=</span> np.log(<span class="va">self</span>.quants) <span class="op">*</span> np.prod(z.shape[<span class="dv">1</span>:])</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A good check whether a flow is correctly implemented or not, is to verify that it is invertible. Hence, we will dequantize a randomly chosen training image, and then quantize it again. We would expect that we would get the exact same image out:</p>
<div id="cell-13" class="cell" data-outputid="761b8332-fb7e-417b-b899-b8e2fca17b35" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Testing invertibility of dequantization layer</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> train_set[<span class="dv">0</span>][<span class="dv">0</span>].unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>ldj <span class="op">=</span> torch.zeros(<span class="dv">1</span>,)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>dequant_module <span class="op">=</span> Dequantization()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>deq_img, ldj <span class="op">=</span> dequant_module(orig_img, ldj, reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>reconst_img, ldj <span class="op">=</span> dequant_module(deq_img, ldj, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>d1, d2 <span class="op">=</span> torch.where(orig_img.squeeze() <span class="op">!=</span> reconst_img.squeeze())</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(d1) <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Dequantization was not invertible."</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d1.shape[<span class="dv">0</span>]):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Original value:"</span>, orig_img[<span class="dv">0</span>,<span class="dv">0</span>,d1[i], d2[i]].item())</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Reconstructed value:"</span>, reconst_img[<span class="dv">0</span>,<span class="dv">0</span>,d1[i], d2[i]].item())</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Successfully inverted dequantization"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer is not strictly invertible due to float precision constraints</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># assert (orig_img == reconst_img).all().item()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:lightning_fabric.utilities.seed:Seed set to 42</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Successfully inverted dequantization</code></pre>
</div>
</div>
<p>The test succeeds as we would expect. However, there is a chance that the test fails due to numerical inaccuracies in the sigmoid invert. While the input space to the inverted sigmoid is scaled between 0 and 1, the output space is between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. And as we use 32 bits to represent the numbers (in addition to applying logs over and over again), such inaccuries can occur and should not be worrisome. Nevertheless, it is good to be aware of them, and can be improved by using a double tensor (float64).</p>
<p>Finally, we can take our dequantization and actually visualize the distribution it transforms the discrete values into:</p>
<div id="cell-15" class="cell" data-outputid="ec9281b2-d8fd-4cb6-90ef-f72972b4b3ac" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_dequantization(quants, prior<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Function for visualizing the dequantization values of discrete values in continuous space</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior over discrete values. If not given, a uniform is assumed</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> prior <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        prior <span class="op">=</span> np.ones(quants, dtype<span class="op">=</span>np.float32) <span class="op">/</span> quants</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> prior <span class="op">/</span> prior.<span class="bu">sum</span>()  <span class="co"># Ensure proper categorical distribution</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    inp <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.01</span>).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># Possible continuous values we want to consider</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    ldj <span class="op">=</span> torch.zeros(inp.shape[<span class="dv">0</span>])</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    dequant_module <span class="op">=</span> Dequantization(quants<span class="op">=</span>quants)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Invert dequantization on continuous values to find corresponding discrete value</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    out, ldj <span class="op">=</span> dequant_module.forward(inp, ldj, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    inp, out, prob <span class="op">=</span> inp.squeeze().numpy(), out.squeeze().numpy(), ldj.exp().numpy()</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> prob <span class="op">*</span> prior[out] <span class="co"># Probability scaled by categorical prior</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot volumes and continuous distribution</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">3</span>))</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    x_ticks <span class="op">=</span> []</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> np.unique(out):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.where(out<span class="op">==</span>v)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> to_rgb(<span class="ss">f"C</span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        plt.fill_between(inp[indices], prob[indices], np.zeros(indices[<span class="dv">0</span>].shape[<span class="dv">0</span>]), color<span class="op">=</span>color<span class="op">+</span>(<span class="fl">0.5</span>,), label<span class="op">=</span><span class="bu">str</span>(v))</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        plt.plot([inp[indices[<span class="dv">0</span>][<span class="dv">0</span>]]]<span class="op">*</span><span class="dv">2</span>,  [<span class="dv">0</span>, prob[indices[<span class="dv">0</span>][<span class="dv">0</span>]]],  color<span class="op">=</span>color)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        plt.plot([inp[indices[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]]]<span class="op">*</span><span class="dv">2</span>, [<span class="dv">0</span>, prob[indices[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]]], color<span class="op">=</span>color)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        x_ticks.append(inp[indices[<span class="dv">0</span>][<span class="dv">0</span>]])</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    x_ticks.append(inp.<span class="bu">max</span>())</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    plt.xticks(x_ticks, [<span class="ss">f"</span><span class="sc">{</span>x<span class="sc">:.1f}</span><span class="ss">"</span> <span class="cf">for</span> x <span class="kw">in</span> x_ticks])</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    plt.plot(inp,prob, color<span class="op">=</span>(<span class="fl">0.0</span>,<span class="fl">0.0</span>,<span class="fl">0.0</span>))</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set final plot properties</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="dv">0</span>, prob.<span class="bu">max</span>()<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    plt.xlim(inp.<span class="bu">min</span>(), inp.<span class="bu">max</span>())</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Dequantization distribution for </span><span class="sc">{</span>quants<span class="sc">}</span><span class="ss"> discrete values"</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>visualize_dequantization(quants<span class="op">=</span><span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-8-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The visualized distribution show the sub-volumes that are assigned to the different discrete values. The value <span class="math inline">\(0\)</span> has its volume between <span class="math inline">\([-\infty, -1.9)\)</span>, the value <span class="math inline">\(1\)</span> is represented by the interval <span class="math inline">\([-1.9, -1.1)\)</span>, etc. The volume for each discrete value has the same probability mass. That’s why the volumes close to the center (e.g.&nbsp;3 and 4) have a smaller area on the z-axis as others (<span class="math inline">\(z\)</span> is being used to denote the output of the whole dequantization flow).</p>
<p>Effectively, the consecutive normalizing flow models discrete images by the following objective:</p>
<p><span class="math display">\[\log p(x) = \log \mathbb{E}_{u\sim q(u|x)}\left[\frac{p(x+u)}{q(u|x)} \right] \geq \mathbb{E}_{u}\left[\log \frac{p(x+u)}{q(u|x)} \right]\]</span></p>
<p>Although normalizing flows are exact in likelihood, we have a lower bound. Specifically, this is an example of the Jensen inequality because we need to move the log into the expectation so we can use Monte-carlo estimates. In general, this bound is considerably smaller than the ELBO in variational autoencoders. Actually, we can reduce the bound ourselves by estimating the expectation not by one, but by <span class="math inline">\(M\)</span> samples. In other words, we can apply importance sampling which leads to the following inequality:</p>
<p><span class="math display">\[\log p(x) = \log \mathbb{E}_{u\sim q(u|x)}\left[\frac{p(x+u)}{q(u|x)} \right] \geq \mathbb{E}_{u}\left[\log \frac{1}{M} \sum_{m=1}^{M} \frac{p(x+u_m)}{q(u_m|x)} \right] \geq \mathbb{E}_{u}\left[\log \frac{p(x+u)}{q(u|x)} \right]\]</span></p>
<p>The importance sampling <span class="math inline">\(\frac{1}{M} \sum_{m=1}^{M} \frac{p(x+u_m)}{q(u_m|x)}\)</span> becomes <span class="math inline">\(\mathbb{E}_{u\sim q(u|x)}\left[\frac{p(x+u)}{q(u|x)} \right]\)</span> if <span class="math inline">\(M\to \infty\)</span>, so that the more samples we use, the tighter the bound is. During testing, we can make use of this property and have it implemented in <code>test_step</code> in <code>ImageFlow</code>. In theory, we could also use this tighter bound during training. However, related work has shown that this does not necessarily lead to an improvement given the additional computational cost, and it is more efficient to stick with a single estimate [5].</p>
</section>
<section id="variational-dequantization" class="level3">
<h3 class="anchored" data-anchor-id="variational-dequantization">Variational Dequantization</h3>
<p>Dequantization uses a uniform distribution for the noise <span class="math inline">\(u\)</span> which effectively leads to images being represented as hypercubes (cube in high dimensions) with sharp borders. However, modeling such sharp borders is not easy for a flow as it uses smooth transformations to convert it into a Gaussian distribution.</p>
<p>Another way of looking at it is if we change the prior distribution in the previous visualization. Imagine we have independent Gaussian noise on pixels which is commonly the case for any real-world taken picture. Therefore, the flow would have to model a distribution as above, but with the individual volumes scaled as follows:</p>
<div id="cell-18" class="cell" data-outputid="dd44dde5-34ae-4000-94bf-54bf68ed6cf8" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>visualize_dequantization(quants<span class="op">=</span><span class="dv">8</span>, prior<span class="op">=</span>np.array([<span class="fl">0.075</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="fl">0.075</span>, <span class="fl">0.025</span>, <span class="fl">0.0125</span>, <span class="fl">0.0125</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-9-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Transforming such a probability into a Gaussian is a difficult task, especially with such hard borders. Dequantization has therefore been extended to more sophisticated, learnable distributions beyond uniform in a variational framework. In particular, if we remember the learning objective <span class="math inline">\(\log p(x) = \log \mathbb{E}_{u}\left[\frac{p(x+u)}{q(u|x)} \right]\)</span>, the uniform distribution can be replaced by a learned distribution <span class="math inline">\(q_{\theta}(u|x)\)</span> with support over <span class="math inline">\(u\in[0,1)^D\)</span>. This approach is called Variational Dequantization and has been proposed by Ho et al.&nbsp;[3]. How can we learn such a distribution? We can use a second normalizing flow that takes <span class="math inline">\(x\)</span> as external input and learns a flexible distribution over <span class="math inline">\(u\)</span>. To ensure a support over <span class="math inline">\([0,1)^D\)</span>, we can apply a sigmoid activation function as final flow transformation.</p>
<p>Inheriting the original dequantization class, we can implement variational dequantization as follows:</p>
<div id="cell-20" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VariationalDequantization(Dequantization):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, var_flows, alpha<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">            var_flows - A list of flow transformations to use for modeling q(u|x)</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">            alpha - Small constant, see Dequantization for details</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(alpha<span class="op">=</span>alpha)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flows <span class="op">=</span> nn.ModuleList(var_flows)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> dequant(<span class="va">self</span>, z, ldj):</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> z.to(torch.float32)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> (z <span class="op">/</span> <span class="fl">255.0</span>) <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span> <span class="co"># We condition the flows on x, i.e. the original image</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prior of u is a uniform distribution as before</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># As most flow transformations are defined on [-infinity,+infinity], we apply an inverse sigmoid first.</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        deq_noise <span class="op">=</span> torch.rand_like(z).detach()</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        deq_noise, ldj <span class="op">=</span> <span class="va">self</span>.sigmoid(deq_noise, ldj, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> flow <span class="kw">in</span> <span class="va">self</span>.flows:</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>            deq_noise, ldj <span class="op">=</span> flow(deq_noise, ldj, reverse<span class="op">=</span><span class="va">False</span>, orig_img<span class="op">=</span>img)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        deq_noise, ldj <span class="op">=</span> <span class="va">self</span>.sigmoid(deq_noise, ldj, reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After the flows, apply u as in standard dequantization</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> (z <span class="op">+</span> deq_noise) <span class="op">/</span> <span class="fl">256.0</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        ldj <span class="op">-=</span> np.log(<span class="fl">256.0</span>) <span class="op">*</span> np.prod(z.shape[<span class="dv">1</span>:])</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Variational dequantization can be used as a substitute for dequantization. We will compare dequantization and variational dequantization in later experiments.</p>
</section>
<section id="coupling-layers" class="level3">
<h3 class="anchored" data-anchor-id="coupling-layers">Coupling layers</h3>
<p>Next, we look at possible transformations to apply inside the flow. A recent popular flow layer, which works well in combination with deep neural networks, is the coupling layer introduced by Dinh et al.&nbsp;[1]. The input <span class="math inline">\(z\)</span> is arbitrarily split into two parts, <span class="math inline">\(z_{1:j}\)</span> and <span class="math inline">\(z_{j+1:d}\)</span>, of which the first remains unchanged by the flow. Yet, <span class="math inline">\(z_{1:j}\)</span> is used to parameterize the transformation for the second part, <span class="math inline">\(z_{j+1:d}\)</span>. Various transformations have been proposed in recent time [3,4], but here we will settle for the simplest and most efficient one: affine coupling. In this coupling layer, we apply an affine transformation by shifting the input by a bias <span class="math inline">\(\mu\)</span> and scale it by <span class="math inline">\(\sigma\)</span>. In other words, our transformation looks as follows:</p>
<p><span class="math display">\[z'_{j+1:d} = \mu_{\theta}(z_{1:j}) + \sigma_{\theta}(z_{1:j}) \odot z_{j+1:d}\]</span></p>
<p>The functions <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are implemented as a shared neural network, and the sum and multiplication are performed element-wise. The LDJ is thereby the sum of the logs of the scaling factors: <span class="math inline">\(\sum_i \left[\log \sigma_{\theta}(z_{1:j})\right]_i\)</span>. Inverting the layer can as simply be done as subtracting the bias and dividing by the scale:</p>
<p><span class="math display">\[z_{j+1:d} = \left(z'_{j+1:d} - \mu_{\theta}(z_{1:j})\right) / \sigma_{\theta}(z_{1:j})\]</span></p>
<p>We can also visualize the coupling layer in form of a computation graph, where <span class="math inline">\(z_1\)</span> represents <span class="math inline">\(z_{1:j}\)</span>, and <span class="math inline">\(z_2\)</span> represents <span class="math inline">\(z_{j+1:d}\)</span>:</p>
<center width="100%" style="padding: 10px">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/coupling_flow.svg?raw=1" width="450px">
</center>
<p>In our implementation, we will realize the splitting of variables as masking. The variables to be transformed, <span class="math inline">\(z_{j+1:d}\)</span>, are masked when passing <span class="math inline">\(z\)</span> to the shared network to predict the transformation parameters. When applying the transformation, we mask the parameters for <span class="math inline">\(z_{1:j}\)</span> so that we have an identity operation for those variables:</p>
<div id="cell-23" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CouplingLayer(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, network, mask, c_in):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Coupling layer inside a normalizing flow.</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">            network - A PyTorch nn.Module constituting the deep neural network for mu and sigma.</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">                      Output shape should be twice the channel size as the input.</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">            mask - Binary mask (0 or 1) where 0 denotes that the element should be transformed,</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">                   while 1 means the latent will be used as input to the NN.</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">            c_in - Number of input channels</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> network</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scaling_factor <span class="op">=</span> nn.Parameter(torch.zeros(c_in))</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register mask as buffer as it is a tensor which is not a parameter,</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but should be part of the modules state.</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'mask'</span>, mask)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z, ldj, reverse<span class="op">=</span><span class="va">False</span>, orig_img<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">            z - Latent input to the flow</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">            ldj - The current ldj of the previous flows.</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">                  The ldj of this layer will be added to this tensor.</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co">            reverse - If True, we apply the inverse of the layer.</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">            orig_img (optional) - Only needed in VarDeq. Allows external</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">                                  input to condition the flow on (e.g. original image)</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply network to masked input</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        z_in <span class="op">=</span> z <span class="op">*</span> <span class="va">self</span>.mask</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> orig_img <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>            nn_out <span class="op">=</span> <span class="va">self</span>.network(z_in)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>            nn_out <span class="op">=</span> <span class="va">self</span>.network(torch.cat([z_in, orig_img], dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        s, t <span class="op">=</span> nn_out.chunk(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stabilize scaling output</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        s_fac <span class="op">=</span> <span class="va">self</span>.scaling_factor.exp().view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> torch.tanh(s <span class="op">/</span> s_fac) <span class="op">*</span> s_fac</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mask outputs (only transform the second part)</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> s <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> t <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask)</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Affine transformation</span></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Whether we first shift and then scale, or the other way round,</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>            <span class="co"># is a design choice, and usually does not have a big impact</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> (z <span class="op">+</span> t) <span class="op">*</span> torch.exp(s)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">+=</span> s.<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> (z <span class="op">*</span> torch.exp(<span class="op">-</span>s)) <span class="op">-</span> t</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">-=</span> s.<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For stabilization purposes, we apply a <span class="math inline">\(\tanh\)</span> activation function on the scaling output. This prevents sudden large output values for the scaling that can destabilize training. To still allow scaling factors smaller or larger than -1 and 1 respectively, we have a learnable parameter per dimension, called <code>scaling_factor</code>. This scales the tanh to different limits. Below, we visualize the effect of the scaling factor on the output activation of the scaling terms:</p>
<div id="cell-25" class="cell" data-outputid="ae73f2b4-ad52-4669-d20d-8b46af08b050" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="fl">0.01</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    scaling_factors <span class="op">=</span> [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    sns.<span class="bu">set</span>()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">3</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, scale <span class="kw">in</span> <span class="bu">enumerate</span>(scaling_factors):</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.tanh(x <span class="op">/</span> scale) <span class="op">*</span> scale</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        ax[i].plot(x.numpy(), y.numpy())</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        ax[i].set_title(<span class="st">"Scaling factor: "</span> <span class="op">+</span> <span class="bu">str</span>(scale))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        ax[i].set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    sns.reset_orig()</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-12-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Coupling layers generalize to any masking technique we could think of. However, the most common approach for images is to split the input <span class="math inline">\(z\)</span> in half, using a checkerboard mask or channel mask. A checkerboard mask splits the variables across the height and width dimensions and assigns each other pixel to <span class="math inline">\(z_{j+1:d}\)</span>. Thereby, the mask is shared across channels. In contrast, the channel mask assigns half of the channels to <span class="math inline">\(z_{j+1:d}\)</span>, and the other half to <span class="math inline">\(z_{1:j+1}\)</span>. Note that when we apply multiple coupling layers, we invert the masking for each other layer so that each variable is transformed a similar amount of times.</p>
<p>Let’s implement a function that creates a checkerboard mask and a channel mask for us:</p>
<div id="cell-27" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_checkerboard_mask(h, w, invert<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> torch.arange(h, dtype<span class="op">=</span>torch.int32), torch.arange(w, dtype<span class="op">=</span>torch.int32)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> torch.meshgrid(x, y, indexing<span class="op">=</span><span class="st">'ij'</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.fmod(xx <span class="op">+</span> yy, <span class="dv">2</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.to(torch.float32).view(<span class="dv">1</span>, <span class="dv">1</span>, h, w)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> invert:</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> mask</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_channel_mask(c_in, invert<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.cat([torch.ones(c_in<span class="op">//</span><span class="dv">2</span>, dtype<span class="op">=</span>torch.float32),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>                      torch.zeros(c_in<span class="op">-</span>c_in<span class="op">//</span><span class="dv">2</span>, dtype<span class="op">=</span>torch.float32)])</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.view(<span class="dv">1</span>, c_in, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> invert:</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> mask</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can also visualize the corresponding masks for an image of size <span class="math inline">\(8\times 8\times 2\)</span> (2 channels):</p>
<div id="cell-29" class="cell" data-outputid="2c698223-3093-469e-9c54-8909f76b46da" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>checkerboard_mask <span class="op">=</span> create_checkerboard_mask(h<span class="op">=</span><span class="dv">8</span>, w<span class="op">=</span><span class="dv">8</span>).expand(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>channel_mask <span class="op">=</span> create_channel_mask(c_in<span class="op">=</span><span class="dv">2</span>).expand(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">8</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>show_imgs(checkerboard_mask.transpose(<span class="dv">0</span>,<span class="dv">1</span>), <span class="st">"Checkerboard mask"</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>show_imgs(channel_mask.transpose(<span class="dv">0</span>,<span class="dv">1</span>), <span class="st">"Channel mask"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-14-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-14-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As a last aspect of coupling layers, we need to decide for the deep neural network we want to apply in the coupling layers. The input to the layers is an image, and hence we stick with a CNN. Because the input to a transformation depends on all transformations before, it is crucial to ensure a good gradient flow through the CNN back to the input, which can be optimally achieved by a ResNet-like architecture. Specifically, we use a Gated ResNet that adds a <span class="math inline">\(\sigma\)</span>-gate to the skip connection, similarly to the input gate in LSTMs. The details are not necessarily important here, and the network is strongly inspired from Flow++ [3] in case you are interested in building even stronger models.</p>
<div id="cell-31" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConcatELU(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Activation function that applies ELU in both direction (inverted and plain).</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Allows non-linearity while providing strong gradients for any input (important for final convolution)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([F.elu(x), F.elu(<span class="op">-</span>x)], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNormChannels(nn.Module):</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c_in, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="co">        This module applies layer norm across channels in an image.</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co">            c_in - Number of channels of the input</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co">            eps - Small constant to stabilize std</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>, c_in, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, c_in, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        var <span class="op">=</span> x.var(dim<span class="op">=</span><span class="dv">1</span>, unbiased<span class="op">=</span><span class="va">False</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> torch.sqrt(var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y <span class="op">*</span> <span class="va">self</span>.gamma <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedConv(nn.Module):</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c_in, c_hidden):</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="co">        This module applies a two-layer convolutional ResNet block with input gate</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="co">            c_in - Number of channels of the input</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="co">            c_hidden - Number of hidden dimensions we want to model (usually similar to c_in)</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>            ConcatELU(),</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">2</span><span class="op">*</span>c_in, c_hidden, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>            ConcatELU(),</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">2</span><span class="op">*</span>c_hidden, <span class="dv">2</span><span class="op">*</span>c_in, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.net(x)</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        val, gate <span class="op">=</span> out.chunk(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> val <span class="op">*</span> torch.sigmoid(gate)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedConvNet(nn.Module):</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c_in, c_hidden<span class="op">=</span><span class="dv">32</span>, c_out<span class="op">=-</span><span class="dv">1</span>, num_layers<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="co">        Module that summarizes the previous blocks to a full convolutional neural network.</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a><span class="co">            c_in - Number of input channels</span></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a><span class="co">            c_hidden - Number of hidden dimensions to use within the network</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a><span class="co">            c_out - Number of output channels. If -1, 2 times the input channels are used (affine coupling)</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a><span class="co">            num_layers - Number of gated ResNet blocks to apply</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>        c_out <span class="op">=</span> c_out <span class="cf">if</span> c_out <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">2</span> <span class="op">*</span> c_in</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">+=</span> [nn.Conv2d(c_in, c_hidden, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)]</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer_index <span class="kw">in</span> <span class="bu">range</span>(num_layers):</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>            layers <span class="op">+=</span> [GatedConv(c_hidden, c_hidden),</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>                       LayerNormChannels(c_hidden)]</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">+=</span> [ConcatELU(),</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>                   nn.Conv2d(<span class="dv">2</span><span class="op">*</span>c_hidden, c_out, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)]</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nn <span class="op">=</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nn[<span class="op">-</span><span class="dv">1</span>].weight.data.zero_()</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nn[<span class="op">-</span><span class="dv">1</span>].bias.data.zero_()</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.nn(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-loop" class="level3">
<h3 class="anchored" data-anchor-id="training-loop">Training loop</h3>
<p>Finally, we can add Dequantization, Variational Dequantization and Coupling Layers together to build our full normalizing flow on MNIST images. We apply 8 coupling layers in the main flow, and 4 for variational dequantization if applied. We apply a checkerboard mask throughout the network as with a single channel (black-white images), we cannot apply channel mask. The overall architecture is visualized below.</p>
<center width="100%" style="padding: 20px">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/vanilla_flow.svg?raw=1" width="900px">
</center>
<div id="cell-33" class="cell" data-scrolled="true" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_simple_flow(use_vardeq<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    flow_layers <span class="op">=</span> []</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_vardeq:</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        vardeq_layers <span class="op">=</span> [CouplingLayer(network<span class="op">=</span>GatedConvNet(c_in<span class="op">=</span><span class="dv">2</span>, c_out<span class="op">=</span><span class="dv">2</span>, c_hidden<span class="op">=</span><span class="dv">16</span>),</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                                       mask<span class="op">=</span>create_checkerboard_mask(h<span class="op">=</span><span class="dv">28</span>, w<span class="op">=</span><span class="dv">28</span>, invert<span class="op">=</span>(i<span class="op">%</span><span class="dv">2</span><span class="op">==</span><span class="dv">1</span>)),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                                       c_in<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        flow_layers <span class="op">+=</span> [VariationalDequantization(var_flows<span class="op">=</span>vardeq_layers)]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        flow_layers <span class="op">+=</span> [Dequantization()]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        flow_layers <span class="op">+=</span> [CouplingLayer(network<span class="op">=</span>GatedConvNet(c_in<span class="op">=</span><span class="dv">1</span>, c_hidden<span class="op">=</span><span class="dv">32</span>),</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>                                      mask<span class="op">=</span>create_checkerboard_mask(h<span class="op">=</span><span class="dv">28</span>, w<span class="op">=</span><span class="dv">28</span>, invert<span class="op">=</span>(i<span class="op">%</span><span class="dv">2</span><span class="op">==</span><span class="dv">1</span>)),</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>                                      c_in<span class="op">=</span><span class="dv">1</span>)]</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    flow_model <span class="op">=</span> ImageFlow(flow_layers).to(device)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> flow_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For implementing the training loop, we use the framework of PyTorch Lightning and reduce the code overhead. If interested, you can take a look at the generated tensorboard file, in particularly the graph to see an overview of flow transformations that are applied. Note that we again provide pre-trained models (see later on in the notebook) as normalizing flows are particularly expensive to train. We have also run validation and testing as this can take some time as well with the added importance sampling.</p>
<div id="cell-35" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_flow(flow, model_name<span class="op">=</span><span class="st">"MNISTFlow"</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a PyTorch Lightning trainer</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    trainer <span class="op">=</span> pl.Trainer(default_root_dir<span class="op">=</span>os.path.join(CHECKPOINT_PATH, model_name),</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                         accelerator<span class="op">=</span><span class="st">"gpu"</span> <span class="cf">if</span> <span class="bu">str</span>(device).startswith(<span class="st">"cuda"</span>) <span class="cf">else</span> <span class="st">"cpu"</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                         devices<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                         max_epochs<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                         gradient_clip_val<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                         callbacks<span class="op">=</span>[ModelCheckpoint(save_weights_only<span class="op">=</span><span class="va">True</span>, mode<span class="op">=</span><span class="st">"min"</span>, monitor<span class="op">=</span><span class="st">"val_bpd"</span>),</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>                                    LearningRateMonitor(<span class="st">"epoch"</span>)],</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                         check_val_every_n_epoch<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    trainer.logger._log_graph <span class="op">=</span> <span class="va">True</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    trainer.logger._default_hp_metric <span class="op">=</span> <span class="va">None</span> <span class="co"># Optional logging argument that we don't need</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    train_data_loader <span class="op">=</span> data.DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">128</span>, shuffle<span class="op">=</span><span class="va">True</span>, drop_last<span class="op">=</span><span class="va">True</span>, pin_memory<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> <span class="va">None</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check whether pretrained model exists. If yes, load it and skip training</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    pretrained_filename <span class="op">=</span> os.path.join(CHECKPOINT_PATH, model_name <span class="op">+</span> <span class="st">".ckpt"</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.isfile(pretrained_filename):</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Found pretrained model, loading..."</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        ckpt <span class="op">=</span> torch.load(pretrained_filename, map_location<span class="op">=</span>device)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        flow.load_state_dict(ckpt[<span class="st">'state_dict'</span>])</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> ckpt.get(<span class="st">"result"</span>, <span class="va">None</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Start training"</span>, model_name)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        trainer.fit(flow, train_data_loader, val_loader)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test best model on validation and test set if no result has been found</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Testing can be expensive due to the importance sampling.</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> result <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        val_result <span class="op">=</span> trainer.test(flow, val_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        start_time <span class="op">=</span> time.time()</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>        test_result <span class="op">=</span> trainer.test(flow, test_loader, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        duration <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> {<span class="st">"test"</span>: test_result, <span class="st">"val"</span>: val_result, <span class="st">"time"</span>: duration <span class="op">/</span> <span class="bu">len</span>(test_loader) <span class="op">/</span> flow.import_samples}</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> flow, result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="multi-scale-architecture" class="level2">
<h2 class="anchored" data-anchor-id="multi-scale-architecture">Multi-scale architecture</h2>
<p>One disadvantage of normalizing flows is that they operate on the exact same dimensions as the input. If the input is high-dimensional, so is the latent space, which requires larger computational cost to learn suitable transformations. However, particularly in the image domain, many pixels contain less information in the sense that we could remove them without loosing the semantical information of the image.</p>
<p>Based on this intuition, deep normalizing flows on images commonly apply a multi-scale architecture [1]. After the first <span class="math inline">\(N\)</span> flow transformations, we split off half of the latent dimensions and directly evaluate them on the prior. The other half is run through <span class="math inline">\(N\)</span> more flow transformations, and depending on the size of the input, we split it again in half or stop overall at this position. The two operations involved in this setup is <code>Squeeze</code> and <code>Split</code> which we will review more closely and implement below.</p>
<section id="squeeze-and-split" class="level3">
<h3 class="anchored" data-anchor-id="squeeze-and-split">Squeeze and Split</h3>
<p>When we want to remove half of the pixels in an image, we have the problem of deciding which variables to cut, and how to rearrange the image. Thus, the squeezing operation is commonly used before split, which divides the image into subsquares of shape <span class="math inline">\(2\times 2\times C\)</span>, and reshapes them into <span class="math inline">\(1\times 1\times 4C\)</span> blocks. Effectively, we reduce the height and width of the image by a factor of 2 while scaling the number of channels by 4. Afterwards, we can perform the split operation over channels without the need of rearranging the pixels. The smaller scale also makes the overall architecture more efficient. Visually, the squeeze operation should transform the input as follows:</p>
<center>
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/Squeeze_operation.svg?raw=1" width="40%">
</center>
<p>The input of <span class="math inline">\(4\times 4\times 1\)</span> is scaled to <span class="math inline">\(2\times 2\times 4\)</span> following the idea of grouping the pixels in <span class="math inline">\(2\times 2\times 1\)</span> subsquares. Next, let’s try to implement this layer:</p>
<div id="cell-38" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SqueezeFlow(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z, ldj, reverse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        B, C, H, W <span class="op">=</span> z.shape</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward direction: H x W x C =&gt; H/2 x W/2 x 4C</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z.reshape(B, C, H<span class="op">//</span><span class="dv">2</span>, <span class="dv">2</span>, W<span class="op">//</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z.permute(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z.reshape(B, <span class="dv">4</span><span class="op">*</span>C, H<span class="op">//</span><span class="dv">2</span>, W<span class="op">//</span><span class="dv">2</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reverse direction: H/2 x W/2 x 4C =&gt; H x W x C</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z.reshape(B, C<span class="op">//</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>, H, W)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z.permute(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> z.reshape(B, C<span class="op">//</span><span class="dv">4</span>, H<span class="op">*</span><span class="dv">2</span>, W<span class="op">*</span><span class="dv">2</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before moving on, we can verify our implementation by comparing our output with the example figure above:</p>
<div id="cell-40" class="cell" data-outputid="d09913d5-c170-4bf2-a7b3-dbc3ba8c6cd4" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>sq_flow <span class="op">=</span> SqueezeFlow()</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>rand_img <span class="op">=</span> torch.arange(<span class="dv">1</span>,<span class="dv">17</span>).view(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image (before)</span><span class="ch">\n</span><span class="st">"</span>, rand_img)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>forward_img, _ <span class="op">=</span> sq_flow(rand_img, ldj<span class="op">=</span><span class="va">None</span>, reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Image (forward)</span><span class="ch">\n</span><span class="st">"</span>, forward_img.permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>)) <span class="co"># Permute for readability</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>reconst_img, _ <span class="op">=</span> sq_flow(forward_img, ldj<span class="op">=</span><span class="va">None</span>, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Image (reverse)</span><span class="ch">\n</span><span class="st">"</span>, reconst_img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image (before)
 tensor([[[[ 1,  2,  3,  4],
          [ 5,  6,  7,  8],
          [ 9, 10, 11, 12],
          [13, 14, 15, 16]]]])

Image (forward)
 tensor([[[[ 1,  2,  5,  6],
          [ 3,  4,  7,  8]],

         [[ 9, 10, 13, 14],
          [11, 12, 15, 16]]]])

Image (reverse)
 tensor([[[[ 1,  2,  3,  4],
          [ 5,  6,  7,  8],
          [ 9, 10, 11, 12],
          [13, 14, 15, 16]]]])</code></pre>
</div>
</div>
<p>The split operation divides the input into two parts, and evaluates one part directly on the prior. So that our flow operation fits to the implementation of the previous layers, we will return the prior probability of the first part as the log determinant jacobian of the layer. It has the same effect as if we would combine all variable splits at the end of the flow, and evaluate them together on the prior.</p>
<div id="cell-42" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SplitFlow(nn.Module):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior <span class="op">=</span> torch.distributions.normal.Normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z, ldj, reverse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>            z, z_split <span class="op">=</span> z.chunk(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">+=</span> <span class="va">self</span>.prior.log_prob(z_split).<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>            z_split <span class="op">=</span> <span class="va">self</span>.prior.sample(sample_shape<span class="op">=</span>z.shape).to(device)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.cat([z, z_split], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>            ldj <span class="op">-=</span> <span class="va">self</span>.prior.log_prob(z_split).<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, ldj</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="building-a-multi-scale-flow" class="level3">
<h3 class="anchored" data-anchor-id="building-a-multi-scale-flow">Building a multi-scale flow</h3>
<p>After defining the squeeze and split operation, we are finally able to build our own multi-scale flow. Deep normalizing flows such as Glow and Flow++ [2,3] often apply a split operation directly after squeezing. However, with shallow flows, we need to be more thoughtful about where to place the split operation as we need at least a minimum amount of transformations on each variable. Our setup is inspired by the original RealNVP architecture [1] which is shallower than other, more recent state-of-the-art architectures.</p>
<p>Hence, for the MNIST dataset, we will apply the first squeeze operation after two coupling layers, but don’t apply a split operation yet. Because we have only used two coupling layers and each the variable has been only transformed once, a split operation would be too early. We apply two more coupling layers before finally applying a split flow and squeeze again. The last four coupling layers operate on a scale of <span class="math inline">\(7\times 7\times 8\)</span>. The full flow architecture is shown below.</p>
<center width="100%" style="padding: 20px">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/multiscale_flow.svg?raw=1" width="1100px">
</center>
<p>Note that while the feature maps inside the coupling layers reduce with the height and width of the input, the increased number of channels is not directly considered. To counteract this, we increase the hidden dimensions for the coupling layers on the squeezed input. The dimensions are often scaled by 2 as this approximately increases the computation cost by 4 canceling with the squeezing operation. However, we will choose the hidden dimensionalities <span class="math inline">\(32, 48, 64\)</span> for the three scales respectively to keep the number of parameters reasonable and show the efficiency of multi-scale architectures.</p>
<div id="cell-44" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_multiscale_flow():</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    flow_layers <span class="op">=</span> []</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    vardeq_layers <span class="op">=</span> [CouplingLayer(network<span class="op">=</span>GatedConvNet(c_in<span class="op">=</span><span class="dv">2</span>, c_out<span class="op">=</span><span class="dv">2</span>, c_hidden<span class="op">=</span><span class="dv">16</span>),</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>                                   mask<span class="op">=</span>create_checkerboard_mask(h<span class="op">=</span><span class="dv">28</span>, w<span class="op">=</span><span class="dv">28</span>, invert<span class="op">=</span>(i<span class="op">%</span><span class="dv">2</span><span class="op">==</span><span class="dv">1</span>)),</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>                                   c_in<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    flow_layers <span class="op">+=</span> [VariationalDequantization(vardeq_layers)]</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    flow_layers <span class="op">+=</span> [CouplingLayer(network<span class="op">=</span>GatedConvNet(c_in<span class="op">=</span><span class="dv">1</span>, c_hidden<span class="op">=</span><span class="dv">32</span>),</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>                                  mask<span class="op">=</span>create_checkerboard_mask(h<span class="op">=</span><span class="dv">28</span>, w<span class="op">=</span><span class="dv">28</span>, invert<span class="op">=</span>(i<span class="op">%</span><span class="dv">2</span><span class="op">==</span><span class="dv">1</span>)),</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>                                  c_in<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    flow_layers <span class="op">+=</span> [SqueezeFlow()]</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        flow_layers <span class="op">+=</span> [CouplingLayer(network<span class="op">=</span>GatedConvNet(c_in<span class="op">=</span><span class="dv">4</span>, c_hidden<span class="op">=</span><span class="dv">48</span>),</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>                                      mask<span class="op">=</span>create_channel_mask(c_in<span class="op">=</span><span class="dv">4</span>, invert<span class="op">=</span>(i<span class="op">%</span><span class="dv">2</span><span class="op">==</span><span class="dv">1</span>)),</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>                                      c_in<span class="op">=</span><span class="dv">4</span>)]</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    flow_layers <span class="op">+=</span> [SplitFlow(),</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>                    SqueezeFlow()]</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Insert your code here!!!</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Complete the last scale ccording to provided description(HINT: c_in should be 8)</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#-------------------------</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    flow_model <span class="op">=</span> ImageFlow(flow_layers).to(device)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> flow_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can show the difference in number of parameters below:</p>
<div id="cell-46" class="cell" data-outputid="aeeaa520-7aad-42dd-c2b6-fb52328d2052" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_num_params(model):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    num_params <span class="op">=</span> <span class="bu">sum</span>([np.prod(p.shape) <span class="cf">for</span> p <span class="kw">in</span> model.parameters()])</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Number of parameters: </span><span class="sc">{:,}</span><span class="st">"</span>.<span class="bu">format</span>(num_params))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>print_num_params(create_simple_flow(use_vardeq<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>print_num_params(create_simple_flow(use_vardeq<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>print_num_params(create_multiscale_flow())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of parameters: 556,312
Number of parameters: 628,388
Number of parameters: 534,122</code></pre>
</div>
</div>
<p>Although the multi-scale flow has almost 3 times the parameters of the single scale flow, it is not necessarily more computationally expensive than its counterpart. We will compare the runtime in the following experiments as well.</p>
</section>
</section>
<section id="analysing-the-flows" class="level2">
<h2 class="anchored" data-anchor-id="analysing-the-flows">Analysing the flows</h2>
<p>In the last part of the notebook, we will train all the models we have implemented above, and try to analyze the effect of the multi-scale architecture and variational dequantization.</p>
<section id="training-flow-variants" class="level3">
<h3 class="anchored" data-anchor-id="training-flow-variants">Training flow variants</h3>
<p>Before we can analyse the flow models, we need to train them first. We provide pre-trained models that contain the validation and test performance, and run-time information. As flow models are computationally expensive, we advice you to rely on those pretrained models for a first run through the notebook.</p>
<div id="cell-49" class="cell" data-scrolled="false" data-outputid="c43eef1e-a195-49e7-cda7-e0354ef501b3" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>flow_dict <span class="op">=</span> {<span class="st">"simple"</span>: {}, <span class="st">"vardeq"</span>: {}, <span class="st">"multiscale"</span>: {}}</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>flow_dict[<span class="st">"simple"</span>][<span class="st">"model"</span>], flow_dict[<span class="st">"simple"</span>][<span class="st">"result"</span>] <span class="op">=</span> train_flow(create_simple_flow(use_vardeq<span class="op">=</span><span class="va">False</span>), model_name<span class="op">=</span><span class="st">"MNISTFlow_simple"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>flow_dict[<span class="st">"vardeq"</span>][<span class="st">"model"</span>], flow_dict[<span class="st">"vardeq"</span>][<span class="st">"result"</span>] <span class="op">=</span> train_flow(create_simple_flow(use_vardeq<span class="op">=</span><span class="va">True</span>), model_name<span class="op">=</span><span class="st">"MNISTFlow_vardeq"</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>flow_dict[<span class="st">"multiscale"</span>][<span class="st">"model"</span>], flow_dict[<span class="st">"multiscale"</span>][<span class="st">"result"</span>] <span class="op">=</span> train_flow(create_multiscale_flow(), model_name<span class="op">=</span><span class="st">"MNISTFlow_multiscale"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tutorial11/MNISTFlow_simple/lightning_logs</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Start training MNISTFlow_simple</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>NameError: name 'nll' is not defined</code></pre>
</div>
</div>
<div id="cell-50" class="cell" data-outputid="574a98e0-dd44-44b4-a54e-373eaa77b41b">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>flow_dict[<span class="st">"simple"</span>][<span class="st">"result"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>{'test': [{'test_bpd': 1.07839834690094}],
 'val': [{'test_bpd': 1.0798484086990356}],
 'time': 0.019570968523147,
 'samp_time': 0.017716965675354003}</code></pre>
</div>
</div>
</section>
<section id="density-modeling-and-sampling" class="level3">
<h3 class="anchored" data-anchor-id="density-modeling-and-sampling">Density modeling and sampling</h3>
<p>Firstly, we can compare the models on their quantitative results. The following table shows all important statistics. The inference time specifies the time needed to determine the probability for a batch of 64 images for each model, and the sampling time the duration it took to sample a batch of 64 images.</p>
<div id="cell-52" class="cell" data-scrolled="true" data-outputid="4f6abba0-f436-4da9-a387-8e4ec144bd97">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>html</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;!--</span> Some HTML code to increase font size <span class="kw">in</span> the following table <span class="op">--&gt;</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;</span>style<span class="op">&gt;</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>th {font<span class="op">-</span>size: <span class="dv">120</span><span class="op">%;</span>}</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>td {font<span class="op">-</span>size: <span class="dv">120</span><span class="op">%;</span>}</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="op">&lt;/</span>style<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<!-- Some HTML code to increase font size in the following table -->
<style>
th {font-size: 120%;}
td {font-size: 120%;}
</style>
</div>
</div>
<div id="cell-53" class="cell" data-scrolled="true" data-outputid="409b5ee8-816a-4fde-cbf3-3f82c49994e6">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tabulate</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, HTML</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> [[key,</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>          <span class="st">"</span><span class="sc">%4.3f</span><span class="st"> bpd"</span> <span class="op">%</span> flow_dict[key][<span class="st">"result"</span>][<span class="st">"val"</span>][<span class="dv">0</span>][<span class="st">"test_bpd"</span>],</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>          <span class="st">"</span><span class="sc">%4.3f</span><span class="st"> bpd"</span> <span class="op">%</span> flow_dict[key][<span class="st">"result"</span>][<span class="st">"test"</span>][<span class="dv">0</span>][<span class="st">"test_bpd"</span>],</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>          <span class="st">"%2.0f ms"</span> <span class="op">%</span> (<span class="dv">1000</span> <span class="op">*</span> flow_dict[key][<span class="st">"result"</span>][<span class="st">"time"</span>]),</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>          <span class="st">"%2.0f ms"</span> <span class="op">%</span> (<span class="dv">1000</span> <span class="op">*</span> flow_dict[key][<span class="st">"result"</span>].get(<span class="st">"samp_time"</span>, <span class="dv">0</span>)),</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>          <span class="st">"</span><span class="sc">{:,}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="bu">sum</span>([np.prod(p.shape) <span class="cf">for</span> p <span class="kw">in</span> flow_dict[key][<span class="st">"model"</span>].parameters()]))]</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> key <span class="kw">in</span> flow_dict]</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>display(HTML(tabulate.tabulate(table, tablefmt<span class="op">=</span><span class="st">'html'</span>, headers<span class="op">=</span>[<span class="st">"Model"</span>, <span class="st">"Validation Bpd"</span>, <span class="st">"Test Bpd"</span>, <span class="st">"Inference time"</span>, <span class="st">"Sampling time"</span>, <span class="st">"Num Parameters"</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Model</th>
<th data-quarto-table-cell-role="th">Validation Bpd</th>
<th data-quarto-table-cell-role="th">Test Bpd</th>
<th data-quarto-table-cell-role="th">Inference time</th>
<th data-quarto-table-cell-role="th">Sampling time</th>
<th data-quarto-table-cell-role="th">Num Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>simple</td>
<td>1.080 bpd</td>
<td>1.078 bpd</td>
<td>20 ms</td>
<td>18 ms</td>
<td>556,312</td>
</tr>
<tr class="even">
<td>vardeq</td>
<td>1.045 bpd</td>
<td>1.043 bpd</td>
<td>26 ms</td>
<td>18 ms</td>
<td>628,388</td>
</tr>
<tr class="odd">
<td>multiscale</td>
<td>1.022 bpd</td>
<td>1.020 bpd</td>
<td>23 ms</td>
<td>15 ms</td>
<td>1,711,818</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Question1: Does variational dequantization improves bits per dimensions.Why?</p>
<p>Question2: Compare the sample quality of variational and standard dequantization?</p>
<div id="cell-56" class="cell" data-outputid="c0362118-d641-46a0-ef5b-d3505e8af11c">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">44</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> flow_dict[<span class="st">"vardeq"</span>][<span class="st">"model"</span>].sample(img_shape<span class="op">=</span>[<span class="dv">16</span>,<span class="dv">1</span>,<span class="dv">28</span>,<span class="dv">28</span>])</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>show_imgs(samples.cpu())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:lightning_fabric.utilities.seed:Seed set to 44</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-27-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-57" class="cell" data-outputid="11cddadf-70c5-404f-bb83-d0f52add4e15">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> flow_dict[<span class="st">"multiscale"</span>][<span class="st">"model"</span>].sample(img_shape<span class="op">=</span>[<span class="dv">16</span>,<span class="dv">8</span>,<span class="dv">7</span>,<span class="dv">7</span>])</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>show_imgs(samples.cpu())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:lightning_fabric.utilities.seed:Seed set to 42</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-28-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>From the few samples, we can see a clear difference between the simple and the multi-scale model. The single-scale model has only learned local, small correlations while the multi-scale model was able to learn full, global relations that form digits. This show-cases another benefit of the multi-scale model. In contrast to VAEs, the outputs are sharp as normalizing flows can naturally model complex, multi-modal distributions while VAEs have the independent decoder output noise. Nevertheless, the samples from this flow are far from perfect as not all samples show true digits.</p>
</section>
<section id="interpolation-in-latent-space" class="level3">
<h3 class="anchored" data-anchor-id="interpolation-in-latent-space">Interpolation in latent space</h3>
<p>Another popular test for the smoothness of the latent space of generative models is to interpolate between two training examples. As normalizing flows are strictly invertible, we can guarantee that any image is represented in the latent space. We again compare the variational dequantization model with the multi-scale model below.</p>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interpolate(model, img1, img2, num_steps<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co">        model - object of ImageFlow class that represents the (trained) flow model</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co">        img1, img2 - Image tensors of shape [1, 28, 28]. Images between which should be interpolated.</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co">        num_steps - Number of interpolation steps. 8 interpolation steps mean 6 intermediate pictures besides img1 and img2</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> torch.stack([img1, img2], dim<span class="op">=</span><span class="dv">0</span>).to(model.device)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    z, _ <span class="op">=</span> model.encode(imgs)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, steps<span class="op">=</span>num_steps, device<span class="op">=</span>z.device).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    interpolations <span class="op">=</span> z[<span class="dv">0</span>:<span class="dv">1</span>] <span class="op">*</span> alpha <span class="op">+</span> z[<span class="dv">1</span>:<span class="dv">2</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> alpha)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    interp_imgs <span class="op">=</span> model.sample(interpolations.shape[:<span class="dv">1</span>] <span class="op">+</span> imgs.shape[<span class="dv">1</span>:], z_init<span class="op">=</span>interpolations)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    show_imgs(interp_imgs, row_size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>exmp_imgs, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-61" class="cell" data-outputid="e059017d-b305-4aca-ed3e-ddd865eeefb2">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    interpolate(flow_dict[<span class="st">"vardeq"</span>][<span class="st">"model"</span>], exmp_imgs[<span class="dv">2</span><span class="op">*</span>i], exmp_imgs[<span class="dv">2</span><span class="op">*</span>i<span class="op">+</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:lightning_fabric.utilities.seed:Seed set to 42</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-30-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-30-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-62" class="cell" data-outputid="c6618a2c-d025-48ee-d91a-b6957d7a7067">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">42</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    interpolate(flow_dict[<span class="st">"multiscale"</span>][<span class="st">"model"</span>], exmp_imgs[<span class="dv">2</span><span class="op">*</span>i], exmp_imgs[<span class="dv">2</span><span class="op">*</span>i<span class="op">+</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:lightning_fabric.utilities.seed:Seed set to 42</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-31-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-31-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Question3: Compare two interploation examples.</p>
</section>
<section id="visualization-of-latents-in-different-levels-of-multi-scale" class="level3">
<h3 class="anchored" data-anchor-id="visualization-of-latents-in-different-levels-of-multi-scale">Visualization of latents in different levels of multi-scale</h3>
<p>In the following we will focus more on the multi-scale flow. We want to analyse what information is being stored in the variables split at early layers, and what information for the final variables. For this, we sample 8 images where each of them share the same final latent variables, but differ in the other part of the latent variables. Below we visualize three examples of this:</p>
<div id="cell-65" class="cell" data-scrolled="false" data-outputid="f1114bcf-215b-4fee-9429-0448b488dab1">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>pl.seed_everything(<span class="dv">44</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    z_init <span class="op">=</span> flow_dict[<span class="st">"multiscale"</span>][<span class="st">"model"</span>].prior.sample(sample_shape<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">7</span>,<span class="dv">7</span>])</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    z_init <span class="op">=</span> z_init.expand(<span class="dv">8</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> flow_dict[<span class="st">"multiscale"</span>][<span class="st">"model"</span>].sample(img_shape<span class="op">=</span>z_init.shape, z_init<span class="op">=</span>z_init)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    show_imgs(samples.cpu())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:lightning_fabric.utilities.seed:Seed set to 44</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-32-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-32-output-3.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-32-output-4.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see that the early split variables indeed have a smaller effect on the image. Still, small differences can be spot when we look carefully at the borders of the digits. For instance, in the middle, the top part of the 3 has different thicknesses for different samples although all of them represent the same coarse structure. This shows that the flow indeed learns to separate the higher-level information in the final variables, while the early split ones contain local noise patterns.</p>
</section>
<section id="visualizing-dequantization" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-dequantization">Visualizing Dequantization</h3>
<p>As a final part of this notebook, we will look at the effect of variational dequantization. We have motivated variational dequantization by the issue of sharp edges/boarders being difficult to model, and a flow would rather prefer smooth, prior-like distributions. To check how what noise distribution <span class="math inline">\(q(u|x)\)</span> the flows in the variational dequantization module have learned, we can plot a histogram of output values from the dequantization and variational dequantization module.</p>
<div id="cell-68" class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_dequant_distribution(model : ImageFlow, imgs : torch.Tensor, title:<span class="bu">str</span><span class="op">=</span><span class="va">None</span>):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co">        model - The flow of which we want to visualize the dequantization distribution</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co">        imgs - Example training images of which we want to visualize the dequantization distribution</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> imgs.to(device)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    ldj <span class="op">=</span> torch.zeros(imgs.shape[<span class="dv">0</span>], dtype<span class="op">=</span>torch.float32).to(device)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>        dequant_vals <span class="op">=</span> []</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">8</span>), leave<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>            d, _ <span class="op">=</span> model.flows[<span class="dv">0</span>](imgs, ldj, reverse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>            dequant_vals.append(d)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>        dequant_vals <span class="op">=</span> torch.cat(dequant_vals, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    dequant_vals <span class="op">=</span> dequant_vals.view(<span class="op">-</span><span class="dv">1</span>).cpu().numpy()</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    sns.<span class="bu">set</span>()</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    plt.hist(dequant_vals, bins<span class="op">=</span><span class="dv">256</span>, color<span class="op">=</span>to_rgb(<span class="st">"C0"</span>)<span class="op">+</span>(<span class="fl">0.5</span>,), edgecolor<span class="op">=</span><span class="st">"C0"</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> title <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>        plt.title(title)</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>sample_imgs, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-69" class="cell" data-outputid="cc33f829-f780-4609-f153-b6e5818a00d9">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>visualize_dequant_distribution(flow_dict[<span class="st">"simple"</span>][<span class="st">"model"</span>], sample_imgs, title<span class="op">=</span><span class="st">"Dequantization"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"215b5ab6b439442480d75a5b71070373","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-34-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-70" class="cell" data-outputid="cd1b51a9-064e-4442-eb78-10340b924979">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>visualize_dequant_distribution(flow_dict[<span class="st">"vardeq"</span>][<span class="st">"model"</span>], sample_imgs, title<span class="op">=</span><span class="st">"Variational dequantization"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"179fae4ce5c945e4971d2ad994530be4","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="NF_homework_files/figure-html/cell-35-output-2.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Question 4: Compare two distributions. Which one is smoother?</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In conclusion, we have seen how to implement our own normalizing flow, and what difficulties arise if we want to apply them on images. Dequantization is a crucial step in mapping the discrete images into continuous space to prevent underisable delta-peak solutions. While dequantization creates hypercubes with hard border, variational dequantization allows us to fit a flow much better on the data. This allows us to obtain a lower bits per dimension score, while not affecting the sampling speed. The most common flow element, the coupling layer, is simple to implement, and yet effective. Furthermore, multi-scale architectures help to capture the global image context while allowing us to efficiently scale up the flow. Normalizing flows are an interesting alternative to VAEs as they allow an exact likelihood estimate in continuous space, and we have the guarantee that every possible input <span class="math inline">\(x\)</span> has a corresponding latent vector <span class="math inline">\(z\)</span>. However, even beyond continuous inputs and images, flows can be applied and allow us to exploit the data structure in latent space, as e.g.&nbsp;on graphs for the task of molecule generation [6]. Recent advances in <a href="https://arxiv.org/pdf/1806.07366.pdf">Neural ODEs</a> allow a flow with infinite number of layers, called Continuous Normalizing Flows, whose potential is yet to fully explore. Overall, normalizing flows are an exciting research area which will continue over the next couple of years.</p>
<p>Advanced Topics in Normalizing Flows - 1x1 convolution</p>
<hr>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The <a href="https://arxiv.org/abs/1807.03039">Glow</a>, a flow-based generative model extends the previous invertible generative models, <a href="https://arxiv.org/abs/1410.8516">NICE</a> and <a href="https://arxiv.org/abs/1605.08803">RealNVP</a>, and simplifies the architecture by replacing the reverse permutation operation on the channel ordering with <strong>Invertible 1x1 Convolutions</strong>. Glow is famous for being the one of the first flow-based models that works on high resolution images and enables manipulation in latent space. Let’s have a look at the interactive <a href="https://openai.com/blog/glow/">demonstration</a> from OpenAI.</p>
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/assets/face_demo.gif?raw=1" width="350px" style="padding: 20px; margin:0 auto">
</center>
Glow consists of a series of steps of flow. Each step of flow comprises <strong>Actnorm</strong> followed by an <strong>Invertible 1×1 Convolution</strong>, and finally a <strong>Coupling Layer</strong>.
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/assets/glow_bb.png?raw=1" width="350px" style="padding: 20px; margin:0 auto">
</center>
<p><strong>Actnorm</strong> performs an affine transformation with a scale and bias parameter per channel, similar to that of batch normalization, but works on mini-batch size 1. The statistics (mean and std), however, are only calculated once to initialize the scale and bias parameters.</p>
<p><strong>Invertible 1×1 Convolution</strong> with equal number of input and output channels is a generalization of any permutation of the channel ordering. Recall the operation between layers of the RealNVP flow, the ordering of channels is switched so that all the data dimensions have a chance to be mixed. 1x1 convolution is proposed to replace this fixed permutation with a learned invertible operation.</p>
<p><strong>Coupling Layer</strong> is a powerful reversible transformation where the forward function, the reverse function and the logdeterminant are computationally efficient. The design is the same as in RealNVP.</p>
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/assets/glow_comp.png?raw=1" width="800px" style="padding: 20px; margin:0 auto">
</center>
<p>In this tutorial, we will be focusing on the implementation of invertible 1x1 convolution layer.</p>
</section>
<section id="invertible-1x1-convolution" class="level2">
<h2 class="anchored" data-anchor-id="invertible-1x1-convolution">Invertible 1x1 convolution</h2>
Given an input of shape <span class="math inline">\(H\times W\times C\)</span> applied with a 1x1 convolution with <span class="math inline">\(C\)</span> filters, meaning the output tensor shape is also going to be <span class="math inline">\(H\times W\times C\)</span>. Thus, each layer has a set of weights <span class="math inline">\(W\)</span> with <span class="math inline">\(C\times C\)</span> values. The forward operation acts just like a typical convolution, while the inverse operation can be computed by simply applying a convolution with <span class="math inline">\(W^{-1}\)</span> weights.
<center width="100%">
<img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/assets/1x1.png?raw=1" width="500px" style="padding: 20px; margin:0 auto">
</center>
<p>Enough descriptions! Now let’s take a look at the code.</p>
<div id="cell-79" class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InvConv2d(nn.Module):</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channel):</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>        weight <span class="op">=</span> torch.randn(in_channel, in_channel)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use the Q matrix from QR decomposition as the initial weight to make sure it's invertible</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>        q, _ <span class="op">=</span> torch.qr(weight)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>        weight <span class="op">=</span> q.unsqueeze(<span class="dv">2</span>).unsqueeze(<span class="dv">3</span>)</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(weight)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, logdet, reverse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>        _, _, height, width <span class="op">=</span> <span class="bu">input</span>.shape</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># You can also use torch.slogdet(self.weight)[1] to summarize the operations below\n",</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>        dlogdet <span class="op">=</span> (</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>            height <span class="op">*</span> width <span class="op">*</span> torch.log(torch.<span class="bu">abs</span>(torch.det(<span class="va">self</span>.weight.squeeze())))</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="va">self</span>.weight)</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>            logdet <span class="op">=</span> logdet <span class="op">+</span> dlogdet</span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="va">self</span>.weight.squeeze().inverse().unsqueeze(<span class="dv">2</span>).unsqueeze(<span class="dv">3</span>))</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>            logdet <span class="op">=</span> logdet <span class="op">-</span> dlogdet</span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, logdet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that to calculate the determinant of <span class="math inline">\(W\)</span> could be computationally expensive, thus there’s also an <a href="https://github.com/rosinality/glow-pytorch/blob/master/model.py#L88">implementation</a> which utilizes LU decomposition to speed up, as suggested in the Glow paper.</p>
<p>The idea is to parameterizing <span class="math inline">\(W\)</span> directly in its LU decomposition:</p>
<p><span class="math display">\[
W = PL(U + \text{diag}(s)),
\]</span></p>
<p>where <span class="math inline">\(P\)</span> is a permutation matrix, <span class="math inline">\(L\)</span> is a lower triangular matrix with ones on the diagonal, <span class="math inline">\(U\)</span> is an upper triangular matrix with zeros on the diagonal, and <span class="math inline">\(s\)</span> is a vector.</p>
<p>The log-determinant is then simply:</p>
<p><span class="math display">\[
\log | \det(W)| = \sum \left(\log |s|\right)
\]</span></p>
<p>Please check out the link above for the implementation.</p>
<section id="a-small-pitfall" class="level3">
<h3 class="anchored" data-anchor-id="a-small-pitfall">A small pitfall</h3>
<p>As you might notice, there’s an inverse operation for the weight <span class="math inline">\(W\)</span> involved when the <strong>1x1 convolution</strong> is forwarding reversely. As a result, an error can occur when the weight <span class="math inline">\(W\)</span> is not invertible, even though it seldom happens.</p>
<p>To our best knowledge, there’s no elegant solution to address this, but an easy way to workaround: If this happens unfortunately during the training, one can try to restart from the recent checkpoint.</p>
</section>
<section id="a-complete-flow-block" class="level3">
<h3 class="anchored" data-anchor-id="a-complete-flow-block">A complete flow block</h3>
<p>Now we have the <strong>Invertible 1x1 Convolution</strong>. Together with the aforementioned <strong>Actnorm</strong> and <strong>Coupling Layer</strong>, we are ready to try out the power of the Glow by plugging the block into the model we had in the NFs tutorial!</p>
<div id="cell-83" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ActNorm(nn.Module):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channel):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loc <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, in_channel, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log_scale <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, in_channel, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"initialized"</span>, torch.tensor(<span class="dv">0</span>, dtype<span class="op">=</span>torch.uint8))</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialize(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>            flatten <span class="op">=</span> <span class="bu">input</span>.permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>).contiguous().view(<span class="bu">input</span>.shape[<span class="dv">1</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Insert your code here!!!</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Implement mean and std</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># (HINT: Flatten, unsqueeze 3 times and permute channels. Do it for both std and mean)</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>            <span class="co">#</span></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>            <span class="co">#-------------------------</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loc.data.copy_(<span class="op">-</span>mean)</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.log_scale.data.copy_(<span class="op">-</span>std.clamp_(<span class="bu">min</span><span class="op">=</span><span class="fl">1e-6</span>).log())</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, logdet, reverse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>        _, _, height, width <span class="op">=</span> <span class="bu">input</span>.shape</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.initialized.item() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.initialize(<span class="bu">input</span>)</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.initialized.fill_(<span class="dv">1</span>)</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>        dlogdet <span class="op">=</span> height <span class="op">*</span> width <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="va">self</span>.log_scale)</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> reverse:</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>            logdet <span class="op">+=</span> dlogdet</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.log_scale.exp() <span class="op">*</span> (<span class="bu">input</span> <span class="op">+</span> <span class="va">self</span>.loc), logdet</span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>            dlogdet <span class="op">*=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>            logdet <span class="op">+=</span> dlogdet</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="bu">input</span> <span class="op">/</span> <span class="va">self</span>.log_scale.exp() <span class="op">-</span> <span class="va">self</span>.loc, logdet</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="conclusion-1" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-1">Conclusion</h2>
<p>We’ve learned an advanced flow-based layer from the Glow model, an <strong>Invertible 1x1 convolution</strong>, which is adapted from the typical 1x1 convolution layers.</p>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li><a href="https://arxiv.org/abs/1807.03039">Glow: Generative Flow with Invertible 1x1 Convolutions</a></li>
<li><a href="https://openai.com/blog/glow/">Glow: Better Reversible Generative Models</a></li>
<li>https://github.com/rosinality/glow-pytorch</li>
<li><a href="https://reurl.cc/9O8bka">Materials from NTU Speech Lab</a></li>
</ul>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>