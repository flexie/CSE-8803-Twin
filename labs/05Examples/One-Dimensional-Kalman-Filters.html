<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.528">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Digital Twins for Physical Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../images/digitaltwin.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://flexie.github.io/CSE-8803-Twin/" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../syllabus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../goals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Goals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../schedule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedule</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../seminar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Seminar</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Final Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#one-dimensional-kalman-filters" id="toc-one-dimensional-kalman-filters" class="nav-link active" data-scroll-target="#one-dimensional-kalman-filters">One Dimensional Kalman Filters</a>
  <ul class="collapse">
  <li><a href="#note-run-these-next-two-lines-if-on-google-collab.-otherwise-make-sure-to-install-filterpy-make-sure-that-kf_book-is-in-the-working-directory." id="toc-note-run-these-next-two-lines-if-on-google-collab.-otherwise-make-sure-to-install-filterpy-make-sure-that-kf_book-is-in-the-working-directory." class="nav-link" data-scroll-target="#note-run-these-next-two-lines-if-on-google-collab.-otherwise-make-sure-to-install-filterpy-make-sure-that-kf_book-is-in-the-working-directory.">Note: Run these next two lines if on Google collab. Otherwise make sure to install filterpy make sure that kf_book is in the working directory.</a></li>
  <li><a href="#problem-description" id="toc-problem-description" class="nav-link" data-scroll-target="#problem-description">Problem Description</a></li>
  <li><a href="#beliefs-as-gaussians" id="toc-beliefs-as-gaussians" class="nav-link" data-scroll-target="#beliefs-as-gaussians">Beliefs as Gaussians</a></li>
  <li><a href="#tracking-with-gaussian-probabilities" id="toc-tracking-with-gaussian-probabilities" class="nav-link" data-scroll-target="#tracking-with-gaussian-probabilities">Tracking with Gaussian Probabilities</a></li>
  <li><a href="#predictions-with-gaussians" id="toc-predictions-with-gaussians" class="nav-link" data-scroll-target="#predictions-with-gaussians">Predictions with Gaussians</a></li>
  <li><a href="#updates-with-gaussians" id="toc-updates-with-gaussians" class="nav-link" data-scroll-target="#updates-with-gaussians">Updates with Gaussians</a>
  <ul class="collapse">
  <li><a href="#understanding-gaussian-multiplication" id="toc-understanding-gaussian-multiplication" class="nav-link" data-scroll-target="#understanding-gaussian-multiplication">Understanding Gaussian Multiplication</a></li>
  <li><a href="#interactive-example" id="toc-interactive-example" class="nav-link" data-scroll-target="#interactive-example">Interactive Example</a></li>
  </ul></li>
  <li><a href="#first-kalman-filter" id="toc-first-kalman-filter" class="nav-link" data-scroll-target="#first-kalman-filter">First Kalman Filter</a></li>
  <li><a href="#code-walkthrough" id="toc-code-walkthrough" class="nav-link" data-scroll-target="#code-walkthrough">Code Walkthrough</a>
  <ul class="collapse">
  <li><a href="#exercise-modify-variance-values" id="toc-exercise-modify-variance-values" class="nav-link" data-scroll-target="#exercise-modify-variance-values">Exercise: Modify Variance Values</a></li>
  <li><a href="#kf-animation" id="toc-kf-animation" class="nav-link" data-scroll-target="#kf-animation">KF Animation</a></li>
  </ul></li>
  <li><a href="#kalman-gain" id="toc-kalman-gain" class="nav-link" data-scroll-target="#kalman-gain">Kalman Gain</a></li>
  <li><a href="#full-description-of-the-algorithm" id="toc-full-description-of-the-algorithm" class="nav-link" data-scroll-target="#full-description-of-the-algorithm">Full Description of the Algorithm</a></li>
  <li><a href="#comparison-with-g-h-and-discrete-bayes-filters" id="toc-comparison-with-g-h-and-discrete-bayes-filters" class="nav-link" data-scroll-target="#comparison-with-g-h-and-discrete-bayes-filters">Comparison with g-h and discrete Bayes Filters</a></li>
  <li><a href="#introduction-to-designing-a-filter" id="toc-introduction-to-designing-a-filter" class="nav-link" data-scroll-target="#introduction-to-designing-a-filter">Introduction to Designing a Filter</a>
  <ul class="collapse">
  <li><a href="#animation" id="toc-animation" class="nav-link" data-scroll-target="#animation">Animation</a></li>
  </ul></li>
  <li><a href="#example-extreme-amounts-of-noise" id="toc-example-extreme-amounts-of-noise" class="nav-link" data-scroll-target="#example-extreme-amounts-of-noise">Example: Extreme Amounts of Noise</a></li>
  <li><a href="#example-incorrect-process-variance" id="toc-example-incorrect-process-variance" class="nav-link" data-scroll-target="#example-incorrect-process-variance">Example: Incorrect Process Variance</a></li>
  <li><a href="#example-bad-initial-estimate" id="toc-example-bad-initial-estimate" class="nav-link" data-scroll-target="#example-bad-initial-estimate">Example: Bad Initial Estimate</a></li>
  <li><a href="#example-large-noise-and-bad-initial-estimate" id="toc-example-large-noise-and-bad-initial-estimate" class="nav-link" data-scroll-target="#example-large-noise-and-bad-initial-estimate">Example: Large Noise and Bad Initial Estimate</a></li>
  <li><a href="#exercise---nonlinear-systems" id="toc-exercise---nonlinear-systems" class="nav-link" data-scroll-target="#exercise---nonlinear-systems">Exercise - Nonlinear Systems</a></li>
  </ul></li>
  <li><a href="#enter-your-discussion-here." id="toc-enter-your-discussion-here." class="nav-link" data-scroll-target="#enter-your-discussion-here.">enter your Discussion here.</a>
  <ul class="collapse">
  <li><a href="#fixed-gain-filters" id="toc-fixed-gain-filters" class="nav-link" data-scroll-target="#fixed-gain-filters">Fixed Gain Filters</a></li>
  <li><a href="#filterpys-implementation" id="toc-filterpys-implementation" class="nav-link" data-scroll-target="#filterpys-implementation">FilterPy’s Implementation</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<p><a href="./table_of_contents.ipynb">Table of Contents</a></p>
<section id="one-dimensional-kalman-filters" class="level1">
<h1>One Dimensional Kalman Filters</h1>
<div id="cell-2" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="note-run-these-next-two-lines-if-on-google-collab.-otherwise-make-sure-to-install-filterpy-make-sure-that-kf_book-is-in-the-working-directory." class="level3">
<h3 class="anchored" data-anchor-id="note-run-these-next-two-lines-if-on-google-collab.-otherwise-make-sure-to-install-filterpy-make-sure-that-kf_book-is-in-the-working-directory.">Note: Run these next two lines if on Google collab. Otherwise make sure to install filterpy make sure that kf_book is in the working directory.</h3>
<div id="cell-4" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install filterpy</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip kf_book.<span class="bu">zip</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#format the book</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> book_format</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>book_format.set_style()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">

        <style>
        .output_wrapper, .output {
            height:auto !important;
            max-height:100000px;
        }
        .output_scroll {
            box-shadow:none !important;
            webkit-box-shadow:none !important;
        }
        </style>
    
</div>
</div>
<p>Now that we understand the discrete Bayes filter and Gaussians we are prepared to implement a Kalman filter. We will do this exactly as we did the discrete Bayes filter - rather than starting with equations we will develop the code step by step based on reasoning about the problem.</p>
<p>“One dimensional” means that the filter only tracks one state variable, such as position on the x-axis. In subsequent chapters we will learn a more general multidimensional form of the filter that can track many state variables simultaneously, such as position, velocity, and acceleration. Recall that we used velocity in the g-h filter to get better estimates than by tracking position alone. The same is true for the Kalman filter.</p>
<p>So why not just jump into the multidimensional form of the filter? To be honest, the math is difficult, and my intuitive approach to developing the filter starts to break down. This math obscures the rather simple principles that allow the Kalman filter to work.</p>
<p>So, in this chapter we learn how to use Gaussians to implement a Bayesian filter. That’s all the Kalman filter is - a Bayesian filter that uses Gaussians. In the next chapter we will switch to a multidimensional form and the full power of the Kalman filter will be unleashed!</p>
</section>
<section id="problem-description" class="level2">
<h2 class="anchored" data-anchor-id="problem-description">Problem Description</h2>
<p>As in the <strong>Discrete Bayes Filter</strong> chapter we will be tracking a moving object in a long hallway at work. Assume that in our latest hackathon someone created an RFID tracker that provides a reasonably accurate position of the dog. The sensor returns the distance of the dog from the left end of the hallway in meters. So, 23.4 would mean the dog is 23.4 meters from the left end of the hallway.</p>
<p>The sensor is not perfect. A reading of 23.4 could correspond to the dog being at 23.7, or 23.0. However, it is very unlikely to correspond to a position of 47.6. Testing during the hackathon confirmed this result - the sensor is ‘reasonably’ accurate, and while it had errors, the errors are small. Furthermore, the errors seemed to be evenly distributed on both sides of the true position; a position of 23 m would equally likely be measured as 22.9 or 23.1. Perhaps we can model this with a Gaussian.</p>
<p>We predict that the dog is moving. This prediction is not perfect. Sometimes our prediction will overshoot, sometimes it will undershoot. We are more likely to undershoot or overshoot by a little than a lot. Perhaps we can also model this with a Gaussian.</p>
</section>
<section id="beliefs-as-gaussians" class="level2">
<h2 class="anchored" data-anchor-id="beliefs-as-gaussians">Beliefs as Gaussians</h2>
<p>We can express our belief in the dog’s position with a Gaussian. Say we believe that our dog is at 10 meters, and the variance in that belief is 1 m<span class="math inline">\(^2\)</span>, or <span class="math inline">\(\mathcal{N}(10,\, 1)\)</span>. A plot of the pdf follows:</p>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> filterpy.stats <span class="im">as</span> stats</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>stats.plot_gaussian_pdf(mean<span class="op">=</span><span class="fl">10.</span>, variance<span class="op">=</span><span class="fl">1.</span>, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                        xlim<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">16</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">.5</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This plot depicts our uncertainty about the dog’s position. It represents a fairly inexact belief. While we believe that it is most likely that the dog is at 10 m, any position from 9 m to 11 m or so are quite likely as well. Assume the dog is standing still, and we query the sensor again. This time it returns 10.2 m. Can we use this additional information to improve our estimate?</p>
<p>Intuition suggests we can. Consider: if we read the sensor 500 times and each time it returned a value between 8 and 12, all centered around 10, we should be very confident that the dog is near 10. Of course, a different interpretation is possible. Perhaps our dog was randomly wandering back and forth in a way that exactly emulated random draws from a normal distribution. But that seems extremely unlikely - I’ve never seen a dog do that. Let’s look at 500 draws from <span class="math inline">\(\mathcal N(10, 1)\)</span>:</p>
<div id="cell-11" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> randn</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">500</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> randn(<span class="dv">500</span>)<span class="op">*</span><span class="fl">1.</span> <span class="op">+</span> <span class="fl">10.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ys)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean of readings is </span><span class="sc">{</span>np<span class="sc">.</span>mean(ys)<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Eyeballing this confirms our intuition - no dog moves like this. However, noisy sensor data certainly looks this way. The computed mean of the readings is almost exactly 10. Assuming the dog is standing still, we say the dog is at position 10 with a variance of 1.</p>
</section>
<section id="tracking-with-gaussian-probabilities" class="level2">
<h2 class="anchored" data-anchor-id="tracking-with-gaussian-probabilities">Tracking with Gaussian Probabilities</h2>
<p>The discrete Bayes filter used a histogram of probabilities to track the dog. Each bin in the histogram represents a position, and the value is the probability of the dog being in that position.</p>
<p>Tracking was performed with a cycle of predictions and updates. We used the equations</p>
<p><span class="math display">\[\begin{aligned}
\bar {\mathbf x} &amp;= \mathbf x \ast f_{\mathbf x}(\bullet)\, \, &amp;\text{Predict} \\
\mathbf x &amp;= \mathcal L \cdot \bar{\mathbf x}\, \, &amp;\text{Update}
\end{aligned}\]</span></p>
<p>to compute the new probability distributions. Recall that <span class="math inline">\(\bar{\mathbf x}\)</span> is the <em>prior</em>, <span class="math inline">\(\mathcal L\)</span> is the <em>likelihood</em> of a measurement given the prior <span class="math inline">\(\bar{\mathbf x}\)</span>, <span class="math inline">\(f_{\mathbf x}(\bullet)\)</span> is the <em>process model</em>, and <span class="math inline">\(\ast\)</span> denotes <em>convolution</em>. <span class="math inline">\(\mathbf x\)</span> is bold to denote that it is a histogram of numbers, or a vector.</p>
<p>This method works, but led to histograms that implied the dog could be in multiple places at once. Also, the computations are very slow for large problems.</p>
<p>Can we replace <span class="math inline">\(\mathbf x\)</span>, the histogram, with a Gaussian <span class="math inline">\(\mathcal N(x, \sigma^2)\)</span>? Absolutely! We’ve learned how to express belief as a Gaussian. A Gaussian, which is a single number pair <span class="math inline">\(\mathcal N(\mu, \sigma^2),\)</span> can replace an entire histogram of probabilities:</p>
<div id="cell-14" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> kf_book.kf_internal <span class="im">as</span> kf_internal</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>kf_internal.gaussian_vs_histogram()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I hope you see the power of this. We can replace hundreds to thousands of numbers with a single pair of numbers: <span class="math inline">\(x = \mathcal N(\mu, \sigma^2)\)</span>.</p>
<p>The tails of the Gaussian extend to infinity on both sides, so it incorporates arbitrarily many bars in the histogram. If this represents our belief in the position of the dog in the hallway, this one Gaussian covers the entire hallway (and the entire universe on that axis). We think that it is likely the dog is at 10, but he could be at 8, 14, or, with infinitesimally small probability, at 10<span class="math inline">\(^{80}\)</span>.</p>
<p>In this chapter we replace histograms with Gaussians:</p>
<p><span class="math display">\[\begin{array}{l|l|c}
\text{discrete Bayes} &amp; \text{Gaussian} &amp; \text{Step}\\
\hline
\bar {\mathbf x} = \mathbf x \ast f(\mathbf x) &amp;
\bar {x}_\mathcal{N} =  x_\mathcal{N} \, \oplus \, f_{x_\mathcal{N}}(\bullet) &amp;
\text{Predict} \\
\mathbf x = \|\mathcal L \bar{\mathbf x}\| &amp; x_\mathcal{N} = L \, \otimes \, \bar{x}_\mathcal{N} &amp; \text{Update}
\end{array}\]</span></p>
<p>where <span class="math inline">\(\oplus\)</span> and <span class="math inline">\(\otimes\)</span> is meant to express some unknown operator on Gaussians. I won’t do it in the rest of the book, but the subscript indicates that <span class="math inline">\(x_\mathcal{N}\)</span> is a Gaussian.</p>
<p>The discrete Bayes filter used convolution for the prediction. We showed that it used the <em>total probabability theorem</em>, computed as a sum, so maybe we can add the Gaussians. It used multiplications to incorporate the measurement into the prior, so maybe we can multiply the Gaussians. Could it be this easy:</p>
<p><span class="math display">\[\begin{aligned}
\bar x &amp;\stackrel{?}{=} x + f_x(\bullet) \\
x &amp;\stackrel{?}{=} \mathcal L \cdot \bar x
\end{aligned}\]</span></p>
<p>This will only work if the sum and product of two Gaussians is another Gaussian. Otherwise after the first epoch <span class="math inline">\(x\)</span> would not be Gaussian, and this scheme falls apart.</p>
</section>
<section id="predictions-with-gaussians" class="level2">
<h2 class="anchored" data-anchor-id="predictions-with-gaussians">Predictions with Gaussians</h2>
<p>We use Newton’s equation of motion to compute current position based on the current velocity and previous position:</p>
<p><span class="math display">\[ \begin{aligned}\bar{x}_k &amp;= x_{k-1} + v_k \Delta t \\
&amp;= x_{k-1} + f_x\end{aligned}\]</span></p>
<p>I’ve dropped the notation <span class="math inline">\(f_x(\bullet)\)</span> in favor of <span class="math inline">\(f_x\)</span> to keep the equations uncluttered.</p>
<p>If the dog is at 10 m, his velocity is 15 m/s, and the epoch is 2 seconds long, we have</p>
<p><span class="math display">\[ \begin{aligned} f_x &amp;= v\Delta t = 15\cdot 2\\
\bar{x}_k &amp;= 10 + (15\cdot 2) = 40 \end{aligned}\]</span></p>
<p>We are uncertain about his current position and velocity, so this will not do. We need to express the uncertainty with a Gaussian.</p>
<p>Position is easy. We define <span class="math inline">\(x\)</span> as a Gaussian. If we think the dog is at 10 m, and the standard deviation of our uncertainty is 0.2 m, we get <span class="math inline">\(x=\mathcal N(10, 0.2^2)\)</span>.</p>
<p>What about our uncertainty in his movement? We define <span class="math inline">\(f_x\)</span> as a Gaussian. If the dog’s velocity is 15 m/s, the epoch is 1 second, and the standard deviation of our uncertainty is 0.7 m/s, we get <span class="math inline">\(f_x = \mathcal N (15, 0.7^2)\)</span>.</p>
<p>The equation for the prior is</p>
<p><span class="math display">\[\bar x = x + f_x\]</span></p>
<p>What is the sum of two Gaussians? In the last chapter I proved that:</p>
<p><span class="math display">\[\begin{gathered}
\mu = \mu_1 + \mu_2 \\
\sigma^2 = \sigma^2_1 + \sigma^2_2
\end{gathered}\]</span></p>
<p>This is fantastic news; the sum of two Gaussians is another Gaussian!</p>
<p>The math works, but does this make intuitive sense? Think of the physical representation of this abstract equation. We have</p>
<p><span class="math display">\[\begin{gathered}
x=\mathcal N(10, 0.2^2)\\
f_x = \mathcal N (15, 0.7^2)
\end{gathered}\]</span></p>
<p>If we add these we get:</p>
<p><span class="math display">\[\begin{aligned}\bar x &amp;= \mu_x + \mu_{f_x} = 10 + 15 &amp;&amp;= 25 \\
\bar\sigma^2 &amp;= \sigma_x^2 + \sigma_{f_x}^2 = 0.2^2 + 0.7^2 &amp;&amp;= 0.53\end{aligned}\]</span></p>
<p>It makes sense that the predicted position is the previous position plus the movement. What about the variance? It is harder to form an intuition about this. However, recall that with the <code>predict()</code> function for the discrete Bayes filter we always lost information. We don’t really know where the dog is moving, so the confidence should get smaller (variance gets larger). <span class="math inline">\(\sigma_{f_x}^2\)</span> is the amount of uncertainty added to the system due to the imperfect prediction about the movement, and so we would add that to the existing uncertainty.</p>
<p>Let’s take advantage of the <code>namedtuple</code> class in Python’s <code>collection</code> module to implement a Gaussian object. We could implement a Gaussian using a tuple, where <span class="math inline">\(\mathcal N(10, 0.04)\)</span> is implemented in Python as <code>g = (10., 0.04)</code>. We would access the mean with <code>g[0]</code> and the variance with <code>g[1]</code>.</p>
<p><code>namedtuple</code> works the same as a tuple, except you provide it with a type name and field names. It’s not important to understand, but I modified the <code>__repr__</code> method to display its value using the notation in this chapter.</p>
<div id="cell-17" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>gaussian <span class="op">=</span> namedtuple(<span class="st">'Gaussian'</span>, [<span class="st">'mean'</span>, <span class="st">'var'</span>])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>gaussian.<span class="fu">__repr__</span> <span class="op">=</span> <span class="kw">lambda</span> s: <span class="ss">f'𝒩(μ=</span><span class="sc">{</span>s[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">, 𝜎²=</span><span class="sc">{</span>s[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">)'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can create a print a Gaussian with:</p>
<div id="cell-19" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>g1 <span class="op">=</span> gaussian(<span class="fl">3.4</span>, <span class="fl">10.1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> gaussian(mean<span class="op">=</span><span class="fl">4.5</span>, var<span class="op">=</span><span class="fl">0.2</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(g1)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(g2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>𝒩(μ=3.400, 𝜎²=10.100)
𝒩(μ=4.500, 𝜎²=0.040)</code></pre>
</div>
</div>
<p>We can access the mean and variance with either subscripts or field names:</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>g1.mean, g1[<span class="dv">0</span>], g1[<span class="dv">1</span>], g1.var</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(3.4, 3.4, 10.1, 10.1)</code></pre>
</div>
</div>
<p>Here is our implementation of the predict function, where <code>pos</code> and <code>movement</code> are Gaussian tuples in the form (<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>):</p>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(pos, movement):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gaussian(pos.mean <span class="op">+</span> movement.mean, pos.var <span class="op">+</span> movement.var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test it. What is the prior if the intitial position is the Gaussian <span class="math inline">\(\mathcal N(10, 0.2^2)\)</span> and the movement is the Gaussian <span class="math inline">\(\mathcal N (15, 0.7^2)\)</span>?</p>
<div id="cell-25" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> gaussian(<span class="fl">10.</span>, <span class="fl">.2</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>move <span class="op">=</span> gaussian(<span class="fl">15.</span>, <span class="fl">.7</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>predict(pos, move)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>𝒩(μ=25.000, 𝜎²=0.530)</code></pre>
</div>
</div>
<p>The prior states that the dog is at 25 m with a variance of 0.53 m<span class="math inline">\(^2\)</span>, which is what we computed by hand.</p>
</section>
<section id="updates-with-gaussians" class="level2">
<h2 class="anchored" data-anchor-id="updates-with-gaussians">Updates with Gaussians</h2>
<p>The discrete Bayes filter encodes our belief about the position of our dog in a histogram of probabilities. The distribution is discrete and multimodal. It can express strong belief that the dog is in two positions at once, and the positions are discrete.</p>
<p>We are proposing that we replace the histogram with a Gaussian. The discrete Bayes filter used this code to compute the posterior:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(likelihood, prior):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> likelihood <span class="op">*</span> prior</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> normalize(posterior)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which is an implementation of the equation:</p>
<p><span class="math display">\[x = \| \mathcal L\bar x \|\]</span></p>
<p>We’ve just shown that we can represent the prior with a Gaussian. What about the likelihood? The likelihood is the probability of the measurement given the current state. We’ve learned how to represent measurements as Gaussians. For example, maybe our sensor states that the dog is at 23 m, with a standard deviation of 0.4 meters. Our measurement, expressed as a likelihood, is <span class="math inline">\(z = \mathcal N (23, 0.16)\)</span>.</p>
<p>Both the likelihood and prior are modeled with Gaussians. Can we multiply Gaussians? Is the product of two Gaussians another Gaussian?</p>
<p>Yes to the former, and almost to the latter! In the last chapter I proved that the product of two Gaussians is proportional to another Gausian.</p>
<p><span class="math display">\[\begin{aligned}
\mu &amp;= \frac{\sigma_1^2 \mu_2 + \sigma_2^2 \mu_1} {\sigma_1^2 + \sigma_2^2}, \\
\sigma^2 &amp;= \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{aligned}\]</span></p>
<p>We can immediately infer several things. If we normalize the result, the product is another Gaussian. If one Gaussian is the likelihood, and the second is the prior, then the mean is a scaled sum of the prior and the measurement. The variance is a combination of the variances of the prior and measurement. Finally, the variances are completely unaffected by the values of the mean!</p>
<p>We put this in Bayesian terms like so:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal N(\mu, \sigma^2) &amp;= \| prior \cdot likelihood \|\\
&amp;= \| \mathcal{N}(\bar\mu, \bar\sigma^2)\cdot \mathcal{N}(\mu_z, \sigma_z^2) \|\\
&amp;= \mathcal N(\frac{\bar\sigma^2 \mu_z + \sigma_z^2 \bar\mu}{\bar\sigma^2 + \sigma_z^2},\frac{\bar\sigma^2\sigma_z^2}{\bar\sigma^2 + \sigma_z^2})
\end{aligned}\]</span></p>
<p>If we implemented that in a function <code>gaussian_multiply()</code> we could implement our filter’s update step as</p>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian_multiply(g1, g2):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> (g1.var <span class="op">*</span> g2.mean <span class="op">+</span> g2.var <span class="op">*</span> g1.mean) <span class="op">/</span> (g1.var <span class="op">+</span> g2.var)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> (g1.var <span class="op">*</span> g2.var) <span class="op">/</span> (g1.var <span class="op">+</span> g2.var)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gaussian(mean, variance)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(prior, likelihood):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> gaussian_multiply(likelihood, prior)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> posterior</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># test the update function</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>predicted_pos <span class="op">=</span> gaussian(<span class="fl">10.</span>, <span class="fl">.2</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>measured_pos <span class="op">=</span> gaussian(<span class="fl">11.</span>, <span class="fl">.1</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>estimated_pos <span class="op">=</span> update(predicted_pos, measured_pos)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>estimated_pos</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>𝒩(μ=10.800, 𝜎²=0.008)</code></pre>
</div>
</div>
<p>Perhaps this would be clearer if we used more specific names:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_dog(dog_pos, measurement):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    estimated_pos <span class="op">=</span> gaussian_multiply(measurement, dog_pos)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> estimated_pos  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That is less abstract, which perhaps helps with comprehension, but it is poor coding practice. We are writing a Kalman filter that works for any problem, not just tracking dogs in a hallway, so we won’t use variable names with ‘dog’ in them. Also, this form obscures the fact that we are multiplying the likelihood by the prior.</p>
<p>We have the majority of our filter implemented, but I fear this step is still a bit confusing. I’ve asserted that we can multiply Gaussians and that it correctly performs the update step, but why is this true? Let’s take a detour and spend some time multiplying Gaussians.</p>
<section id="understanding-gaussian-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="understanding-gaussian-multiplication">Understanding Gaussian Multiplication</h3>
<p>Let’s plot the pdf of <span class="math inline">\(\mathcal{N}(10,\, 1) \times \mathcal{N}(10,\, 1)\)</span>. Can you determine its shape without looking at the result? What should the new mean be? Will the curve be wider, narrower, or the same as <span class="math inline">\(\mathcal{N}(10,\, 1)\)</span>?</p>
<div id="cell-31" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> gaussian(<span class="fl">10.</span>, <span class="fl">1.</span>)  <span class="co"># Gaussian N(10, 1)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>product <span class="op">=</span> gaussian_multiply(z, z)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.arange(<span class="dv">5</span>, <span class="dv">15</span>, <span class="fl">0.1</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> [stats.gaussian(x, z.mean, z.var) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ys, label<span class="op">=</span><span class="st">'$\mathcal</span><span class="sc">{N}</span><span class="st">(10,1)$'</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> [stats.gaussian(x, product.mean, product.var) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ys, label<span class="op">=</span><span class="st">'$\mathcal</span><span class="sc">{N}</span><span class="st">(10,1) </span><span class="ch">\\</span><span class="st">times \mathcal</span><span class="sc">{N}</span><span class="st">(10,1)$'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(product)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result of the multiplication is taller and narrow than the original Gaussian but the mean is unchanged. Does this match your intuition?</p>
<p>Think of the Gaussians as two measurements. If I measure twice and get 10 meters each time, I should conclude that the length is close to 10 meters. Thus the mean should be 10. It would make no sense to conclude the length is actually 11, or 9.5. Also, I am more confident with two measurements than with one, so the variance of the result should be smaller.</p>
<p>“Measure twice, cut once” is a well known saying. Gaussian multiplication is a mathematical model of this physical fact.</p>
<p>I’m unlikely to get the same measurement twice in a row. Now let’s plot the pdf of <span class="math inline">\(\mathcal{N}(10.2,\, 1) \times \mathcal{N}(9.7,\, 1)\)</span>. What do you think the result will be? Think about it, and then look at the graph.</p>
<div id="cell-33" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="fl">3.3</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="ss">f'$(</span><span class="sc">{</span>m<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>m<span class="sc">}</span><span class="ss">)$'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>'$(3.3,3.3)$'</code></pre>
</div>
</div>
<div id="cell-34" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_products(g1, g2): </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    product <span class="op">=</span> gaussian_multiply(g1, g2)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> np.arange(<span class="dv">5</span>, <span class="dv">15</span>, <span class="fl">0.1</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> [stats.gaussian(x, g1.mean, g1.var) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    plt.plot(xs, ys, label<span class="op">=</span><span class="st">'$\mathcal</span><span class="sc">{N}</span><span class="st">$'</span> <span class="op">+</span> <span class="ss">f'$(</span><span class="sc">{</span>g1<span class="sc">.</span>mean<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>g1<span class="sc">.</span>var<span class="sc">}</span><span class="ss">)$'</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> [stats.gaussian(x, g2.mean, g2.var) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    plt.plot(xs, ys, label<span class="op">=</span><span class="st">'$\mathcal</span><span class="sc">{N}</span><span class="st">$'</span> <span class="op">+</span> <span class="st">'$(</span><span class="sc">{g2.mean}</span><span class="st">,</span><span class="sc">{ge.var}</span><span class="st">)$'</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> [stats.gaussian(x, product.mean, product.var) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    plt.plot(xs, ys, label<span class="op">=</span><span class="st">'product'</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    plt.legend()<span class="op">;</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> gaussian(<span class="fl">10.2</span>, <span class="dv">1</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> gaussian(<span class="fl">9.7</span>, <span class="dv">1</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>plot_products(z1, z2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you ask two people to measure the distance of a table from a wall, and one gets 10.2 meters, and the other got 9.7 meters, your best guess must be the average, 9.95 meters if you trust the skills of both equally.</p>
<p>Recall the g-h filter. We agreed that if I weighed myself on two scales, and the first read 160 lbs while the second read 170 lbs, and both were equally accurate, the best estimate was 165 lbs. Furthermore I should be a bit more confident about 165 lbs vs 160 lbs or 170 lbs because I now have two readings, both near this estimate, increasing my confidence that neither is wildly wrong.</p>
<p>This becomes counter-intuitive in more complicated situations, so let’s consider it further. Perhaps a more reasonable assumption would be that one person made a mistake, and the true distance is either 10.2 or 9.7, but certainly not 9.95. Surely that is possible. But we know we have noisy measurements, so we have no reason to think one of the measurements has no noise, or that one person made a gross error that allows us to discard their measurement. Given all available information, the best estimate must be 9.95.</p>
<p>In the update step of the Kalman filter we are not combining two measurements, but one measurement and the prior, our estimate before incorporating the measurement. We went through this logic for the g-h filter. It doesn’t matter if we are incorporating information from two measurements, or a measurement and a prediction, the math is the same.</p>
<p>Let’s look at that. I’ll create a fairly inaccurate prior of <span class="math inline">\(\mathcal N(8.5, 1.5)\)</span> and a more accurate measurement of <span class="math inline">\(\mathcal N(10.2, 0.5).\)</span> By “accurate” I mean the sensor variance is smaller than the prior’s variance, not that I somehow know that the dog is closer to 10.2 than 8.5. Next I’ll plot the reverse relationship: an accurate prior of <span class="math inline">\(\mathcal N(8.5, 0.5)\)</span> and a inaccurate measurement of <span class="math inline">\(\mathcal N(10.2, 1.5)\)</span>.</p>
<div id="cell-36" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>prior, z <span class="op">=</span> gaussian(<span class="fl">8.5</span>, <span class="fl">1.5</span>), gaussian(<span class="fl">10.2</span>, <span class="fl">0.5</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>plot_products(prior, z)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>prior, z <span class="op">=</span> gaussian(<span class="fl">8.5</span>, <span class="fl">0.5</span>), gaussian(<span class="fl">10.2</span>, <span class="fl">1.5</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>plot_products(prior, z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result is a Gaussian that is taller than either input. This makes sense - we have incorporated information, so our variance should have been reduced. And notice how the result is far closer to the the input with the smaller variance. We have more confidence in that value, so it makes sense to weight it more heavily.</p>
<p>It <em>seems</em> to work, but is it really correct? There is more to say about this, but I want to get a working filter going so you can experience it in concrete terms. After that we will revisit Gaussian multiplication and determine why it is correct.</p>
</section>
<section id="interactive-example" class="level3">
<h3 class="anchored" data-anchor-id="interactive-example">Interactive Example</h3>
<p>This interactive code provides sliders to alter the mean and variance of two Gaussians that are being multiplied together. As you move the sliders the plot is redrawn. Place your cursor inside the code cell and press CTRL+Enter to execute it.</p>
<div id="cell-39" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> interactive_gaussian(m1, m2, v1, v2):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    g1 <span class="op">=</span> gaussian(m1, v1)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    g2 <span class="op">=</span> gaussian(m2, v2)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    plot_products(g1, g2)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>interact(interactive_gaussian,</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>         m1<span class="op">=</span>(<span class="dv">5</span>, <span class="fl">10.</span>, <span class="fl">.5</span>), m2<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">15</span>, <span class="fl">.5</span>), </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>         v1<span class="op">=</span>(<span class="fl">.1</span>, <span class="dv">2</span>, <span class="fl">.1</span>), v2<span class="op">=</span>(<span class="fl">.1</span>, <span class="dv">2</span>, <span class="fl">.1</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a475b8df3b2841afa07da1def9d3dd5e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
<section id="first-kalman-filter" class="level2">
<h2 class="anchored" data-anchor-id="first-kalman-filter">First Kalman Filter</h2>
<p>Let’s get back to concrete terms and implement a Kalman filter. We’ve implemented the <code>update()</code> and <code>predict()</code> functions. We just need to write some boilerplate code to simulate a dog and create the measurements. I’ve put a <code>DogSimulation</code> class in <code>kf_internal</code> to avoid getting distracted with that task.</p>
<p>This boilerplate code sets up the problem by definine the means, variances, and generating the simulated dog movement.</p>
<div id="cell-41" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> kf_book.kf_internal <span class="im">as</span> kf_internal</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kf_book.kf_internal <span class="im">import</span> DogSimulation</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">13</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">1.</span> <span class="co"># variance in the dog's movement</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">2.</span> <span class="co"># variance in the sensor</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> gaussian(<span class="fl">0.</span>, <span class="fl">20.</span><span class="op">**</span><span class="dv">2</span>)  <span class="co"># dog's position, N(0, 20**2)</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>velocity <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> <span class="fl">1.</span> <span class="co"># time step in seconds</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(velocity<span class="op">*</span>dt, process_var) <span class="co"># displacement to add to x</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate dog and get measurements</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>x.mean, </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    velocity<span class="op">=</span>process_model.mean, </span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    measurement_var<span class="op">=</span>sensor_var, </span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    process_var<span class="op">=</span>process_model.var)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="co"># create list of measurements</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And here is the Kalman filter.</p>
<div id="cell-43" class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'PREDICT</span><span class="ch">\t\t\t</span><span class="st">UPDATE'</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'     x      var</span><span class="ch">\t\t</span><span class="st">  z</span><span class="ch">\t</span><span class="st">    x      var'</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># perform Kalman filter on measurement z</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:    </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(x, process_model)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> gaussian(z, sensor_var)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> update(prior, likelihood)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    kf_internal.print_gh(prior, x, z)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'final estimate:        </span><span class="sc">{</span>x<span class="sc">.</span>mean<span class="sc">:10.3f}</span><span class="ss">'</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'actual final position: </span><span class="sc">{</span>dog<span class="sc">.</span>x<span class="sc">:10.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PREDICT         UPDATE
     x      var       z     x      var
  1.000  401.000    1.354     1.352   1.990
  2.352    2.990    1.882     2.070   1.198
  3.070    2.198    4.341     3.736   1.047
  4.736    2.047    7.156     5.960   1.012
  6.960    2.012    6.939     6.949   1.003
  7.949    2.003    6.844     7.396   1.001
  8.396    2.001    9.847     9.122   1.000
 10.122    2.000    12.553   11.338   1.000
 12.338    2.000    16.273   14.305   1.000
 15.305    2.000    14.800   15.053   1.000

final estimate:            15.053
actual final position:     14.838</code></pre>
</div>
</div>
<p>Here is an animation of the filter. Predictions are plotted with a red triangle. After the prediction, the filter receives the next measurement, plotted as a black circle. The filter then forms an estimate part way between the two.</p>
<div id="cell-45" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kf_book <span class="im">import</span> book_plots <span class="im">as</span> book_plots</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets.widgets <span class="im">import</span> IntSlider</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># save output in these lists for plotting</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>xs, predictions <span class="op">=</span> [], []</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(velocity, process_var) </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co"># perform Kalman filter</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> gaussian(<span class="fl">0.</span>, <span class="fl">20.</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:    </span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(x, process_model)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> gaussian(z, sensor_var)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> update(prior, likelihood)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># save results</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    predictions.append(prior.mean)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    xs.append(x.mean)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_filter(step):</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    plt.cla()</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    step <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> step <span class="op">//</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    book_plots.plot_predictions(predictions[:i])    </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>        book_plots.plot_measurements(zs[:i<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        book_plots.plot_filter(xs[:i<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> step <span class="op">%</span> <span class="dv">3</span> <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>        book_plots.plot_measurements(zs[:i])</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>        book_plots.plot_filter(xs[:i<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>        book_plots.plot_measurements(zs[:i])</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>        book_plots.plot_filter(xs[:i])</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="dv">0</span>, <span class="dv">20</span>)</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>interact(plot_filter, step<span class="op">=</span>IntSlider(value<span class="op">=</span><span class="dv">1</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">1</span>, <span class="bu">max</span><span class="op">=</span><span class="bu">len</span>(predictions)<span class="op">*</span><span class="dv">3</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"43607f2b28a64ce49854da9ff10a9ea5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>I’ve plotted the prior (labeled <em>prediction</em>), the measurements, and the filter output. For each iteration of the loop we form a prior, take a measurement, form a likelihood from the measurement, and then incorporate the likelihood into the prior.</p>
<p>If you look at the plot you can see that the filter estimate is always between the measurement and prediction. Recall that for the g-h filter we argued that the estimate must always be between the measurement and prior. It makes no sense to choose a value outside of the two values. If I predict I am at 10, but measure that I am at 9, it would be foolish to decide that I must be at 8, or 11.</p>
</section>
<section id="code-walkthrough" class="level2">
<h2 class="anchored" data-anchor-id="code-walkthrough">Code Walkthrough</h2>
<p>Now let’s walk through the code.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">2.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These are the variances for the process model and sensor. The meaning of sensor variance should be clear - it is how much variance there is in each measurement. The process variance is how much error there is in the process model. We are predicting that at each time step the dog moves forward one meter. Dogs rarely do what we expect, and things like hills or the whiff of a squirrel will change his progress. If this was a robot responding to digital commands the performance would be much better, and perhaps the variance would be <span class="math inline">\(\sigma^2=.05\)</span>. These are not ‘magic’ numbers; the square root of the variance is the distance error in meters. It is easy to get a Kalman filter working by just plugging in numbers, but if the numbers do not reflect reality the performance of the filter will be poor.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> gaussian(<span class="fl">0.</span>, <span class="fl">20.</span><span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is the dog’s initial position expressed as a Gaussian. The position is 0 meters, and the variance to 400 m<span class="math inline">\(^2\)</span>, which is a standard deviation of 20 meters. You can think of this as saying “I believe with 99.7% accuracy the position is 0 plus or minus 60 meters”. This is because with Gaussians ~99.7% of values fall within <span class="math inline">\(\pm3\sigma\)</span> of the mean.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(velocity, process_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is the process model - the description of how we think the dog moves. How do I know the velocity? Magic? Consider it a prediction, or perhaps we have a secondary velocity sensor. If this is a robot then this would be a control input to the robot. In subsequent chapters we will learn how to handle situations where you don’t have a velocity sensor or input, so please accept this simplification for now.</p>
<p>Next we initialize the simulation and create 10 measurements:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">=</span>x.mean, </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    velocity<span class="op">=</span>process_model.mean, </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    measurement_var<span class="op">=</span>sensor_var, </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    process_var<span class="op">=</span>process_model.var)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we enter our <code>predict() ... update()</code> loop.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(x, process_model)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> gaussian(z, sensor_var)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> update(prior, likelihood)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The first time through the loop <code>prior</code> is <code>(1.0, 401.0)</code>, as can be seen in the printed table. After the prediction, we believe that we are at 1.0, and the variance is now 401, up from 400. The variance got worse, which is what always happens during the prediction step because it involves a loss of information.</p>
<p>Then we call the update function using <code>prior</code> as the current position.</p>
<p>For this I get this as the result: <code>pos = (1.352, 1.990), z = 1.354</code>.</p>
<p>What is happening? The dog is actually at 1.0 but the measured position is 1.354 due to sensor noise. That is pretty far from the predicted value of 1. The variance of the prior is 401 m<span class="math inline">\(^2\)</span>. A large variance implies that confidence is very low, so the filter estimates the position to be very close to the measurement: 1.352.</p>
<p>Now look at the variance: 1.99 m<span class="math inline">\(^2\)</span>. It has dropped tremendously from 401 m<span class="math inline">\(^2\)</span>. Why? Well, the RFID has a reasonably small variance of 2.0 m<span class="math inline">\(^2\)</span>, so we trust it far more than the prior. However, the previous belief does contain a bit of useful information, so our variance is now slightly smaller than 2.0.</p>
<p>Now the software loops, calling <code>predict()</code> and <code>update()</code> in turn. By the end the final estimated position is 15.053 vs the actual position of 14.838. The variance has converged to 1.0 m<span class="math inline">\(^2\)</span>.</p>
<p>Now look at the plot. The noisy measurements are plotted with black circles, and the filter results are drawn with a solid blue line. Both are quite noisy, but notice how much noisier the measurements are. I plotted the prediction (prior) with red triangles. The estimate always lies between the prior and the measurement. This is your first Kalman filter and it seems to work!</p>
<p>The filtering is implemented in only a few lines of code. Most of the code is either initialization, storing of data, simulating the dog movement, and printing results. The code that performs the filtering is very succinct:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> predict(x, process_model)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> gaussian(z, sensor_var)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> update(prior, likelihood)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we didn’t use the <code>predict</code> and <code>update</code> functions the code might be:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predict</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    dx <span class="op">=</span> velocity<span class="op">*</span>dt</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> pos <span class="op">+</span> dx</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> var <span class="op">+</span> process_var</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    pos  <span class="op">=</span> (var<span class="op">*</span>z <span class="op">+</span> sensor_var<span class="op">*</span>pos) <span class="op">/</span> (var <span class="op">+</span> sensor_var)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> (var <span class="op">*</span> sensor_var) <span class="op">/</span> (var <span class="op">+</span> sensor_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Just 5 lines of very simple math implements the entire filter!</p>
<p>In this example I only plotted 10 data points so the output from the print statements would not overwhelm us. Now let’s look at the filter’s performance with more data. The variance is plotted as a lightly shaded yellow area between dotted lines. I’ve increased the size of the process and sensor variance so they are easier to see on the chart - for a real Kalman filter of course you will not be randomly changing these values.</p>
<div id="cell-48" class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">4.5</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> gaussian(<span class="fl">0.</span>, <span class="fl">400.</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">1.</span>, process_var)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(x.mean, process_model.mean, sensor_var, process_var)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>xs, priors <span class="op">=</span> np.zeros((N, <span class="dv">2</span>)), np.zeros((N, <span class="dv">2</span>))</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, z <span class="kw">in</span> <span class="bu">enumerate</span>(zs):</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(x, process_model)    </span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> update(prior, gaussian(z, sensor_var))</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    priors[i] <span class="op">=</span> prior</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    xs[i] <span class="op">=</span> x</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(xs[:, <span class="dv">0</span>], var<span class="op">=</span>priors[:, <span class="dv">1</span>])</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>book_plots.plot_predictions(priors[:, <span class="dv">0</span>])</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>book_plots.show_legend()</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>kf_internal.print_variance(xs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we can see that the variance converges to 2.1623 in 9 steps. This means that we have become very confident in our position estimate. It is equal to <span class="math inline">\(\sigma=1.47\)</span> meters. Contrast this to the sensor’s <span class="math inline">\(\sigma=2.12\)</span> meters. The first few measurements are unsure due to our uncertainty of the initial position, but the filter quickly converges to an estimate with lower variance than the sensor!</p>
<p>This code fully implements a Kalman filter. If you have tried to read the literature you are perhaps surprised, because this looks nothing like the endless pages of math in those books. So long as we worry about <em>using</em> the equations rather than <em>deriving</em> them the topic is approachable. Moreover, I hope you’ll agree that you have a decent intuitive grasp of what is happening. We represent beliefs with Gaussians, and they get better over time because more measurements means we have more data to work with.</p>
<section id="exercise-modify-variance-values" class="level3">
<h3 class="anchored" data-anchor-id="exercise-modify-variance-values">Exercise: Modify Variance Values</h3>
<p>Modify the values of <code>process_var</code> and <code>sensor_var</code> and note the effect on the filter and on the variance. Which has a larger effect on the variance convergence? For example, which results in a smaller variance:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>or:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="dv">40</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="kf-animation" class="level3">
<h3 class="anchored" data-anchor-id="kf-animation">KF Animation</h3>
<p>If you are reading this in a browser you will be able to see an animation of the filter tracking the dog directly below this sentence. <img src="animations/05_dog_track.gif"></p>
<p>The top plot shows the output of the filter in green, and the measurements with a dashed red line. The bottom plot shows the Gaussian at each step.</p>
<p>When the track first starts you can see that the measurements varies quite a bit from the initial prediction. At this point the Gaussian probability is small (the curve is low and wide) so the filter does not trust its prediction. As a result, the filter adjusts its estimate a large amount. As the filter innovates you can see that as the Gaussian becomes taller, indicating greater certainty in the estimate, the filter’s output becomes very close to a straight line. At <code>x = 15</code> and greater you can see that there is a large amount of noise in the measurement, but the filter does not react much to it compared to how much it changed for the first noisy measurement.</p>
</section>
</section>
<section id="kalman-gain" class="level2">
<h2 class="anchored" data-anchor-id="kalman-gain">Kalman Gain</h2>
<p>We see that the filter works. Now let’s go back to the math to understand what is happening. The posterior <span class="math inline">\(x\)</span> is computed as the likelihood times the prior (<span class="math inline">\(\mathcal L \bar x\)</span>), where both are Gaussians.</p>
<p>Therefore the mean of the posterior is given by:</p>
<p><span class="math display">\[
\mu=\frac{\bar\sigma^2\, \mu_z + \sigma_z^2 \, \bar\mu} {\bar\sigma^2 + \sigma_z^2}
\]</span></p>
<p>I use the subscript <span class="math inline">\(z\)</span> to denote the measurement. We can rewrite this as:</p>
<p><span class="math display">\[\mu = \left( \frac{\bar\sigma^2}{\bar\sigma^2 + \sigma_z^2}\right) \mu_z + \left(\frac{\sigma_z^2}{\bar\sigma^2 + \sigma_z^2}\right)\bar\mu\]</span></p>
<p>In this form it is easy to see that we are scaling the measurement and the prior by weights:</p>
<p><span class="math display">\[\mu = W_1 \mu_z + W_2 \bar\mu\]</span></p>
<p>The weights sum to one because the denominator is a normalization term. We introduce a new term, <span class="math inline">\(K=W_1\)</span>, giving us:</p>
<p><span class="math display">\[\begin{aligned}
\mu &amp;= K \mu_z + (1-K) \bar\mu\\
&amp;= \bar\mu + K(\mu_z - \bar\mu)
\end{aligned}\]</span></p>
<p>where</p>
<p><span class="math display">\[K = \frac {\bar\sigma^2}{\bar\sigma^2 + \sigma_z^2}\]</span></p>
<p><span class="math inline">\(K\)</span> is the <em>Kalman gain</em>. It’s the crux of the Kalman filter. It is a scaling term that chooses a value partway between <span class="math inline">\(\mu_z\)</span> and <span class="math inline">\(\bar\mu\)</span>.</p>
<p>Let’s work a few examples. If the measurement is nine times more accurate than the prior, then <span class="math inline">\(\bar\sigma^2 = 9\sigma_z^2\)</span>, and</p>
<p><span class="math display">\[\begin{aligned}
\mu&amp;=\frac{9 \sigma_z^2 \mu_z + \sigma_z^2\, \bar\mu} {9 \sigma_z^2 + \sigma_\mathtt{z}^2} \\
&amp;= \left(\frac{9}{10}\right) \mu_z + \left(\frac{1}{10}\right) \bar\mu
\end{aligned}
\]</span></p>
<p>Hence <span class="math inline">\(K = \frac 9 {10}\)</span>, and to form the posterior we take nine tenths of the measurement and one tenth of the prior.</p>
<p>If the measurement and prior are equally accurate, then <span class="math inline">\(\bar\sigma^2 = \sigma_z^2\)</span> and</p>
<p><span class="math display">\[\begin{gathered}
\mu=\frac{\sigma_z^2\,  (\bar\mu + \mu_z)}{2\sigma_\mathtt{z}^2} \\
= \left(\frac{1}{2}\right)\bar\mu + \left(\frac{1}{2}\right)\mu_z
\end{gathered}\]</span></p>
<p>which is the average of the two means. It makes intuitive sense to take the average of two equally accurate values.</p>
<p>We can also express the variance in terms of the Kalman gain:</p>
<p><span class="math display">\[\begin{aligned}
\sigma^2 &amp;= \frac{\bar\sigma^2 \sigma_z^2 } {\bar\sigma^2 + \sigma_z^2} \\
&amp;= K\sigma_z^2 \\
&amp;= (1-K)\bar\sigma^2
\end{aligned}\]</span></p>
<p>We can understand this by looking at this chart:</p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> kf_book.book_plots <span class="im">as</span> book_plots</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>book_plots.show_residual_chart()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="One-Dimensional-Kalman-Filters_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The Kalman gain <span class="math inline">\(K\)</span> is a scale factor that chooses a value along the residual. This leads to an alternative but equivalent implementation for <code>update()</code> and <code>predict()</code>:</p>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(prior, measurement):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    x, P <span class="op">=</span> prior        <span class="co"># mean and variance of prior</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    z, R <span class="op">=</span> measurement  <span class="co"># mean and variance of measurement</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> z <span class="op">-</span> x        <span class="co"># residual</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> P <span class="op">/</span> (P <span class="op">+</span> R)  <span class="co"># Kalman gain</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x <span class="op">+</span> K<span class="op">*</span>y      <span class="co"># posterior</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> K) <span class="op">*</span> P  <span class="co"># posterior variance</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gaussian(x, P)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(posterior, movement):</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    x, P <span class="op">=</span> posterior <span class="co"># mean and variance of posterior</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    dx, Q <span class="op">=</span> movement <span class="co"># mean and variance of movement</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x <span class="op">+</span> dx</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> P <span class="op">+</span> Q</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gaussian(x, P)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Why have I written it in this form, and why have I chosen these terrible variable names? A few related reasons. A majority of books and papers present the Kalman filter in this form. My derivation of the filter from Bayesian principles is not unknown, but it is not used nearly as often. Alternative derivations naturally lead to this form of the equations. Also, the equations for the multivariate Kalman filter look almost exactly like these equations. So, you need to learn and understand them.</p>
<p>Where do the names <code>z</code>, <code>P</code>, <code>Q</code>, and <code>R</code> come from? You will see them used in the rest of this book. In the literature <span class="math inline">\(R\)</span> is nearly universally used for the measurement noise, <span class="math inline">\(Q\)</span> for the process noise and <span class="math inline">\(P\)</span> for the variance of the state. Using <span class="math inline">\(z\)</span> for the measurement is common, albeit not universal. Almost every book and paper you read will use these variable names. Get used to them.</p>
<p>This is also a powerful way to think about filtering. This is the way we reasoned about the g-h filter. It emphasizes taking the residual <span class="math inline">\(y = \mu_z - \bar\mu\)</span>, finding the Kalman gain as a ratio of our uncertainty in the prior and measurement <span class="math inline">\(K = P/(P+R)\)</span>, and computing the posterior by adding <span class="math inline">\(Ky\)</span> to the prior.</p>
<p>The Bayesian aspect is obscured in this form, as is the fact that we are multiplying the likelihood by the prior. Both viewpoints are equivalent because the math is identical. I chose the Bayesian approach because I think it give a much more intuitive yet deep understanding of the probabilistic reasoning. This alternative form using <span class="math inline">\(K\)</span> gives a deep understanding of what is known as the <em>orthogonal projection</em> approach. Dr.&nbsp;Kalman used that derivation, not Bayesian reasoning, when he invented this filter. You will understand more about this in the next few chapters.</p>
</section>
<section id="full-description-of-the-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="full-description-of-the-algorithm">Full Description of the Algorithm</h2>
<p>Recall the diagram we used for the g-h filter: <img src="./figs/residual_chart.png"></p>
<p>We’ve been doing the same thing in this chapter. The Kalman filter makes a prediction, takes a measurement, and then forms a new estimate somewhere between the two.</p>
<p><strong>This is extremely important to understand</strong>: Every filter in this book implements the same algorithm, just with different mathematical details. The math can become challenging in later chapters, but the idea is easy to understand.</p>
<p>It is important to see past the details of the equations of a specific filter and understand <em>what</em> the equations are calculating and <em>why</em>. There are a tremendous number of filters. They all use different math to implement the same algorithm. The choice of math affects the quality of results and what problems can be represented, but not the underlying ideas.</p>
<p>Here is the generic algorithm:</p>
<p><strong>Initialization</strong></p>
<pre><code>1. Initialize the state of the filter
2. Initialize our belief in the state</code></pre>
<p><strong>Predict</strong></p>
<pre><code>1. Use system behavior to predict state at the next time step
2. Adjust belief to account for the uncertainty in prediction</code></pre>
<p><strong>Update</strong></p>
<pre><code>1. Get a measurement and associated belief about its accuracy
2. Compute residual between estimated state and measurement
3. Compute scaling factor based on whether the measurement
or prediction is more accurate
4. set state between the prediction and measurement based 
on scaling factor
5. update belief in the state based on how certain we are 
in the measurement</code></pre>
<p>You will be hard pressed to find a Bayesian filter algorithm that does not fit into this form. Some filters will not include some aspects, such as error in the prediction, and others will have very complicated methods of computation, but this is what they all do.</p>
<p>The equations for the univariate Kalman filter are:</p>
<p><u>Predict</u></p>
<p><span class="math inline">\(\begin{array}{|l|l|l|} \hline \text{Equation} &amp; \text{Implementation} &amp; \text{Kalman Form}\\ \hline  \bar x = x + f_x &amp; \bar\mu = \mu + \mu_{f_x} &amp; \bar x = x + dx\\ &amp; \bar\sigma^2 = \sigma^2 + \sigma_{f_x}^2 &amp; \bar P = P + Q\\ \hline \end{array}\)</span></p>
<p><u>Update</u></p>
<p><span class="math inline">\(\begin{array}{|l|l|l|} \hline \text{Equation} &amp; \text{Implementation}&amp; \text{Kalman Form}\\ \hline  x = \| \mathcal L\bar x\| &amp; y = z - \bar\mu &amp; y = z - \bar x\\  &amp; K = \frac {\bar\sigma^2} {\bar\sigma^2 + \sigma_z^2} &amp; K = \frac {\bar P}{\bar P+R}\\  &amp; \mu = \bar \mu + Ky &amp; x = \bar x + Ky\\  &amp; \sigma^2 = \frac {\bar\sigma^2 \sigma_z^2} {\bar\sigma^2 + \sigma_z^2} &amp; P = (1-K)\bar P\\ \hline \end{array}\)</span></p>
</section>
<section id="comparison-with-g-h-and-discrete-bayes-filters" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-g-h-and-discrete-bayes-filters">Comparison with g-h and discrete Bayes Filters</h2>
<p>Now is a good time to understand the differences between these three filters in terms of how we model errors. For the g-h filter we modeled our measurements as shown in this graph:</p>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>book_plots.plot_errorbars([(<span class="dv">160</span>, <span class="dv">3</span>, <span class="st">'A'</span>), (<span class="dv">170</span>, <span class="dv">9</span>, <span class="st">'B'</span>)], xlims<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">180</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Sensor A returned a measurement of 160, and sensor B returned 170. The bars are <a href="https://en.wikipedia.org/wiki/Error_bar"><em>error bars</em></a> - they illustrate the possible range of error for the measurement. Hence, the actual value that A is measuring can be between 157 to 163, and B is measuring a value between 161 to 179.</p>
<p>I did not define it at the time, but this is a [<em>uniform distribution</em>](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)). A uniform distribution assigns equal probability to any event in the range. According to this model it is equally likely for sensor A to read 157, 160, or 163. Any value outside these ranges have 0 probability.</p>
<p>We can model this situation with Gaussians. I’ll use <span class="math inline">\(\mathcal{N}(160, 3^2)\)</span> for sensor A, and <span class="math inline">\(\mathcal{N}(170, 9^2)\)</span> for sensor B. I’ve plotted these below with the uniform distribution error bars for comparison.</p>
<div id="cell-62" class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.arange(<span class="dv">145</span>, <span class="dv">190</span>, <span class="fl">0.1</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> [stats.gaussian(x, <span class="dv">160</span>, <span class="dv">3</span><span class="op">**</span><span class="dv">2</span>) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ys, label<span class="op">=</span><span class="st">'A'</span>, color<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> [stats.gaussian(x, <span class="dv">170</span>, <span class="dv">9</span><span class="op">**</span><span class="dv">2</span>) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ys, label<span class="op">=</span><span class="st">'B'</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>plt.errorbar(<span class="dv">160</span>, [<span class="fl">0.04</span>], xerr<span class="op">=</span>[<span class="dv">3</span>], fmt<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'g'</span>, capthick<span class="op">=</span><span class="dv">2</span>, capsize<span class="op">=</span><span class="dv">10</span>)    </span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>plt.errorbar(<span class="dv">170</span>, [<span class="fl">0.015</span>], xerr<span class="op">=</span>[<span class="dv">9</span>], fmt<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'b'</span>, capthick<span class="op">=</span><span class="dv">2</span>, capsize<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using a uniform or Gaussian distribution is a modeling choice. Neither exactly describes reality. In most cases the Gaussian distribution is more realistic. Most sensors are more likely to return readings near the value being measured, and unlikely to return a reading far from that value. The Gaussian models this tendency. In contrast the uniform distribution assumes that any measurement within a range is equally likely.</p>
<p>Now let’s see the <em>discrete distribution</em> used in the discrete Bayes filter. This model divides the range of possible values into discrete ranges and assigns a probability to each bucket. This assignment can be entirely arbitrary so long as the probabilities sum to one.</p>
<p>Let’s plot the data for one sensor using a uniform distribution, a Gaussian distribution, and a discrete distribution.</p>
<div id="cell-64" class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> random <span class="im">import</span> random</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.arange(<span class="dv">145</span>, <span class="dv">190</span>, <span class="fl">0.1</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> [stats.gaussian(x, <span class="dv">160</span>, <span class="dv">3</span><span class="op">**</span><span class="dv">2</span>) <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>belief <span class="op">=</span> np.array([random() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>)])</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>belief <span class="op">=</span> belief <span class="op">/</span> <span class="bu">sum</span>(belief)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">155</span>, <span class="dv">165</span>, <span class="bu">len</span>(belief))</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>plt.gca().bar(x, belief, width<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ys, label<span class="op">=</span><span class="st">'A'</span>, color<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>plt.errorbar(<span class="dv">160</span>, [<span class="fl">0.04</span>], xerr<span class="op">=</span>[<span class="dv">3</span>], fmt<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'k'</span>, capthick<span class="op">=</span><span class="dv">2</span>, capsize<span class="op">=</span><span class="dv">10</span>)    </span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">150</span>, <span class="dv">170</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I used random numbers to form the discrete distribution to illustrate that it can model any arbitrary probability distribution. This provides it with enormous power. With enough discrete buckets we can model the error characteristics of any sensor no matter how complicated. But with this power comes mathematical intractability. Multiplying or adding Gaussians takes two lines of math, and the result is another Gaussian. This regularity allows us to perform powerful analysis on the performance and behavior of our filters. Multiplying or adding a discrete distribution requires looping over the data, and we have no easy way to characterize the result. Analyzing the performance characteristics of a filter based on a discrete distribution is extremely difficult to impossible.</p>
<p>There is no ‘correct’ choice here. Later in the book we will introduce the <em>particle filter</em> which uses a discrete distribution. It is an extremely powerful technique because it can handle arbitrarily complex situations. This comes at the cost of slow performance, and resistance to analytical analysis.</p>
<p>For now we will ignore these matters and return to using Gaussians for the next several chapters. As we progress you will learn the strengths and limitations of using Gaussians in our mathematical models.</p>
</section>
<section id="introduction-to-designing-a-filter" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-designing-a-filter">Introduction to Designing a Filter</h2>
<p>So far we have developed filters for a position sensor. We are used to this problem by now, and may feel ill-equipped to implement a Kalman filter for a different problem. To be honest, there is still quite a bit of information missing from this presentation. Following chapters will fill in the gaps. Still, let’s get a feel for it by designing and implementing a Kalman filter for a thermometer. The sensor for the thermometer outputs a voltage that corresponds to the temperature that is being measured. We have read the manufacturer’s specifications for the sensor, and it tells us that the sensor exhibits white noise with a standard deviation of 0.13 volts.</p>
<p>We can simulate the temperature sensor measurement with this function:</p>
<div id="cell-67" class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> volt(voltage, std):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> voltage <span class="op">+</span> (randn() <span class="op">*</span> std)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we need to write the Kalman filter processing loop. As with our previous problem, we need to perform a cycle of predicting and updating. The sensing step probably seems clear - call <code>volt()</code> to get the measurement, pass the result into <code>update()</code> method, but what about the predict step? We do not have a sensor to detect ‘movement’ in the voltage, and for any small duration we expect the voltage to remain constant. How shall we handle this?</p>
<p>As always, we will trust in the math. We have no known movement, so we will set that to zero. However, that means that we are predicting that the temperature will never change. If that is true, then over time we should become extremely confident in our results. Once the filter has enough measurements it will become very confident that it can predict the subsequent temperatures, and this will lead it to ignoring measurements that result due to an actual temperature change. This is called a <em>smug</em> filter, and is something you want to avoid. So we will add a bit of error to our prediction step to tell the filter not to discount changes in voltage over time. In the code below I set <code>process_var = .05**2</code>. This is the expected variance in the change of voltage over each time step. I chose this value merely to be able to show how the variance changes through the update and predict steps. For a real sensor you would set this value for the actual amount of change you expect. For example, this would be an extremely small number if it is a thermometer for ambient air temperature in a house, and a high number if this is a thermocouple in a chemical reaction chamber. We will say more about selecting the actual value in the later chapters.</p>
<p>Let’s see what happens.</p>
<div id="cell-69" class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>temp_change <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>voltage_std <span class="op">=</span> <span class="fl">.13</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">.05</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>actual_voltage <span class="op">=</span> <span class="fl">16.3</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> gaussian(<span class="fl">25.</span>, <span class="fl">1000.</span>) <span class="co"># initial state</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">0.</span>, process_var)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [volt(actual_voltage, voltage_std) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> []</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>estimates <span class="op">=</span> []</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(x, process_model)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> update(prior, gaussian(z, voltage_std<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># save for latter plotting</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    estimates.append(x.mean)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    ps.append(x.var)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the filter output and the variance</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(estimates, var<span class="op">=</span>np.array(ps))</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>book_plots.show_legend()</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">16</span>, <span class="dv">17</span>)</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>book_plots.set_labels(x<span class="op">=</span><span class="st">'step'</span>, y<span class="op">=</span><span class="st">'volts'</span>)</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>plt.plot(ps)</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Variance'</span>)</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Variance converges to </span><span class="sc">{</span>ps[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first plot shows the individual sensor measurements vs the filter output. Despite a lot of noise in the sensor we quickly discover the approximate voltage of the sensor. In the run I just completed at the time of authorship, the last voltage output from the filter is <span class="math inline">\(16.213\)</span>, which is quite close to the <span class="math inline">\(16.4\)</span> used by the <code>volt()</code> function. On other runs I have gotten larger and smaller results.</p>
<p>Spec sheets are what they sound like - specifications. Any individual sensor will exhibit different performance based on normal manufacturing variations. Values are often maximums - the spec is a guarantee that the performance will be at least that good. If you buy an expensive piece of equipment it often comes with a sheet of paper displaying the test results of your specific item; this is usually very trustworthy. On the other hand, if this is a cheap sensor it is likely it received little to no testing prior to being sold. Manufacturers typically test a small subset of their output to verify that a sample falls within the desired performance range. If you have a critical application you will need to read the specification sheet carefully to figure out exactly what they mean by their ranges. Do they guarantee their number is a maximum, or is it, say, the <span class="math inline">\(3\sigma\)</span> error rate? Is every item tested? Is the variance normal, or some other distribution? Finally, manufacturing is not perfect. Your part might be defective and not match the performance on the sheet.</p>
<p>For example, I am looking at a data sheet for an airflow sensor. There is a field <em>Repeatability</em>, with the value <span class="math inline">\(\pm 0.50\%\)</span>. Is this a Gaussian? Is there a bias? For example, perhaps the repeatability is nearly <span class="math inline">\(0.0\%\)</span> at low temperatures, and always nearly <span class="math inline">\(+0.50\%\)</span> at high temperatures. Data sheets for electrical components often contain a section of “Typical Performance Characteristics”. These are used to capture information that cannot be easily conveyed in a table. For example, I am looking at a chart showing output voltage vs current for a LM555 timer. There are three curves showing the performance at different temperatures. The response is ideally linear, but all three lines are curved. This clarifies that errors in voltage outputs are probably not Gaussian - in this chip’s case higher temperatures lead to lower voltage output, and the voltage output is quite nonlinear if the input current is very high.</p>
<p>As you might guess, modeling the performance of your sensors is one of the harder parts of creating a Kalman filter that performs well.</p>
<section id="animation" class="level3">
<h3 class="anchored" data-anchor-id="animation">Animation</h3>
<p>For those reading this in a browser here is an animation showing the filter working. If you are not using a browser you can see this plot at https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/animations/05_volt_animate.gif.</p>
<p><img src="animations/05_volt_animate.gif"></p>
<p>The top plot in the animation draws a green line for the predicted next voltage, then a red ‘+’ for the actual measurement, draws a light red line to show the residual, and then draws a blue line to the filter’s output. You can see that when the filter starts the corrections made are quite large, but after only a few updates the filter only adjusts its output by a small amount even when the measurement is far from it.</p>
<p>The lower plot shows the Gaussian belief as the filter innovates. When the filter starts the Gaussian curve is centered over 25, our initial guess for the voltage, and is very wide and short due to our initial uncertainty. But as the filter innovates, the Gaussian quickly moves to about 16.0 and becomes taller, reflecting the growing confidence that the filter has in it’s estimate for the voltage. You will also note that the Gaussian’s height bounces up and down a little bit. If you watch closely you will see that the Gaussian becomes a bit shorter and more spread out during the prediction step, and becomes taller and narrower as the filter incorporates another measurement.</p>
<p>Think of this animation in terms of the g-h filter. At each step the g-h filter makes a prediction, takes a measurement, computes the residual (the difference between the prediction and the measurement), and then selects a point on the residual line based on the scaling factor <span class="math inline">\(g\)</span>. The Kalman filter is doing exactly the same thing, except that the scaling factor <span class="math inline">\(g\)</span> varies with time. As the filter becomes more confident in its state the scaling factor favors the filter’s prediction over the measurement.</p>
</section>
</section>
<section id="example-extreme-amounts-of-noise" class="level2">
<h2 class="anchored" data-anchor-id="example-extreme-amounts-of-noise">Example: Extreme Amounts of Noise</h2>
<p>With the dog filter I didn’t put a lot of noise in the signal, and I ‘guessed’ that the dog was at position 0. How does the filter perform in real world conditions? I will start by injecting more noise in the RFID sensor while leaving the process variance at 2 m<span class="math inline">\(^2\)</span>. I will inject an extreme amount of noise - noise that apparently swamps the actual measurement. What does your intuition say about the filter’s performance if the sensor has a standard deviation of 300 meters? In other words, an actual position of 1.0 m might be reported as 287.9 m, or -589.6 m, or any other number in roughly that range. Think about it before you scroll down.</p>
<div id="cell-75" class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">300.</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">1.</span>, process_var)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> gaussian(<span class="fl">0.</span>, <span class="fl">500.</span>)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(pos.mean, <span class="fl">1.</span>, sensor_var, process_var)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> []</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(pos, process_model)    </span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> update(prior, gaussian(zs[i], sensor_var))</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    ps.append(pos.mean)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(ps)</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this example the noise is extreme yet the filter still outputs a nearly straight line! This is an astonishing result! What do you think might be the cause of this performance?</p>
<p>We get a nearly straight line because our process error is small. A small process error tells the filter that the prediction is very trustworthy, and the prediction is a straight line, so the filter outputs a nearly straight line.</p>
</section>
<section id="example-incorrect-process-variance" class="level2">
<h2 class="anchored" data-anchor-id="example-incorrect-process-variance">Example: Incorrect Process Variance</h2>
<p>That last filter looks fantastic! Why wouldn’t we set the process variance very low, as it guarantees the result will be straight and smooth?</p>
<p>The process variance tells the filter how much the system is changing over time. If you lie to the filter by setting this number artificially low the filter will not be able to react to changes that are happening. Let’s have the dog increase his velocity by a small amount at each time step and see how the filter performs with a process variance of 0.001 m<span class="math inline">\(^2\)</span>.</p>
<div id="cell-78" class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">20.</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">.001</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">1.</span>, process_var)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> gaussian(<span class="fl">0.</span>, <span class="fl">500.</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(pos.mean, <span class="dv">1</span>, sensor_var, process_var<span class="op">*</span><span class="dv">10000</span>)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>zs, ps <span class="op">=</span> [], []</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    dog.velocity <span class="op">+=</span> <span class="fl">0.04</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    zs.append(dog.move_and_sense())</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(pos, process_model)    </span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> update(prior, gaussian(z, sensor_var))</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>    ps.append(pos.mean)</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(ps)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="One-Dimensional-Kalman-Filters_files/figure-html/cell-31-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It is easy to see that the filter is not correctly responding to the measurements. The measurements clearly indicate that the dog is changing speed but the filter has been told that it’s predictions are nearly perfect so it almost entirely ignores them. I encourage you to adjust the amount of movement in the dog vs process variance. We will also be studying this topic much more in the later chapters. The key point is to recognize that math requires that the variances correctly describe your system. The filter does not ‘notice’ that it is diverging from the measurements and correct itself. It computes the Kalman gain from the variance of the prior and the measurement, and forms the estimate depending on which is more accurate.</p>
</section>
<section id="example-bad-initial-estimate" class="level2">
<h2 class="anchored" data-anchor-id="example-bad-initial-estimate">Example: Bad Initial Estimate</h2>
<p>Now let’s look at the results when we make a bad initial estimate of position. To avoid obscuring the results I’ll reduce the sensor variance to 30, but set the initial position to 1000 meters. Can the filter recover from a 1000 meter error?</p>
<div id="cell-81" class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">5.</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> gaussian(<span class="fl">1000.</span>, <span class="fl">500.</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">1.</span>, process_var)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(<span class="dv">0</span>, <span class="dv">1</span>, sensor_var, process_var)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> []</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(pos, process_model)    </span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> update(prior, gaussian(z, sensor_var))</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    ps.append(pos.mean)</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(ps)</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again the answer is yes! Because we are relatively sure about our belief in the sensor (<span class="math inline">\(\sigma^2=5^2\)</span>) after only the first step we have changed our position estimate from 1000 m to roughly 50 m. After another 5-10 measurements we have converged to the correct value. This is how we get around the chicken and egg problem of initial guesses. In practice we would likely assign the first measurement from the sensor as the initial value, but you can see it doesn’t matter much if we wildly guess at the initial conditions - the Kalman filter still converges so long as the filter variances are chosen to match the actual process and measurement variances.</p>
</section>
<section id="example-large-noise-and-bad-initial-estimate" class="level2">
<h2 class="anchored" data-anchor-id="example-large-noise-and-bad-initial-estimate">Example: Large Noise and Bad Initial Estimate</h2>
<p>What about the worst of both worlds, large noise and a bad initial estimate?</p>
<div id="cell-84" class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">30000.</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> gaussian(<span class="fl">1000.</span>, <span class="fl">500.</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">1.</span>, process_var)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(<span class="dv">0</span>, <span class="dv">1</span>, sensor_var, process_var)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> []</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(pos, process_model) </span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> update(prior, gaussian(z, sensor_var))</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>    ps.append(pos.mean)</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(ps)</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This time the filter struggles. Notice that the previous example only computed 100 updates, whereas this example uses 1000. By my eye it takes the filter 400 or so iterations to become reasonable accurate, but maybe over 600 before the results are good. Kalman filters are good, but we cannot expect miracles. If we have extremely noisy data and extremely bad initial conditions, this is as good as it gets.</p>
<p>Finally, let’s implement the suggestion of using the first measurement as the initial position.</p>
<div id="cell-86" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>sensor_var <span class="op">=</span> <span class="fl">30000.</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>process_var <span class="op">=</span> <span class="fl">2.</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>process_model <span class="op">=</span> gaussian(<span class="fl">1.</span>, process_var)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>dog <span class="op">=</span> DogSimulation(<span class="dv">0</span>, <span class="dv">1</span>, sensor_var, process_var)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>zs <span class="op">=</span> [dog.move_and_sense() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> gaussian(zs[<span class="dv">0</span>], <span class="fl">500.</span>)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> []</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> zs:</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> predict(pos, process_model) </span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> update(prior, gaussian(z, sensor_var))</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>    ps.append(pos.mean)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>book_plots.plot_measurements(zs, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>book_plots.plot_filter(ps)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This simple change significantly improves the results. On some runs it takes 200 iterations or so to settle to a good solution, but other runs it converges very rapidly. This all depends on the amount of noise in the first measurement. A large amount of noise causes the initial estimate to be far from the dog’s position.</p>
<p>200 iterations may seem like a lot, but the amount of noise we are injecting is truly huge. In the real world we use sensors like thermometers, laser range finders, GPS satellites, computer vision, and so on. None have the enormous errors in these examples. A reasonable variance for a cheap thermometer might be 0.2 C<span class="math inline">\(^{\circ 2}\)</span>, and our code is using 30,000 C<span class="math inline">\(^{\circ 2}\)</span>.</p>
</section>
<section id="exercise---nonlinear-systems" class="level2">
<h2 class="anchored" data-anchor-id="exercise---nonlinear-systems">Exercise - Nonlinear Systems</h2>
<p>Our equations for the Kalman filter are linear:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{N}(\bar\mu,\, \bar\sigma^2) &amp;= \mathcal{N}(\mu,\, \sigma^2) + \mathcal{N}(\mu_\mathtt{move},\, \sigma^2_\mathtt{move})\\
\mathcal{N}(\mu,\, \sigma^2) &amp;= \mathcal{N}(\bar\mu,\, \bar\sigma^2)  \times \mathcal{N}(\mu_\mathtt{z},\, \sigma^2_\mathtt{z})
\end{aligned}\]</span></p>
<p>Do you suppose that this filter works well or poorly with nonlinear systems?</p>
<p>Implement a Kalman filter that uses the following equation to generate the measurement value</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> math.sin(i<span class="op">/</span><span class="fl">3.</span>) <span class="op">*</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Adjust the variance and initial positions to see the effect. What is, for example, the result of a very bad initial guess?</p>
<div id="cell-89" class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">#enter your code here.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="enter-your-discussion-here." class="level1">
<h1>enter your Discussion here.</h1>
<section id="fixed-gain-filters" class="level2">
<h2 class="anchored" data-anchor-id="fixed-gain-filters">Fixed Gain Filters</h2>
<p>Embedded computers usually have extremely limited processors. Many do not have floating point circuitry. These simple equations can impose a heavy burden on the chip. This is less true as technology advances, but do not underestimate the value of spending one dollar less on a processor when you will be buying millions of them.</p>
<p>In the example above the variance of the filter converged to a fixed value. This will always happen if the variance of the measurement and process is a constant. You can take advantage of this fact by running simulations to determine what the variance converges to. Then you can hard code this value into your filter. So long as you initialize the filter to a good starting guess (I recommend using the first measurement as your initial value) the filter will perform very well. For example, the dog tracking filter can be reduced to this:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(x, z):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> <span class="fl">.13232</span>  <span class="co"># experimentally derived Kalman gain</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> z <span class="op">-</span> x   <span class="co"># residual</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x <span class="op">+</span> K<span class="op">*</span>y <span class="co"># posterior</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(x):</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">+</span> vel<span class="op">*</span>dt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I used the Kalman gain form of the update function to emphasize that we do not need to consider the variances at all. If the variances converge to a single value so does the Kalman gain.</p>
</section>
<section id="filterpys-implementation" class="level2">
<h2 class="anchored" data-anchor-id="filterpys-implementation">FilterPy’s Implementation</h2>
<p>FilterPy implements <code>predict()</code> and <code>update()</code>. They work not only for the univariate case developed in this chapter, but the more general multivariate case that we learn in subsequent chapters. Because of this their interface is slightly different. They do not take Gaussians as tuples, but as two separately named variables.</p>
<p><code>predict()</code> takes several arguments, but we will only need to use these four:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>predict(x, P, u, Q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>x</code> is the state of the system. <code>P</code> is the variance of the system. <code>u</code> is the movement due to the process, and <code>Q</code> is the noise in the process. You will need to used named arguments when you call <code>predict()</code> because most of the arguments are optional. The third argument to <code>predict()</code> is <strong>not</strong> <code>u</code>.</p>
<p>These may strike you as terrible variable names. They are! As I already mentioned they come from a long history of control theory, and every paper or book you read will use these names. So, we just have to get used to it. Refusing to memorize them means you will never be able to read the literature.</p>
<p>Let’s try it for the state <span class="math inline">\(\mathcal N(10, 3)\)</span> and the movement <span class="math inline">\(\mathcal N(1, 4)\)</span>. We’d expect a final position of 11 (10+1) with a variance of 7 (3+4).</p>
<div id="cell-93" class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> filterpy.kalman <span class="im">as</span> kf</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>kf.predict(x<span class="op">=</span><span class="fl">10.</span>, P<span class="op">=</span><span class="fl">3.</span>, u<span class="op">=</span><span class="fl">1.</span>, Q<span class="op">=</span><span class="fl">4.</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>(11.0, 7.0)</code></pre>
</div>
</div>
<p><code>update</code> also takes several arguments, but for now you will be interested in these four:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>update(x, P, z, R)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As before, <code>x</code> and <code>P</code> are the state and variance of the system. <code>z</code> is the measurement, and <code>R</code> is the measurement variance. Let’s perform the last predict statement to get our prior, and then perform an update:</p>
<div id="cell-95" class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>x, P <span class="op">=</span> kf.predict(x<span class="op">=</span><span class="fl">10.</span>, P<span class="op">=</span><span class="fl">3.</span>, u<span class="op">=</span><span class="fl">1.</span>, Q<span class="op">=</span><span class="fl">2.</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>x, P <span class="op">=</span> kf.update(x<span class="op">=</span>x, P<span class="op">=</span>P, z<span class="op">=</span><span class="fl">12.</span>, R<span class="op">=</span><span class="fl">3.5</span><span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.3f}</span><span class="ss"> </span><span class="sc">{</span>P<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11.000
11.364 4.455</code></pre>
</div>
</div>
<p>I gave it a noisy measurement with a big variance, so the estimate remained close to the prior of 11.</p>
<p>One final point. I did not use the variable name <code>prior</code> for the output of the predict step. I will not use that variable name in the rest of the book. The Kalman filter equations just use <span class="math inline">\(\mathbf x\)</span>. Both the prior and the posterior are the estimated state of the system, the former is the estimate before the measurement is incorporated, and the latter is after the measurement has been incorporated.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The Kalman filter that we describe in this chapter is a special, restricted case of the more general filter we will learn next. Most texts do not discuss this one dimensional form. However, I think it is a vital stepping stone. We started the book with the g-h filter, then implemented the discrete Bayes filter, and now implemented the one dimensional Kalman filter. I have tried to show you that each of these filters use the same algorithm and reasoning. The mathematics of the Kalman filter that we will learn shortly is fairly sophisticated, and it can be difficult to understand the underlying simplicity of the filter. That sophistication comes with significant benefits: the generalized filter will markedly outperform the filters in this chapter.</p>
<p>This chapter takes time to assimilate. To truly understand it you will probably have to work through this chapter several times. I encourage you to change the various constants in the code and observe the results. Convince yourself that Gaussians are a good representation of a unimodal belief of the position of a dog in a hallway, the position of an aircraft in the sky, or the temperature of a chemical reaction chamber. Then convince yourself that multiplying Gaussians truly does compute a new belief from your prior belief and the new measurement. Finally, convince yourself that if you are measuring movement, that adding the Gaussians together updates your belief.</p>
<p>Most of all, spend enough time with the <strong>Full Description of the Algorithm</strong> section to ensure you understand the algorithm and how it relates to the g-h filter and discrete Bayes filter. There is just one ‘trick’ here - selecting a value somewhere between a prediction and a measurement. Each algorithm performs that trick with different math, but all use the same logic.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section></div></main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"0358e78f6f36480cb5ebf0f89f49934b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"041e9d6f999b4c2e8dd8006923b45a57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"0c66f10d9a2a4902b04094b62e5e154d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"0cb91a73b33441158fce28138d0636c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16191c7bd4294856ae6f7d7ab9e2bef9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ba4c1d94f65462192ffcb9f0a22b848":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e6ed0bbc3af4016ac96e8084f2a2bef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"2a4208cd088b4882a8d5a592f610a41a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a738977ecb549a993d47244d6ac50fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b5d38d7b2774008a7eeb84c37bcfb1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":false,"description":"sensor_noise","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_c14c7c9853874ef0a496e114210eae6f","max":100,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_5d214435449b45529e6a52291dcc1866","value":5}},"3109e908fc98419d806699d7a2880531":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"38e20eaaa9024a7092c43257bf6e5a68":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_0cb91a73b33441158fce28138d0636c7","msg_id":"","outputs":[]}},"3984603d140f4245ae402c3abf88710a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39b233342714416aa2a0a329dc931641":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e977b7cfed94243a71c70bcb8c918a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_41f1078f32254aa7b820bdd425d0896e","IPY_MODEL_62808e56615d416491559e92ff34d5f1","IPY_MODEL_82fb3c28b0f74a83becd6d65cbfa313a","IPY_MODEL_618b63b1caa54ba99541fbae4f78d2dc","IPY_MODEL_ba41ac9b29af49329e1ef8680bebbdf9"],"layout":"IPY_MODEL_962322a3acc94c34a4845d91e7097787"}},"40cea4dd9a854916b0a7036a2a961e3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"start_pos","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_a588d2a1208542fcbd5eba235f75ef56","max":10,"min":-10,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_8dfce204eadc41ea87d489d85bb563a7","value":0}},"41592f65aa444b6689cee2940b11c9e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41f1078f32254aa7b820bdd425d0896e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"start_pos","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_b250bfdd55e34b608edb8d2e14a17081","max":10,"min":-10,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_ea5aed6a83894ad0a4568fa111331625","value":0}},"4201a1f5e7e24c68b9d8e54e47d28aad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4392b29d01ba46ce8e8dd84d04fa326a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"4b951bc3ddb24e2f975b6c8f168285a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"5328932ead3145c2884c7a9eb24f5e62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"v1","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_6fc9d6a25d424100be095b70f12b9ccb","max":2,"min":0.1,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_041e9d6f999b4c2e8dd8006923b45a57","value":1}},"5d214435449b45529e6a52291dcc1866":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"5f42b0854cd24b25a613131fa6af4a63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"m2","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_41592f65aa444b6689cee2940b11c9e9","max":15,"min":10,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.5,"style":"IPY_MODEL_1e6ed0bbc3af4016ac96e8084f2a2bef","value":12}},"618b63b1caa54ba99541fbae4f78d2dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":false,"description":"process_noise","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_a5a5eff11fd5487cb1b87e15a4af8a80","max":100,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_6a64998029a04ff3bd5d93a2d1fba2f8","value":5}},"61b17c1656c6477693ff3ec0db2c5844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":false,"description":"velocity","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_ee15c3b7c9e84f62b4e1faab168a87b6","max":2,"min":-2,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_0358e78f6f36480cb5ebf0f89f49934b","value":1}},"62808e56615d416491559e92ff34d5f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":false,"description":"sensor_noise","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_2a738977ecb549a993d47244d6ac50fd","max":100,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_b85af9dfbd9042faa46d1a49a5f303ed","value":5}},"63d0e481df73436bb8cbe263dac07a3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":false,"description":"process_noise","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_2a4208cd088b4882a8d5a592f610a41a","max":40,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_3109e908fc98419d806699d7a2880531","value":0.1}},"672aead129ee424398d8038aa1a35bec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a64998029a04ff3bd5d93a2d1fba2f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"6a9da3a29b25419eb33c946c4347212f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"v2","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_39b233342714416aa2a0a329dc931641","max":2,"min":0.1,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_0c66f10d9a2a4902b04094b62e5e154d","value":1}},"6e8ff778989f4724a96bc66efc9dc29d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"step","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_ee9177e774d7450f863ecaca7c8c150a","max":30,"min":1,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_4392b29d01ba46ce8e8dd84d04fa326a","value":1}},"6fc9d6a25d424100be095b70f12b9ccb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82fb3c28b0f74a83becd6d65cbfa313a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":false,"description":"velocity","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_b4ae3832588c476aa88f2895267337e5","max":2,"min":-2,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.1,"style":"IPY_MODEL_4b951bc3ddb24e2f975b6c8f168285a3","value":1}},"8dfce204eadc41ea87d489d85bb563a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"962322a3acc94c34a4845d91e7097787":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9941654d582e4a399135e4e2630ef016":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a588d2a1208542fcbd5eba235f75ef56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5a5eff11fd5487cb1b87e15a4af8a80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aea352b1772c4625aae4c2793d43295c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"b250bfdd55e34b608edb8d2e14a17081":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4ae3832588c476aa88f2895267337e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85af9dfbd9042faa46d1a49a5f303ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"ba41ac9b29af49329e1ef8680bebbdf9":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_672aead129ee424398d8038aa1a35bec","msg_id":"","outputs":[]}},"c14c7c9853874ef0a496e114210eae6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c24c7036e2ce430b98dd992307a8aa92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6e8ff778989f4724a96bc66efc9dc29d","IPY_MODEL_fae8fbc196054b0da7e7a76ec1106272"],"layout":"IPY_MODEL_3984603d140f4245ae402c3abf88710a"}},"c41a34711a5f4f5c84895d51d23607e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_40cea4dd9a854916b0a7036a2a961e3d","IPY_MODEL_2b5d38d7b2774008a7eeb84c37bcfb1a","IPY_MODEL_61b17c1656c6477693ff3ec0db2c5844","IPY_MODEL_63d0e481df73436bb8cbe263dac07a3d","IPY_MODEL_f0171695805543d0b868ef4dc1e86052"],"layout":"IPY_MODEL_ecb024e32052401db633017aa0511550"}},"dfdbe538295c49f9ab55220d1d4c0de2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"m1","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_4201a1f5e7e24c68b9d8e54e47d28aad","max":10,"min":5,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.5,"style":"IPY_MODEL_aea352b1772c4625aae4c2793d43295c","value":7.5}},"e6bd73b7f70147228df681be89a514dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_dfdbe538295c49f9ab55220d1d4c0de2","IPY_MODEL_5f42b0854cd24b25a613131fa6af4a63","IPY_MODEL_5328932ead3145c2884c7a9eb24f5e62","IPY_MODEL_6a9da3a29b25419eb33c946c4347212f","IPY_MODEL_38e20eaaa9024a7092c43257bf6e5a68"],"layout":"IPY_MODEL_9941654d582e4a399135e4e2630ef016"}},"ea5aed6a83894ad0a4568fa111331625":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"ecb024e32052401db633017aa0511550":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee15c3b7c9e84f62b4e1faab168a87b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee9177e774d7450f863ecaca7c8c150a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0171695805543d0b868ef4dc1e86052":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_16191c7bd4294856ae6f7d7ab9e2bef9","msg_id":"","outputs":[]}},"fae8fbc196054b0da7e7a76ec1106272":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_1ba4c1d94f65462192ffcb9f0a22b848","msg_id":"","outputs":[]}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>